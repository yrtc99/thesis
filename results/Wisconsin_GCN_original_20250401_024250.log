[2025-04-01 02:42:53,246]: repeat 1/3
[2025-04-01 02:42:53,246]: Manual random seed:0
[2025-04-01 02:42:53,247]: auto fixed data split seed to 0, model init seed to 0
/home/rjrou/miniconda3/envs/dhgr_env/lib/python3.11/site-packages/torch_sparse/matmul.py:97: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647176074/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)
  C = torch.sparse.mm(A, B)
2.6.1
cannot load google_drive_downloader, install it if needed.
Namespace(model='GCN', num_layer=2, repeat=3, num_epoch=200, dataset='Wisconsin', data_dir='../data', gpu='0', running_id='0', log_dir=None, hidden=64, moment=1, seed=0, gnn_seed=None, auto_fixed_seed=False, use_center_moment=True, lr=0.01, wd=0.005, graph_learn=True, rewiring_type='original', lr_gl=0.001, wd_gl=0.005, thres_min_deg=3, thres_min_deg_ratio=1.0, save_dir_gl='../ckpt/', window=[10000, 10000], shuffle=[False, False], epoch_train_gl=200, epoch_finetune_gl=30, seed_gl=0, k=8, cat_self=True, drop_last=[False, False], prunning=True, epsilon=None, thres_prunning=0.3, use_cpu_cache=False, drop_edge=False, prob_drop_edge=0.0)
cpu
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(11)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0244
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0108
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0094
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0092
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0106
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0079
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0075
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0076
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0077
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0111
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0113
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0099
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0092
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0078
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0069
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0077
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0069
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0073
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0069
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0077
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0090
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0092
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0089
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0090
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0090
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0090
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0090
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0091
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0092
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0091
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0081
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0071
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0065
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0064
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0068
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0071
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0080
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0074
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0074
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0088
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0091
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0091
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0092
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0090
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0074
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0070
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0069
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0067
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0082
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0097
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0099
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0094
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0093
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0091
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0088
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0093
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0094
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0082
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0072
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0072
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0071
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0077
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0086
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0075
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0076
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0093
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0094
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0094
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0078
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0071
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0069
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0067
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0076
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0070
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0073
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0071
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0075
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0090
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0088
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0070
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0075
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0076
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0082
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0095
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0078
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0069
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0069
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0070
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0069
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0069
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0070
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0095
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0100
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0092
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0072
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0067
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0080
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0090
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0089
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0093
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0091
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0091
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0090
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0092
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0093
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0092
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0094
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0071
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0087
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0072
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0072
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0071
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0073
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0071
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0069
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0068
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0068
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0073
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0075
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0089
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0093[2025-04-01 02:42:55,089]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:42:55,124]: Epoch: 001, Loss:1.7721 Train: 0.5833, Val:0.5375, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:42:55,129]: Epoch: 002, Loss:1.1676 Train: 0.7917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0048
[2025-04-01 02:42:55,135]: Epoch: 003, Loss:0.6897 Train: 0.8500, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:42:55,141]: Epoch: 004, Loss:0.5576 Train: 0.8750, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:42:55,148]: Epoch: 005, Loss:0.4781 Train: 0.8833, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:55,156]: Epoch: 006, Loss:0.4622 Train: 0.9083, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:55,164]: Epoch: 007, Loss:0.3550 Train: 0.9250, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:55,175]: Epoch: 008, Loss:0.3437 Train: 0.9167, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0109
[2025-04-01 02:42:55,185]: Epoch: 009, Loss:0.3846 Train: 0.9083, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0093
[2025-04-01 02:42:55,193]: Epoch: 010, Loss:0.2571 Train: 0.9167, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:55,200]: Epoch: 011, Loss:0.3083 Train: 0.9333, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:55,208]: Epoch: 012, Loss:0.2798 Train: 0.9250, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:42:55,216]: Epoch: 013, Loss:0.2139 Train: 0.9333, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:55,224]: Epoch: 014, Loss:0.3041 Train: 0.9500, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:55,232]: Epoch: 015, Loss:0.2564 Train: 0.9500, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:55,240]: Epoch: 016, Loss:0.2350 Train: 0.9583, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:55,248]: Epoch: 017, Loss:0.2162 Train: 0.9500, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:42:55,256]: Epoch: 018, Loss:0.2264 Train: 0.9417, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:55,264]: Epoch: 019, Loss:0.1861 Train: 0.9417, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:55,272]: Epoch: 020, Loss:0.2492 Train: 0.9417, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:55,278]: Epoch: 021, Loss:0.2018 Train: 0.9500, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:42:55,286]: Epoch: 022, Loss:0.1789 Train: 0.9500, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:42:55,294]: Epoch: 023, Loss:0.1993 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:55,302]: Epoch: 024, Loss:0.1707 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:42:55,309]: Epoch: 025, Loss:0.1460 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:55,316]: Epoch: 026, Loss:0.1223 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:55,323]: Epoch: 027, Loss:0.1684 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:55,330]: Epoch: 028, Loss:0.1827 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:55,336]: Epoch: 029, Loss:0.1644 Train: 0.9500, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:55,343]: Epoch: 030, Loss:0.1548 Train: 0.9500, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:55,350]: Epoch: 031, Loss:0.1923 Train: 0.9583, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:55,358]: Epoch: 032, Loss:0.1848 Train: 0.9583, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:55,365]: Epoch: 033, Loss:0.1874 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:55,371]: Epoch: 034, Loss:0.1259 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:55,379]: Epoch: 035, Loss:0.1648 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:55,387]: Epoch: 036, Loss:0.1441 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:55,393]: Epoch: 037, Loss:0.1603 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:55,400]: Epoch: 038, Loss:0.1668 Train: 0.9833, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:55,408]: Epoch: 039, Loss:0.1406 Train: 0.9833, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:55,415]: Epoch: 040, Loss:0.1348 Train: 0.9750, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:55,424]: Epoch: 041, Loss:0.1342 Train: 0.9667, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:55,431]: Epoch: 042, Loss:0.1618 Train: 0.9583, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:55,437]: Epoch: 043, Loss:0.1409 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:55,445]: Epoch: 044, Loss:0.1449 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:55,452]: Epoch: 045, Loss:0.1791 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:55,459]: Epoch: 046, Loss:0.1425 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:55,467]: Epoch: 047, Loss:0.1679 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:42:55,474]: Epoch: 048, Loss:0.1167 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:55,481]: Epoch: 049, Loss:0.1346 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:42:55,488]: Epoch: 050, Loss:0.1410 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:55,495]: Epoch: 051, Loss:0.1078 Train: 0.9583, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:55,502]: Epoch: 052, Loss:0.1688 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,510]: Epoch: 053, Loss:0.1443 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:55,517]: Epoch: 054, Loss:0.1467 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:55,524]: Epoch: 055, Loss:0.1896 Train: 0.9833, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:55,531]: Epoch: 056, Loss:0.1136 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:55,537]: Epoch: 057, Loss:0.1382 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0057
[2025-04-01 02:42:55,544]: Epoch: 058, Loss:0.1281 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,550]: Epoch: 059, Loss:0.1336 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:55,557]: Epoch: 060, Loss:0.1386 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,564]: Epoch: 061, Loss:0.1246 Train: 0.9833, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:55,571]: Epoch: 062, Loss:0.1313 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:55,578]: Epoch: 063, Loss:0.1481 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:55,585]: Epoch: 064, Loss:0.1177 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:55,591]: Epoch: 065, Loss:0.1034 Train: 0.9583, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:42:55,597]: Epoch: 066, Loss:0.1488 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:42:55,604]: Epoch: 067, Loss:0.1046 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,610]: Epoch: 068, Loss:0.1386 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:55,617]: Epoch: 069, Loss:0.1543 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:55,625]: Epoch: 070, Loss:0.1305 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:55,632]: Epoch: 071, Loss:0.0933 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:55,638]: Epoch: 072, Loss:0.1320 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:55,645]: Epoch: 073, Loss:0.1448 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:55,651]: Epoch: 074, Loss:0.1273 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:55,658]: Epoch: 075, Loss:0.1255 Train: 0.9833, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:42:55,665]: Epoch: 076, Loss:0.1325 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:55,673]: Epoch: 077, Loss:0.1222 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:55,680]: Epoch: 078, Loss:0.1135 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:55,687]: Epoch: 079, Loss:0.0972 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:42:55,693]: Epoch: 080, Loss:0.0960 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:55,701]: Epoch: 081, Loss:0.1172 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:55,707]: Epoch: 082, Loss:0.1234 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:55,716]: Epoch: 083, Loss:0.1209 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:55,722]: Epoch: 084, Loss:0.1511 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:55,728]: Epoch: 085, Loss:0.0856 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:42:55,735]: Epoch: 086, Loss:0.0924 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:55,742]: Epoch: 087, Loss:0.1235 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:55,749]: Epoch: 088, Loss:0.1202 Train: 0.9583, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:55,757]: Epoch: 089, Loss:0.1255 Train: 0.9583, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:55,765]: Epoch: 090, Loss:0.1670 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:55,770]: Epoch: 091, Loss:0.1207 Train: 0.9667, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0055
[2025-04-01 02:42:55,777]: Epoch: 092, Loss:0.1529 Train: 0.9750, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:42:55,784]: Epoch: 093, Loss:0.1244 Train: 0.9667, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:42:55,790]: Epoch: 094, Loss:0.1054 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0053
[2025-04-01 02:42:55,797]: Epoch: 095, Loss:0.1404 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:55,804]: Epoch: 096, Loss:0.1218 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:55,810]: Epoch: 097, Loss:0.1127 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:42:55,818]: Epoch: 098, Loss:0.1378 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:42:55,826]: Epoch: 099, Loss:0.1683 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:55,834]: Epoch: 100, Loss:0.1366 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:55,842]: Epoch: 101, Loss:0.1105 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:55,849]: Epoch: 102, Loss:0.1154 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:55,856]: Epoch: 103, Loss:0.1491 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:42:55,864]: Epoch: 104, Loss:0.1121 Train: 0.9667, Val:0.7500, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:42:55,870]: Epoch: 105, Loss:0.1054 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:55,878]: Epoch: 106, Loss:0.0950 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:42:55,885]: Epoch: 107, Loss:0.1110 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,892]: Epoch: 108, Loss:0.1134 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:55,900]: Epoch: 109, Loss:0.1504 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:55,907]: Epoch: 110, Loss:0.0987 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:42:55,913]: Epoch: 111, Loss:0.1263 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:42:55,920]: Epoch: 112, Loss:0.1093 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:55,928]: Epoch: 113, Loss:0.1260 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:55,936]: Epoch: 114, Loss:0.1092 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:55,942]: Epoch: 115, Loss:0.1800 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:55,949]: Epoch: 116, Loss:0.1378 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:55,956]: Epoch: 117, Loss:0.1255 Train: 0.9583, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:55,963]: Epoch: 118, Loss:0.1655 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:55,970]: Epoch: 119, Loss:0.1514 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:55,976]: Epoch: 120, Loss:0.1084 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:55,983]: Epoch: 121, Loss:0.1341 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:55,989]: Epoch: 122, Loss:0.1532 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:55,997]: Epoch: 123, Loss:0.1481 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:56,003]: Epoch: 124, Loss:0.1434 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:42:56,011]: Epoch: 125, Loss:0.1415 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:56,017]: Epoch: 126, Loss:0.1337 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:56,024]: Epoch: 127, Loss:0.1420 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:56,031]: Epoch: 128, Loss:0.1303 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:56,037]: Epoch: 129, Loss:0.1066 Train: 0.9667, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:56,044]: Epoch: 130, Loss:0.1068 Train: 0.9833, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:56,052]: Epoch: 131, Loss:0.0995 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:56,059]: Epoch: 132, Loss:0.1387 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:56,067]: Epoch: 133, Loss:0.0936 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:56,074]: Epoch: 134, Loss:0.1411 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:56,080]: Epoch: 135, Loss:0.1150 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:42:56,087]: Epoch: 136, Loss:0.1392 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:56,094]: Epoch: 137, Loss:0.1132 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:56,101]: Epoch: 138, Loss:0.1088 Train: 0.9833, Val:0.7500, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:42:56,107]: Epoch: 139, Loss:0.1274 Train: 0.9833, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:42:56,114]: Epoch: 140, Loss:0.0988 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:56,120]: Epoch: 141, Loss:0.1256 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:56,127]: Epoch: 142, Loss:0.0992 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:56,135]: Epoch: 143, Loss:0.1431 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:56,143]: Epoch: 144, Loss:0.0931 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:56,150]: Epoch: 145, Loss:0.1133 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:56,156]: Epoch: 146, Loss:0.1049 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:56,164]: Epoch: 147, Loss:0.1057 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:56,172]: Epoch: 148, Loss:0.1003 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:42:56,180]: Epoch: 149, Loss:0.1079 Train: 0.9750, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:42:56,188]: Epoch: 150, Loss:0.1141 Train: 0.9667, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:42:56,196]: Epoch: 151, Loss:0.1116 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:42:56,203]: Epoch: 152, Loss:0.1268 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:56,211]: Epoch: 153, Loss:0.1069 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:56,217]: Epoch: 154, Loss:0.0811 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:42:56,224]: Epoch: 155, Loss:0.0899 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:56,230]: Epoch: 156, Loss:0.0932 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:56,239]: Epoch: 157, Loss:0.1065 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:56,246]: Epoch: 158, Loss:0.0801 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:56,254]: Epoch: 159, Loss:0.1051 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:56,260]: Epoch: 160, Loss:0.1284 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:42:56,268]: Epoch: 161, Loss:0.0744 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:56,274]: Epoch: 162, Loss:0.1498 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:56,281]: Epoch: 163, Loss:0.1006 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:56,288]: Epoch: 164, Loss:0.1181 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:56,294]: Epoch: 165, Loss:0.1371 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:56,302]: Epoch: 166, Loss:0.1042 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:56,308]: Epoch: 167, Loss:0.0823 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:56,315]: Epoch: 168, Loss:0.1165 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:56,323]: Epoch: 169, Loss:0.0943 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:56,330]: Epoch: 170, Loss:0.1545 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:56,337]: Epoch: 171, Loss:0.1037 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:42:56,344]: Epoch: 172, Loss:0.1163 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:56,350]: Epoch: 173, Loss:0.1389 Train: 0.9833, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:56,360]: Epoch: 174, Loss:0.0920 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0098
[2025-04-01 02:42:56,369]: Epoch: 175, Loss:0.1668 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:42:56,378]: Epoch: 176, Loss:0.0853 Train: 0.9667, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:42:56,386]: Epoch: 177, Loss:0.1271 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:56,393]: Epoch: 178, Loss:0.1221 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:56,399]: Epoch: 179, Loss:0.0981 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:42:56,406]: Epoch: 180, Loss:0.1175 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:42:56,414]: Epoch: 181, Loss:0.1183 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:42:56,421]: Epoch: 182, Loss:0.0902 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:42:56,428]: Epoch: 183, Loss:0.1172 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:42:56,435]: Epoch: 184, Loss:0.0763 Train: 0.9750, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:56,443]: Epoch: 185, Loss:0.1035 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:56,449]: Epoch: 186, Loss:0.1195 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:56,456]: Epoch: 187, Loss:0.1306 Train: 0.9667, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:42:56,463]: Epoch: 188, Loss:0.1033 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:56,469]: Epoch: 189, Loss:0.1035 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:42:56,477]: Epoch: 190, Loss:0.1042 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:42:56,485]: Epoch: 191, Loss:0.0929 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:42:56,494]: Epoch: 192, Loss:0.1352 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:42:56,501]: Epoch: 193, Loss:0.1115 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:56,509]: Epoch: 194, Loss:0.1162 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:42:56,517]: Epoch: 195, Loss:0.1502 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:56,524]: Epoch: 196, Loss:0.0951 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:56,530]: Epoch: 197, Loss:0.1185 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0054
[2025-04-01 02:42:56,537]: Epoch: 198, Loss:0.1072 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:56,544]: Epoch: 199, Loss:0.0932 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:56,552]: Epoch: 200, Loss:0.1144 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:56,552]: [Run-1 score] {'train': 0.9583333333333334, 'val': 0.775, 'test': 0.6862745098039216}
[2025-04-01 02:42:56,552]: repeat 2/3
[2025-04-01 02:42:56,552]: Manual random seed:0
[2025-04-01 02:42:56,553]: auto fixed data split seed to 0, model init seed to 1
/home/rjrou/DHGR/GraphLearner.py:557: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(load_path).to(data.x.device)
[2025-04-01 02:42:56,556]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:42:56,565]: Epoch: 001, Loss:1.6269 Train: 0.7250, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:42:56,571]: Epoch: 002, Loss:1.0054 Train: 0.8000, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:42:56,578]: Epoch: 003, Loss:0.6750 Train: 0.8583, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:42:56,586]: Epoch: 004, Loss:0.5285 Train: 0.8667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:56,594]: Epoch: 005, Loss:0.4396 Train: 0.9000, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:56,600]: Epoch: 006, Loss:0.3976 Train: 0.8917, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:42:56,608]: Epoch: 007, Loss:0.3825 Train: 0.8833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:56,614]: Epoch: 008, Loss:0.3051 Train: 0.9333, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:56,620]: Epoch: 009, Loss:0.2891 Train: 0.9250, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:56,628]: Epoch: 010, Loss:0.2597 Train: 0.9167, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:56,635]: Epoch: 011, Loss:0.2642 Train: 0.9250, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:56,643]: Epoch: 012, Loss:0.2384 Train: 0.9583, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:56,651]: Epoch: 013, Loss:0.2256 Train: 0.9500, Val:0.7875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:56,659]: Epoch: 014, Loss:0.2016 Train: 0.9333, Val:0.7875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:42:56,666]: Epoch: 015, Loss:0.2503 Train: 0.9333, Val:0.7875, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:42:56,674]: Epoch: 016, Loss:0.2027 Train: 0.9500, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:56,680]: Epoch: 017, Loss:0.2116 Train: 0.9833, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:42:56,686]: Epoch: 018, Loss:0.1581 Train: 0.9583, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:56,694]: Epoch: 019, Loss:0.1770 Train: 0.9500, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:56,699]: Epoch: 020, Loss:0.1886 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:42:56,707]: Epoch: 021, Loss:0.1611 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:56,714]: Epoch: 022, Loss:0.2333 Train: 0.9833, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:42:56,720]: Epoch: 023, Loss:0.1641 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:42:56,728]: Epoch: 024, Loss:0.1748 Train: 0.9667, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:56,736]: Epoch: 025, Loss:0.1999 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:56,743]: Epoch: 026, Loss:0.1861 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:56,750]: Epoch: 027, Loss:0.1457 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:56,758]: Epoch: 028, Loss:0.1415 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:56,766]: Epoch: 029, Loss:0.1633 Train: 0.9500, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:56,774]: Epoch: 030, Loss:0.1321 Train: 0.9583, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:56,781]: Epoch: 031, Loss:0.1343 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:56,788]: Epoch: 032, Loss:0.1572 Train: 0.9667, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:56,795]: Epoch: 033, Loss:0.1683 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:42:56,801]: Epoch: 034, Loss:0.2005 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:56,808]: Epoch: 035, Loss:0.1530 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:56,814]: Epoch: 036, Loss:0.1581 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:56,820]: Epoch: 037, Loss:0.1126 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:42:56,827]: Epoch: 038, Loss:0.1303 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:56,834]: Epoch: 039, Loss:0.1202 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:56,840]: Epoch: 040, Loss:0.1378 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:56,848]: Epoch: 041, Loss:0.1417 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:56,856]: Epoch: 042, Loss:0.1311 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:56,863]: Epoch: 043, Loss:0.1199 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:42:56,869]: Epoch: 044, Loss:0.1278 Train: 0.9833, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0056
[2025-04-01 02:42:56,876]: Epoch: 045, Loss:0.1356 Train: 0.9667, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:42:56,884]: Epoch: 046, Loss:0.1719 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:56,892]: Epoch: 047, Loss:0.1799 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:56,899]: Epoch: 048, Loss:0.1305 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:56,907]: Epoch: 049, Loss:0.1473 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:56,914]: Epoch: 050, Loss:0.1099 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:56,921]: Epoch: 051, Loss:0.2121 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:56,928]: Epoch: 052, Loss:0.1530 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:42:56,936]: Epoch: 053, Loss:0.1241 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:56,944]: Epoch: 054, Loss:0.2211 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:56,949]: Epoch: 055, Loss:0.1613 Train: 0.9667, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0056
[2025-04-01 02:42:56,957]: Epoch: 056, Loss:0.1631 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:56,964]: Epoch: 057, Loss:0.1170 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:56,972]: Epoch: 058, Loss:0.1927 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:56,978]: Epoch: 059, Loss:0.1304 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:56,986]: Epoch: 060, Loss:0.1139 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:56,993]: Epoch: 061, Loss:0.1195 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:56,999]: Epoch: 062, Loss:0.1474 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0055
[2025-04-01 02:42:57,006]: Epoch: 063, Loss:0.1286 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:57,014]: Epoch: 064, Loss:0.1454 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:57,020]: Epoch: 065, Loss:0.1377 Train: 0.9833, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:42:57,027]: Epoch: 066, Loss:0.1517 Train: 0.9667, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:42:57,033]: Epoch: 067, Loss:0.1206 Train: 0.9583, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:42:57,040]: Epoch: 068, Loss:0.1198 Train: 0.9833, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:57,048]: Epoch: 069, Loss:0.1145 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:57,057]: Epoch: 070, Loss:0.1282 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:42:57,065]: Epoch: 071, Loss:0.1465 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:57,071]: Epoch: 072, Loss:0.1238 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:57,079]: Epoch: 073, Loss:0.0952 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:57,085]: Epoch: 074, Loss:0.1261 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:57,092]: Epoch: 075, Loss:0.1072 Train: 0.9833, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:42:57,099]: Epoch: 076, Loss:0.0934 Train: 0.9583, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:57,106]: Epoch: 077, Loss:0.1263 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:57,112]: Epoch: 078, Loss:0.1604 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:42:57,120]: Epoch: 079, Loss:0.1451 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:57,126]: Epoch: 080, Loss:0.1035 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:57,134]: Epoch: 081, Loss:0.2295 Train: 0.9583, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:57,140]: Epoch: 082, Loss:0.1741 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0055
[2025-04-01 02:42:57,147]: Epoch: 083, Loss:0.1758 Train: 0.9667, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:57,154]: Epoch: 084, Loss:0.1372 Train: 0.9417, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:42:57,161]: Epoch: 085, Loss:0.1736 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:57,168]: Epoch: 086, Loss:0.1829 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:57,176]: Epoch: 087, Loss:0.1575 Train: 0.9833, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:42:57,184]: Epoch: 088, Loss:0.1877 Train: 0.9833, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:57,190]: Epoch: 089, Loss:0.1896 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:57,198]: Epoch: 090, Loss:0.1707 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:57,205]: Epoch: 091, Loss:0.0963 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:57,212]: Epoch: 092, Loss:0.0972 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:42:57,218]: Epoch: 093, Loss:0.1132 Train: 0.9667, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:42:57,225]: Epoch: 094, Loss:0.1553 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:57,232]: Epoch: 095, Loss:0.1366 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:57,240]: Epoch: 096, Loss:0.1159 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:42:57,249]: Epoch: 097, Loss:0.1562 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:57,257]: Epoch: 098, Loss:0.1292 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:57,264]: Epoch: 099, Loss:0.1499 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:57,273]: Epoch: 100, Loss:0.1796 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:57,280]: Epoch: 101, Loss:0.0924 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:57,287]: Epoch: 102, Loss:0.1077 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:57,295]: Epoch: 103, Loss:0.1348 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:57,304]: Epoch: 104, Loss:0.1227 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0088
[2025-04-01 02:42:57,312]: Epoch: 105, Loss:0.1980 Train: 0.9917, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:57,320]: Epoch: 106, Loss:0.0979 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:57,327]: Epoch: 107, Loss:0.1085 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:57,335]: Epoch: 108, Loss:0.1134 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:57,343]: Epoch: 109, Loss:0.1284 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:57,350]: Epoch: 110, Loss:0.0959 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:57,358]: Epoch: 111, Loss:0.1495 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:42:57,366]: Epoch: 112, Loss:0.1095 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:57,374]: Epoch: 113, Loss:0.0972 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:42:57,382]: Epoch: 114, Loss:0.0929 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:42:57,388]: Epoch: 115, Loss:0.1620 Train: 0.9833, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0054
[2025-04-01 02:42:57,395]: Epoch: 116, Loss:0.1089 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:57,402]: Epoch: 117, Loss:0.1569 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:57,408]: Epoch: 118, Loss:0.1117 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:42:57,416]: Epoch: 119, Loss:0.1361 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:57,422]: Epoch: 120, Loss:0.1241 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:57,429]: Epoch: 121, Loss:0.1518 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:57,436]: Epoch: 122, Loss:0.1387 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:57,443]: Epoch: 123, Loss:0.1164 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:42:57,450]: Epoch: 124, Loss:0.1166 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:42:57,458]: Epoch: 125, Loss:0.1069 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:57,464]: Epoch: 126, Loss:0.1127 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:42:57,470]: Epoch: 127, Loss:0.1246 Train: 0.9917, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:42:57,478]: Epoch: 128, Loss:0.0981 Train: 0.9917, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:42:57,484]: Epoch: 129, Loss:0.1012 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:42:57,492]: Epoch: 130, Loss:0.0850 Train: 0.9417, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:57,499]: Epoch: 131, Loss:0.1528 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:57,506]: Epoch: 132, Loss:0.1668 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:57,514]: Epoch: 133, Loss:0.1130 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:57,522]: Epoch: 134, Loss:0.1172 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:57,529]: Epoch: 135, Loss:0.1275 Train: 0.9667, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:57,536]: Epoch: 136, Loss:0.0967 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:57,543]: Epoch: 137, Loss:0.0812 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:57,551]: Epoch: 138, Loss:0.0911 Train: 0.9750, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:57,557]: Epoch: 139, Loss:0.0967 Train: 0.9583, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:57,563]: Epoch: 140, Loss:0.1561 Train: 0.9667, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:42:57,571]: Epoch: 141, Loss:0.1180 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:57,577]: Epoch: 142, Loss:0.1210 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:42:57,584]: Epoch: 143, Loss:0.1380 Train: 0.9500, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:57,590]: Epoch: 144, Loss:0.1299 Train: 0.9583, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:57,598]: Epoch: 145, Loss:0.1293 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:57,606]: Epoch: 146, Loss:0.1224 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:42:57,615]: Epoch: 147, Loss:0.0878 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:57,621]: Epoch: 148, Loss:0.1146 Train: 0.9667, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:42:57,628]: Epoch: 149, Loss:0.1291 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:57,636]: Epoch: 150, Loss:0.0995 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:57,644]: Epoch: 151, Loss:0.0925 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:57,650]: Epoch: 152, Loss:0.1085 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:42:57,657]: Epoch: 153, Loss:0.1197 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:57,665]: Epoch: 154, Loss:0.1074 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:57,673]: Epoch: 155, Loss:0.0792 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:57,684]: Epoch: 156, Loss:0.1115 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0105
[2025-04-01 02:42:57,698]: Epoch: 157, Loss:0.0826 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0136
[2025-04-01 02:42:57,720]: Epoch: 158, Loss:0.0762 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0219
[2025-04-01 02:42:57,730]: Epoch: 159, Loss:0.1207 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0098
[2025-04-01 02:42:57,744]: Epoch: 160, Loss:0.1225 Train: 0.9917, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0138
[2025-04-01 02:42:57,752]: Epoch: 161, Loss:0.1360 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:42:57,760]: Epoch: 162, Loss:0.1102 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:57,767]: Epoch: 163, Loss:0.1225 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:42:57,773]: Epoch: 164, Loss:0.1404 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:42:57,780]: Epoch: 165, Loss:0.1019 Train: 0.9667, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:42:57,787]: Epoch: 166, Loss:0.1285 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:42:57,795]: Epoch: 167, Loss:0.1169 Train: 0.9917, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:57,803]: Epoch: 168, Loss:0.1438 Train: 0.9917, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:42:57,809]: Epoch: 169, Loss:0.1161 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:42:57,817]: Epoch: 170, Loss:0.1212 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:57,824]: Epoch: 171, Loss:0.0870 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:57,831]: Epoch: 172, Loss:0.1049 Train: 0.9833, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:57,839]: Epoch: 173, Loss:0.1071 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:57,845]: Epoch: 174, Loss:0.1320 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:42:57,852]: Epoch: 175, Loss:0.1040 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:42:57,860]: Epoch: 176, Loss:0.1083 Train: 0.9833, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:57,867]: Epoch: 177, Loss:0.0902 Train: 0.9833, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:57,875]: Epoch: 178, Loss:0.0822 Train: 0.9583, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:57,881]: Epoch: 179, Loss:0.1135 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:57,888]: Epoch: 180, Loss:0.0853 Train: 0.9833, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:57,894]: Epoch: 181, Loss:0.1010 Train: 0.9833, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:42:57,903]: Epoch: 182, Loss:0.0800 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:42:57,912]: Epoch: 183, Loss:0.2039 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:42:57,923]: Epoch: 184, Loss:0.0802 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0105
[2025-04-01 02:42:57,931]: Epoch: 185, Loss:0.0969 Train: 0.9667, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:57,939]: Epoch: 186, Loss:0.2374 Train: 0.9833, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:57,945]: Epoch: 187, Loss:0.0933 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:42:57,954]: Epoch: 188, Loss:0.1902 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:42:57,962]: Epoch: 189, Loss:0.1581 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:42:57,970]: Epoch: 190, Loss:0.1270 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:57,977]: Epoch: 191, Loss:0.2634 Train: 0.9667, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:42:57,984]: Epoch: 192, Loss:0.1405 Train: 0.9833, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:57,990]: Epoch: 193, Loss:0.1136 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:42:57,998]: Epoch: 194, Loss:0.1414 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:58,005]: Epoch: 195, Loss:0.1619 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:58,012]: Epoch: 196, Loss:0.1480 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:58,019]: Epoch: 197, Loss:0.1783 Train: 0.9917, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:58,027]: Epoch: 198, Loss:0.1155 Train: 0.9917, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:42:58,035]: Epoch: 199, Loss:0.1456 Train: 0.9833, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:58,043]: Epoch: 200, Loss:0.1349 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:58,044]: [Run-2 score] {'train': 0.95, 'val': 0.7875, 'test': 0.6666666666666666}
[2025-04-01 02:42:58,044]: repeat 3/3
[2025-04-01 02:42:58,044]: Manual random seed:0
[2025-04-01 02:42:58,044]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:42:58,048]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:42:58,059]: Epoch: 001, Loss:1.7164 Train: 0.7833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:42:58,065]: Epoch: 002, Loss:1.0206 Train: 0.8250, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:42:58,072]: Epoch: 003, Loss:0.7572 Train: 0.8500, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:42:58,080]: Epoch: 004, Loss:0.4918 Train: 0.8667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:58,088]: Epoch: 005, Loss:0.4326 Train: 0.8833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:42:58,096]: Epoch: 006, Loss:0.4043 Train: 0.8833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:58,104]: Epoch: 007, Loss:0.3521 Train: 0.8833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:58,111]: Epoch: 008, Loss:0.3067 Train: 0.9083, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:58,119]: Epoch: 009, Loss:0.3261 Train: 0.9333, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:58,127]: Epoch: 010, Loss:0.2847 Train: 0.9583, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:58,134]: Epoch: 011, Loss:0.2805 Train: 0.9333, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:58,141]: Epoch: 012, Loss:0.2721 Train: 0.9417, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:58,149]: Epoch: 013, Loss:0.2625 Train: 0.9583, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:58,156]: Epoch: 014, Loss:0.2508 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:58,163]: Epoch: 015, Loss:0.2247 Train: 0.9583, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:58,173]: Epoch: 016, Loss:0.2267 Train: 0.9583, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0101
[2025-04-01 02:42:58,183]: Epoch: 017, Loss:0.2273 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0092
[2025-04-01 02:42:58,189]: Epoch: 018, Loss:0.1932 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:58,197]: Epoch: 019, Loss:0.2136 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:42:58,205]: Epoch: 020, Loss:0.1509 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:58,213]: Epoch: 021, Loss:0.2189 Train: 0.9667, Val:0.7750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:58,221]: Epoch: 022, Loss:0.1723 Train: 0.9583, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:58,228]: Epoch: 023, Loss:0.2044 Train: 0.9583, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:42:58,236]: Epoch: 024, Loss:0.2543 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:58,244]: Epoch: 025, Loss:0.1781 Train: 0.9500, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,251]: Epoch: 026, Loss:0.1735 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,259]: Epoch: 027, Loss:0.1600 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:58,269]: Epoch: 028, Loss:0.1524 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0098
[2025-04-01 02:42:58,277]: Epoch: 029, Loss:0.1436 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,285]: Epoch: 030, Loss:0.1426 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,292]: Epoch: 031, Loss:0.1199 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:58,300]: Epoch: 032, Loss:0.2077 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:42:58,308]: Epoch: 033, Loss:0.1294 Train: 0.9583, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:58,315]: Epoch: 034, Loss:0.2408 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:42:58,323]: Epoch: 035, Loss:0.1256 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:42:58,331]: Epoch: 036, Loss:0.1403 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:58,339]: Epoch: 037, Loss:0.1470 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:58,347]: Epoch: 038, Loss:0.1876 Train: 0.9750, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:58,355]: Epoch: 039, Loss:0.1823 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:58,361]: Epoch: 040, Loss:0.1645 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:42:58,368]: Epoch: 041, Loss:0.2022 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:58,376]: Epoch: 042, Loss:0.1294 Train: 0.9667, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:42:58,385]: Epoch: 043, Loss:0.1267 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:42:58,393]: Epoch: 044, Loss:0.2019 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:42:58,402]: Epoch: 045, Loss:0.1593 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:42:58,412]: Epoch: 046, Loss:0.1374 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:42:58,420]: Epoch: 047, Loss:0.1459 Train: 0.9583, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:42:58,429]: Epoch: 048, Loss:0.1591 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:42:58,438]: Epoch: 049, Loss:0.1411 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:42:58,447]: Epoch: 050, Loss:0.1440 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:42:58,456]: Epoch: 051, Loss:0.1239 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:42:58,467]: Epoch: 052, Loss:0.1210 Train: 0.9667, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0107
[2025-04-01 02:42:58,478]: Epoch: 053, Loss:0.1401 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0105
[2025-04-01 02:42:58,489]: Epoch: 054, Loss:0.1767 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0108
[2025-04-01 02:42:58,496]: Epoch: 055, Loss:0.1736 Train: 0.9667, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:58,505]: Epoch: 056, Loss:0.1815 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:42:58,513]: Epoch: 057, Loss:0.1631 Train: 0.9667, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:58,523]: Epoch: 058, Loss:0.1607 Train: 0.9667, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0094
[2025-04-01 02:42:58,536]: Epoch: 059, Loss:0.1564 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0132
[2025-04-01 02:42:58,544]: Epoch: 060, Loss:0.1706 Train: 0.9667, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:58,550]: Epoch: 061, Loss:0.1497 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0058
[2025-04-01 02:42:58,558]: Epoch: 062, Loss:0.1609 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:58,566]: Epoch: 063, Loss:0.1060 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:42:58,573]: Epoch: 064, Loss:0.1071 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:42:58,581]: Epoch: 065, Loss:0.1142 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:42:58,589]: Epoch: 066, Loss:0.1343 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:58,596]: Epoch: 067, Loss:0.1317 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,603]: Epoch: 068, Loss:0.1306 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:58,609]: Epoch: 069, Loss:0.1879 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:58,617]: Epoch: 070, Loss:0.0987 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:58,626]: Epoch: 071, Loss:0.0928 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:42:58,633]: Epoch: 072, Loss:0.1452 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:58,641]: Epoch: 073, Loss:0.1209 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:58,647]: Epoch: 074, Loss:0.1087 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:42:58,656]: Epoch: 075, Loss:0.1172 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:42:58,664]: Epoch: 076, Loss:0.0926 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:58,673]: Epoch: 077, Loss:0.1313 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:42:58,681]: Epoch: 078, Loss:0.1133 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:58,687]: Epoch: 079, Loss:0.1447 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:58,695]: Epoch: 080, Loss:0.1566 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:58,703]: Epoch: 081, Loss:0.1561 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:58,711]: Epoch: 082, Loss:0.0995 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:58,718]: Epoch: 083, Loss:0.1022 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:42:58,725]: Epoch: 084, Loss:0.1133 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:42:58,733]: Epoch: 085, Loss:0.1116 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:58,740]: Epoch: 086, Loss:0.1062 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:58,747]: Epoch: 087, Loss:0.1800 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:58,754]: Epoch: 088, Loss:0.1227 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:42:58,760]: Epoch: 089, Loss:0.0909 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:42:58,769]: Epoch: 090, Loss:0.1634 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:42:58,777]: Epoch: 091, Loss:0.1012 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:58,785]: Epoch: 092, Loss:0.1604 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:58,794]: Epoch: 093, Loss:0.1453 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:58,800]: Epoch: 094, Loss:0.1363 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:42:58,808]: Epoch: 095, Loss:0.1002 Train: 0.9833, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:42:58,816]: Epoch: 096, Loss:0.1097 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:58,823]: Epoch: 097, Loss:0.1162 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:58,832]: Epoch: 098, Loss:0.0880 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:42:58,839]: Epoch: 099, Loss:0.1446 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:58,846]: Epoch: 100, Loss:0.1480 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:58,852]: Epoch: 101, Loss:0.1122 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:58,860]: Epoch: 102, Loss:0.1706 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:58,868]: Epoch: 103, Loss:0.1572 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:58,875]: Epoch: 104, Loss:0.1546 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:42:58,883]: Epoch: 105, Loss:0.1339 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:42:58,889]: Epoch: 106, Loss:0.1202 Train: 0.9750, Val:0.7500, Test: 0.7255, Time(s/epoch):0.0065
[2025-04-01 02:42:58,897]: Epoch: 107, Loss:0.1842 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:42:58,904]: Epoch: 108, Loss:0.1160 Train: 0.9750, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:58,910]: Epoch: 109, Loss:0.1210 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:58,918]: Epoch: 110, Loss:0.1279 Train: 0.9667, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:58,925]: Epoch: 111, Loss:0.1736 Train: 0.9750, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:42:58,932]: Epoch: 112, Loss:0.1029 Train: 0.9750, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:42:58,940]: Epoch: 113, Loss:0.1513 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:42:58,948]: Epoch: 114, Loss:0.1161 Train: 0.9833, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:42:58,955]: Epoch: 115, Loss:0.1310 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:58,961]: Epoch: 116, Loss:0.1027 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:42:58,968]: Epoch: 117, Loss:0.0943 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:58,975]: Epoch: 118, Loss:0.1074 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:58,983]: Epoch: 119, Loss:0.1470 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:42:58,989]: Epoch: 120, Loss:0.1878 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:58,997]: Epoch: 121, Loss:0.1165 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:59,003]: Epoch: 122, Loss:0.1922 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:59,009]: Epoch: 123, Loss:0.1519 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:42:59,018]: Epoch: 124, Loss:0.1471 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:42:59,024]: Epoch: 125, Loss:0.1598 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:59,031]: Epoch: 126, Loss:0.1098 Train: 0.9750, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:42:59,038]: Epoch: 127, Loss:0.1031 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:42:59,045]: Epoch: 128, Loss:0.1578 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:42:59,053]: Epoch: 129, Loss:0.1944 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:42:59,061]: Epoch: 130, Loss:0.1915 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:59,068]: Epoch: 131, Loss:0.1398 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:42:59,075]: Epoch: 132, Loss:0.1648 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:42:59,083]: Epoch: 133, Loss:0.1989 Train: 0.9667, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:59,091]: Epoch: 134, Loss:0.1628 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:59,097]: Epoch: 135, Loss:0.1521 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:42:59,106]: Epoch: 136, Loss:0.1547 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:42:59,111]: Epoch: 137, Loss:0.1378 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0055
[2025-04-01 02:42:59,119]: Epoch: 138, Loss:0.1269 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:59,127]: Epoch: 139, Loss:0.1210 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:42:59,136]: Epoch: 140, Loss:0.1400 Train: 0.9583, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:42:59,144]: Epoch: 141, Loss:0.1550 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:42:59,153]: Epoch: 142, Loss:0.1261 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0086
[2025-04-01 02:42:59,161]: Epoch: 143, Loss:0.1464 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:42:59,169]: Epoch: 144, Loss:0.1231 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:42:59,178]: Epoch: 145, Loss:0.0945 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:42:59,186]: Epoch: 146, Loss:0.1475 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:42:59,193]: Epoch: 147, Loss:0.1299 Train: 0.9833, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:59,201]: Epoch: 148, Loss:0.1592 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:42:59,209]: Epoch: 149, Loss:0.1002 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:59,216]: Epoch: 150, Loss:0.1179 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:42:59,225]: Epoch: 151, Loss:0.1114 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:42:59,233]: Epoch: 152, Loss:0.1146 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:42:59,241]: Epoch: 153, Loss:0.1205 Train: 0.9667, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:59,249]: Epoch: 154, Loss:0.1231 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:42:59,256]: Epoch: 155, Loss:0.1343 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:42:59,263]: Epoch: 156, Loss:0.1115 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:42:59,269]: Epoch: 157, Loss:0.1423 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:42:59,277]: Epoch: 158, Loss:0.1284 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:42:59,285]: Epoch: 159, Loss:0.1253 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:42:59,292]: Epoch: 160, Loss:0.1119 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:42:59,298]: Epoch: 161, Loss:0.1150 Train: 0.9750, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:42:59,307]: Epoch: 162, Loss:0.0858 Train: 0.9667, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:59,316]: Epoch: 163, Loss:0.0963 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:42:59,325]: Epoch: 164, Loss:0.1428 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:42:59,331]: Epoch: 165, Loss:0.1199 Train: 0.9583, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:42:59,340]: Epoch: 166, Loss:0.1501 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:42:59,348]: Epoch: 167, Loss:0.1185 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:42:59,356]: Epoch: 168, Loss:0.1213 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:59,365]: Epoch: 169, Loss:0.0880 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:42:59,372]: Epoch: 170, Loss:0.1345 Train: 0.9750, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:42:59,379]: Epoch: 171, Loss:0.1396 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:59,388]: Epoch: 172, Loss:0.1081 Train: 0.9750, Val:0.7750, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:42:59,397]: Epoch: 173, Loss:0.1122 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:42:59,404]: Epoch: 174, Loss:0.1071 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:42:59,413]: Epoch: 175, Loss:0.1509 Train: 0.9583, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:42:59,422]: Epoch: 176, Loss:0.1469 Train: 0.9750, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:42:59,430]: Epoch: 177, Loss:0.1101 Train: 0.9750, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:42:59,438]: Epoch: 178, Loss:0.1406 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:42:59,447]: Epoch: 179, Loss:0.1700 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:42:59,454]: Epoch: 180, Loss:0.1241 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:42:59,463]: Epoch: 181, Loss:0.1116 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:42:59,469]: Epoch: 182, Loss:0.1249 Train: 0.9667, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:42:59,476]: Epoch: 183, Loss:0.0945 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:42:59,484]: Epoch: 184, Loss:0.0928 Train: 0.9833, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:42:59,490]: Epoch: 185, Loss:0.1405 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:42:59,499]: Epoch: 186, Loss:0.1162 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:42:59,506]: Epoch: 187, Loss:0.1156 Train: 0.9750, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:42:59,514]: Epoch: 188, Loss:0.1080 Train: 0.9917, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:42:59,522]: Epoch: 189, Loss:0.1242 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:42:59,530]: Epoch: 190, Loss:0.0798 Train: 0.9833, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:42:59,537]: Epoch: 191, Loss:0.1095 Train: 0.9833, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:42:59,544]: Epoch: 192, Loss:0.1210 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:42:59,552]: Epoch: 193, Loss:0.0951 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:42:59,560]: Epoch: 194, Loss:0.1135 Train: 0.9750, Val:0.7625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:42:59,568]: Epoch: 195, Loss:0.1362 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:42:59,576]: Epoch: 196, Loss:0.1036 Train: 0.9917, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:42:59,583]: Epoch: 197, Loss:0.1136 Train: 0.9917, Val:0.7375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:42:59,589]: Epoch: 198, Loss:0.1048 Train: 0.9833, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:42:59,598]: Epoch: 199, Loss:0.1108 Train: 0.9750, Val:0.7625, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:42:59,605]: Epoch: 200, Loss:0.1207 Train: 0.9750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:42:59,605]: [Run-3 score] {'train': 0.9666666666666667, 'val': 0.775, 'test': 0.6666666666666666}
[2025-04-01 02:42:59,605]: repeat 1/3
[2025-04-01 02:42:59,605]: Manual random seed:0
[2025-04-01 02:42:59,606]: auto fixed data split seed to 0, model init seed to 0

Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0074
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0084
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0090
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0094
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0072
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0074
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0069
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0072
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0070
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0071
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0084
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0068
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0073
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0070
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0071
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0067
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0080
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0091
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0081
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0069
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0067
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0067
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0067
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0070
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0071
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0090
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0095
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0089
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0091
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0090
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0091
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0087
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0066
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0065
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0071
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0072
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0085
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0094
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0095
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0093
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0092
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0095
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0099
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0097
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0069
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0072
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0068
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0070
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0070
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0074
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0071
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0068
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0072
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0073
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0093
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0093
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0090
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0075
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0070
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0086
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0091
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0090
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0091
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0090
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0094
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0093
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0091
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0090
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0091
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0090
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0089
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0090
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0093
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0070
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0070
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0072
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0068
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0088
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0091
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.3322 Time(s/epoch):0.0066
Epoch: 002, Supervised Loss:1.2726 Time(s/epoch):0.0065
Epoch: 003, Supervised Loss:1.2222 Time(s/epoch):0.0060
Epoch: 004, Supervised Loss:1.2074 Time(s/epoch):0.0052
Epoch: 005, Supervised Loss:1.1940 Time(s/epoch):0.0046
Epoch: 006, Supervised Loss:1.1774 Time(s/epoch):0.0047
Epoch: 007, Supervised Loss:1.1751 Time(s/epoch):0.0047
Epoch: 008, Supervised Loss:1.1926 Time(s/epoch):0.0044
Epoch: 009, Supervised Loss:1.2237 Time(s/epoch):0.0054
Epoch: 010, Supervised Loss:1.2610 Time(s/epoch):0.0062
Epoch: 011, Supervised Loss:1.3006 Time(s/epoch):0.0060
Epoch: 012, Supervised Loss:1.3341 Time(s/epoch):0.0060
Epoch: 013, Supervised Loss:1.3535 Time(s/epoch):0.0060
Epoch: 014, Supervised Loss:1.3588 Time(s/epoch):0.0060
Epoch: 015, Supervised Loss:1.3555 Time(s/epoch):0.0059
Epoch: 016, Supervised Loss:1.3487 Time(s/epoch):0.0058
Epoch: 017, Supervised Loss:1.3410 Time(s/epoch):0.0061
Epoch: 018, Supervised Loss:1.3336 Time(s/epoch):0.0060
Epoch: 019, Supervised Loss:1.3272 Time(s/epoch):0.0062
Epoch: 020, Supervised Loss:1.3232 Time(s/epoch):0.0047
Epoch: 021, Supervised Loss:1.3221 Time(s/epoch):0.0049
Epoch: 022, Supervised Loss:1.3228 Time(s/epoch):0.0047
Epoch: 023, Supervised Loss:1.3225 Time(s/epoch):0.0064
Epoch: 024, Supervised Loss:1.3198 Time(s/epoch):0.0063
Epoch: 025, Supervised Loss:1.3150 Time(s/epoch):0.0062
Epoch: 026, Supervised Loss:1.3102 Time(s/epoch):0.0060
Epoch: 027, Supervised Loss:1.3065 Time(s/epoch):0.0058
Epoch: 028, Supervised Loss:1.3035 Time(s/epoch):0.0062
Epoch: 029, Supervised Loss:1.3002 Time(s/epoch):0.0061
Epoch: 030, Supervised Loss:1.2956 Time(s/epoch):0.0061
0.8617705 515
Add 404 edges.
Prune 366 edges from torch.Size([2, 515]) to torch.Size([2, 149])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.746
Data(x=[251, 1703], edge_index=[2, 548], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.2991 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.746
Data(x=[251, 1703], edge_index=[2, 548], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4886 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.746
Data(x=[251, 1703], edge_index=[2, 548], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5587 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(19)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0144
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0082
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0119
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0083
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0123
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0096
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0101
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0099
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0084
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0098
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0097
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0088
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0084
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0077
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0077
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0080
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0094
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0100
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0086
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0093
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0092
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0074
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0073
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0075
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0075
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0076
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0074
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0098
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0095
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0094
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0097
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0095
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0098
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0102
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0099
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0097
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0096
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0098
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0096
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0098
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0100
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0105
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0104
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0102
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0109
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0103
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0079
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0069
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0078
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0072
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0087
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0093
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0095
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0095
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0094
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0080
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0076
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0086
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0095
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0096
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0099
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0082
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0083
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0097
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0081
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0071
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0073
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0071
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0086
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0097
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0095
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0093
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0093
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0100
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0095
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0079
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0074
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0074
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0072
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0076
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0073
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0073
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0071
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0077
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0092
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0085
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0071
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0073
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0072
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0072
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0068
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0072
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0077
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0078
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0070
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0071
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0073
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0071
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0073
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0071
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0070
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0071
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0076
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0097
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0100
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0098
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0098
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0081
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0075
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0078
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0098
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0097
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0094
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0095
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0098
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0099
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0087
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0074
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0073
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0071
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0075
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0075
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0081
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0093
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0080
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0072
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0082
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0097
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0096
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0096
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0093
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0093
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0093
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0078
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0071
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0081
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0075
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0076
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0075
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0092
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0100[2025-04-01 02:43:01,509]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:01,519]: Epoch: 001, Loss:1.7420 Train: 0.5750, Val:0.4250, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:01,527]: Epoch: 002, Loss:1.2137 Train: 0.8083, Val:0.6000, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:01,536]: Epoch: 003, Loss:0.7876 Train: 0.8333, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0082
[2025-04-01 02:43:01,543]: Epoch: 004, Loss:0.5965 Train: 0.8667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:01,552]: Epoch: 005, Loss:0.5391 Train: 0.8667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0089
[2025-04-01 02:43:01,559]: Epoch: 006, Loss:0.4427 Train: 0.8917, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0071
[2025-04-01 02:43:01,567]: Epoch: 007, Loss:0.3709 Train: 0.8833, Val:0.6250, Test: 0.7255, Time(s/epoch):0.0075
[2025-04-01 02:43:01,574]: Epoch: 008, Loss:0.3345 Train: 0.9083, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0074
[2025-04-01 02:43:01,581]: Epoch: 009, Loss:0.3183 Train: 0.9333, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0069
[2025-04-01 02:43:01,588]: Epoch: 010, Loss:0.2329 Train: 0.9417, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0065
[2025-04-01 02:43:01,595]: Epoch: 011, Loss:0.2440 Train: 0.9500, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0071
[2025-04-01 02:43:01,602]: Epoch: 012, Loss:0.2505 Train: 0.9500, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0064
[2025-04-01 02:43:01,609]: Epoch: 013, Loss:0.2456 Train: 0.9500, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0064
[2025-04-01 02:43:01,617]: Epoch: 014, Loss:0.2622 Train: 0.9500, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0080
[2025-04-01 02:43:01,623]: Epoch: 015, Loss:0.2780 Train: 0.9500, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:01,632]: Epoch: 016, Loss:0.1909 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0081
[2025-04-01 02:43:01,638]: Epoch: 017, Loss:0.1953 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:01,645]: Epoch: 018, Loss:0.1824 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0068
[2025-04-01 02:43:01,652]: Epoch: 019, Loss:0.2090 Train: 0.9583, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:01,659]: Epoch: 020, Loss:0.1842 Train: 0.9500, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0064
[2025-04-01 02:43:01,668]: Epoch: 021, Loss:0.2490 Train: 0.9500, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0090
[2025-04-01 02:43:01,677]: Epoch: 022, Loss:0.1835 Train: 0.9417, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0086
[2025-04-01 02:43:01,686]: Epoch: 023, Loss:0.1546 Train: 0.9417, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0088
[2025-04-01 02:43:01,694]: Epoch: 024, Loss:0.1337 Train: 0.9417, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:01,702]: Epoch: 025, Loss:0.1337 Train: 0.9333, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:01,710]: Epoch: 026, Loss:0.1753 Train: 0.9500, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:01,716]: Epoch: 027, Loss:0.1688 Train: 0.9583, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0059
[2025-04-01 02:43:01,723]: Epoch: 028, Loss:0.1753 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0069
[2025-04-01 02:43:01,729]: Epoch: 029, Loss:0.1399 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0057
[2025-04-01 02:43:01,737]: Epoch: 030, Loss:0.1448 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:01,745]: Epoch: 031, Loss:0.1715 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:01,752]: Epoch: 032, Loss:0.1437 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:01,760]: Epoch: 033, Loss:0.1316 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:01,768]: Epoch: 034, Loss:0.1172 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:01,777]: Epoch: 035, Loss:0.1334 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:01,784]: Epoch: 036, Loss:0.1874 Train: 0.9500, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:01,792]: Epoch: 037, Loss:0.1686 Train: 0.9500, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:01,799]: Epoch: 038, Loss:0.1496 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:01,806]: Epoch: 039, Loss:0.1722 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:01,813]: Epoch: 040, Loss:0.1417 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0072
[2025-04-01 02:43:01,822]: Epoch: 041, Loss:0.1278 Train: 0.9667, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:01,828]: Epoch: 042, Loss:0.1420 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0061
[2025-04-01 02:43:01,835]: Epoch: 043, Loss:0.1412 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:01,842]: Epoch: 044, Loss:0.1293 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:01,850]: Epoch: 045, Loss:0.1280 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0078
[2025-04-01 02:43:01,857]: Epoch: 046, Loss:0.1433 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:01,864]: Epoch: 047, Loss:0.1414 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:01,872]: Epoch: 048, Loss:0.1483 Train: 0.9667, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:01,879]: Epoch: 049, Loss:0.1315 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:01,886]: Epoch: 050, Loss:0.1208 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0071
[2025-04-01 02:43:01,893]: Epoch: 051, Loss:0.1337 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0072
[2025-04-01 02:43:01,899]: Epoch: 052, Loss:0.1258 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:01,908]: Epoch: 053, Loss:0.1354 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0088
[2025-04-01 02:43:01,917]: Epoch: 054, Loss:0.1636 Train: 0.9583, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:01,923]: Epoch: 055, Loss:0.1734 Train: 0.9583, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:01,930]: Epoch: 056, Loss:0.1204 Train: 0.9583, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:01,938]: Epoch: 057, Loss:0.1574 Train: 0.9750, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0075
[2025-04-01 02:43:01,945]: Epoch: 058, Loss:0.0965 Train: 0.9667, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:01,954]: Epoch: 059, Loss:0.1580 Train: 0.9583, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0084
[2025-04-01 02:43:01,962]: Epoch: 060, Loss:0.1901 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:01,970]: Epoch: 061, Loss:0.1320 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:01,978]: Epoch: 062, Loss:0.1374 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:01,986]: Epoch: 063, Loss:0.1224 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:01,994]: Epoch: 064, Loss:0.1355 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:02,001]: Epoch: 065, Loss:0.1534 Train: 0.9583, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0069
[2025-04-01 02:43:02,007]: Epoch: 066, Loss:0.1211 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:02,014]: Epoch: 067, Loss:0.1209 Train: 0.9667, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:02,022]: Epoch: 068, Loss:0.1125 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0074
[2025-04-01 02:43:02,030]: Epoch: 069, Loss:0.1420 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:02,037]: Epoch: 070, Loss:0.1194 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:02,045]: Epoch: 071, Loss:0.1528 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:02,052]: Epoch: 072, Loss:0.1371 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:02,060]: Epoch: 073, Loss:0.1204 Train: 0.9833, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0079
[2025-04-01 02:43:02,068]: Epoch: 074, Loss:0.1001 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:02,076]: Epoch: 075, Loss:0.0891 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:02,084]: Epoch: 076, Loss:0.1048 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,093]: Epoch: 077, Loss:0.1184 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0084
[2025-04-01 02:43:02,101]: Epoch: 078, Loss:0.1151 Train: 0.9750, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:02,109]: Epoch: 079, Loss:0.1314 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:02,117]: Epoch: 080, Loss:0.1609 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:02,124]: Epoch: 081, Loss:0.1510 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0073
[2025-04-01 02:43:02,132]: Epoch: 082, Loss:0.1334 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:02,140]: Epoch: 083, Loss:0.1293 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:02,146]: Epoch: 084, Loss:0.1247 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:02,154]: Epoch: 085, Loss:0.1264 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:02,161]: Epoch: 086, Loss:0.1065 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:02,168]: Epoch: 087, Loss:0.0973 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0068
[2025-04-01 02:43:02,177]: Epoch: 088, Loss:0.1004 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0095
[2025-04-01 02:43:02,186]: Epoch: 089, Loss:0.1087 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0086
[2025-04-01 02:43:02,194]: Epoch: 090, Loss:0.1455 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:02,201]: Epoch: 091, Loss:0.1282 Train: 0.9833, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0076
[2025-04-01 02:43:02,208]: Epoch: 092, Loss:0.1195 Train: 0.9667, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0068
[2025-04-01 02:43:02,215]: Epoch: 093, Loss:0.1295 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0066
[2025-04-01 02:43:02,222]: Epoch: 094, Loss:0.1385 Train: 0.9833, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0071
[2025-04-01 02:43:02,229]: Epoch: 095, Loss:0.0959 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0059
[2025-04-01 02:43:02,236]: Epoch: 096, Loss:0.1108 Train: 0.9750, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0077
[2025-04-01 02:43:02,244]: Epoch: 097, Loss:0.1195 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:02,253]: Epoch: 098, Loss:0.0911 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:02,259]: Epoch: 099, Loss:0.1909 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:02,267]: Epoch: 100, Loss:0.1299 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:02,275]: Epoch: 101, Loss:0.0968 Train: 0.9833, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0081
[2025-04-01 02:43:02,283]: Epoch: 102, Loss:0.1050 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0077
[2025-04-01 02:43:02,291]: Epoch: 103, Loss:0.1535 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:02,298]: Epoch: 104, Loss:0.1001 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0068
[2025-04-01 02:43:02,306]: Epoch: 105, Loss:0.1307 Train: 0.9917, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:02,312]: Epoch: 106, Loss:0.1184 Train: 0.9917, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0062
[2025-04-01 02:43:02,318]: Epoch: 107, Loss:0.1230 Train: 0.9917, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:02,326]: Epoch: 108, Loss:0.1141 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:02,334]: Epoch: 109, Loss:0.1084 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:02,340]: Epoch: 110, Loss:0.1197 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0057
[2025-04-01 02:43:02,348]: Epoch: 111, Loss:0.0981 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0071
[2025-04-01 02:43:02,355]: Epoch: 112, Loss:0.1161 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:02,361]: Epoch: 113, Loss:0.1082 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:02,369]: Epoch: 114, Loss:0.1084 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:02,377]: Epoch: 115, Loss:0.1064 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:02,385]: Epoch: 116, Loss:0.1016 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:02,393]: Epoch: 117, Loss:0.1192 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:02,399]: Epoch: 118, Loss:0.1167 Train: 0.9667, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:02,407]: Epoch: 119, Loss:0.1248 Train: 0.9583, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0079
[2025-04-01 02:43:02,414]: Epoch: 120, Loss:0.1282 Train: 0.9667, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0067
[2025-04-01 02:43:02,422]: Epoch: 121, Loss:0.1205 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0070
[2025-04-01 02:43:02,428]: Epoch: 122, Loss:0.1028 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:02,435]: Epoch: 123, Loss:0.0838 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0069
[2025-04-01 02:43:02,443]: Epoch: 124, Loss:0.1053 Train: 0.9667, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0074
[2025-04-01 02:43:02,451]: Epoch: 125, Loss:0.1459 Train: 0.9833, Val:0.6375, Test: 0.8431, Time(s/epoch):0.0076
[2025-04-01 02:43:02,457]: Epoch: 126, Loss:0.0934 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0066
[2025-04-01 02:43:02,464]: Epoch: 127, Loss:0.1112 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0068
[2025-04-01 02:43:02,473]: Epoch: 128, Loss:0.1045 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0090
[2025-04-01 02:43:02,480]: Epoch: 129, Loss:0.0978 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:02,487]: Epoch: 130, Loss:0.1091 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:02,494]: Epoch: 131, Loss:0.0938 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0065
[2025-04-01 02:43:02,501]: Epoch: 132, Loss:0.0972 Train: 0.9833, Val:0.6750, Test: 0.7451, Time(s/epoch):0.0075
[2025-04-01 02:43:02,509]: Epoch: 133, Loss:0.1173 Train: 0.9917, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:02,516]: Epoch: 134, Loss:0.0846 Train: 0.9750, Val:0.7000, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:02,524]: Epoch: 135, Loss:0.1099 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,532]: Epoch: 136, Loss:0.1027 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:02,538]: Epoch: 137, Loss:0.1091 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:02,546]: Epoch: 138, Loss:0.1232 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,553]: Epoch: 139, Loss:0.0966 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:02,560]: Epoch: 140, Loss:0.1139 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:02,567]: Epoch: 141, Loss:0.0878 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:02,573]: Epoch: 142, Loss:0.0957 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:02,582]: Epoch: 143, Loss:0.1039 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0080
[2025-04-01 02:43:02,589]: Epoch: 144, Loss:0.1028 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0076
[2025-04-01 02:43:02,597]: Epoch: 145, Loss:0.1420 Train: 0.9583, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:02,604]: Epoch: 146, Loss:0.1351 Train: 0.9750, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:02,612]: Epoch: 147, Loss:0.1199 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,618]: Epoch: 148, Loss:0.1299 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0059
[2025-04-01 02:43:02,625]: Epoch: 149, Loss:0.1340 Train: 0.9833, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0072
[2025-04-01 02:43:02,633]: Epoch: 150, Loss:0.1384 Train: 0.9833, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:02,639]: Epoch: 151, Loss:0.1002 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0056
[2025-04-01 02:43:02,646]: Epoch: 152, Loss:0.1174 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:02,654]: Epoch: 153, Loss:0.1065 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:02,662]: Epoch: 154, Loss:0.1327 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,668]: Epoch: 155, Loss:0.0898 Train: 0.9833, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0058
[2025-04-01 02:43:02,676]: Epoch: 156, Loss:0.1187 Train: 0.9750, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0074
[2025-04-01 02:43:02,682]: Epoch: 157, Loss:0.1239 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:02,689]: Epoch: 158, Loss:0.1045 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0064
[2025-04-01 02:43:02,697]: Epoch: 159, Loss:0.1093 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:02,705]: Epoch: 160, Loss:0.1178 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0085
[2025-04-01 02:43:02,712]: Epoch: 161, Loss:0.1056 Train: 0.9833, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0062
[2025-04-01 02:43:02,719]: Epoch: 162, Loss:0.1140 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:02,727]: Epoch: 163, Loss:0.1267 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:02,735]: Epoch: 164, Loss:0.0949 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:02,743]: Epoch: 165, Loss:0.0951 Train: 0.9833, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:02,751]: Epoch: 166, Loss:0.1060 Train: 0.9833, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:02,759]: Epoch: 167, Loss:0.0995 Train: 0.9750, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0078
[2025-04-01 02:43:02,766]: Epoch: 168, Loss:0.0881 Train: 0.9667, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0070
[2025-04-01 02:43:02,773]: Epoch: 169, Loss:0.1258 Train: 0.9667, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:02,779]: Epoch: 170, Loss:0.1305 Train: 0.9750, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0059
[2025-04-01 02:43:02,787]: Epoch: 171, Loss:0.1089 Train: 0.9750, Val:0.7000, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:02,794]: Epoch: 172, Loss:0.1160 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:02,803]: Epoch: 173, Loss:0.0980 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0084
[2025-04-01 02:43:02,810]: Epoch: 174, Loss:0.1062 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:02,818]: Epoch: 175, Loss:0.0996 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:02,826]: Epoch: 176, Loss:0.0716 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:02,834]: Epoch: 177, Loss:0.1515 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:02,842]: Epoch: 178, Loss:0.1175 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:02,850]: Epoch: 179, Loss:0.0957 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0071
[2025-04-01 02:43:02,858]: Epoch: 180, Loss:0.1015 Train: 0.9833, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0078
[2025-04-01 02:43:02,865]: Epoch: 181, Loss:0.1156 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:02,873]: Epoch: 182, Loss:0.1151 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0079
[2025-04-01 02:43:02,880]: Epoch: 183, Loss:0.1050 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0069
[2025-04-01 02:43:02,887]: Epoch: 184, Loss:0.1150 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0065
[2025-04-01 02:43:02,895]: Epoch: 185, Loss:0.0994 Train: 0.9667, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:02,903]: Epoch: 186, Loss:0.1023 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0083
[2025-04-01 02:43:02,909]: Epoch: 187, Loss:0.1201 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0062
[2025-04-01 02:43:02,918]: Epoch: 188, Loss:0.1024 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0081
[2025-04-01 02:43:02,925]: Epoch: 189, Loss:0.0761 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:02,931]: Epoch: 190, Loss:0.1118 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0060
[2025-04-01 02:43:02,938]: Epoch: 191, Loss:0.1276 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:02,946]: Epoch: 192, Loss:0.1396 Train: 0.9917, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:02,953]: Epoch: 193, Loss:0.1228 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:02,960]: Epoch: 194, Loss:0.1185 Train: 0.9667, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:02,966]: Epoch: 195, Loss:0.1463 Train: 0.9583, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0062
[2025-04-01 02:43:02,974]: Epoch: 196, Loss:0.2181 Train: 0.9583, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:02,981]: Epoch: 197, Loss:0.1192 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:02,988]: Epoch: 198, Loss:0.1050 Train: 0.9750, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:02,996]: Epoch: 199, Loss:0.1021 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0074
[2025-04-01 02:43:03,003]: Epoch: 200, Loss:0.1845 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0063
[2025-04-01 02:43:03,003]: [Run-1 score] {'train': 0.975, 'val': 0.7, 'test': 0.7843137254901961}
[2025-04-01 02:43:03,003]: repeat 2/3
[2025-04-01 02:43:03,003]: Manual random seed:0
[2025-04-01 02:43:03,003]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:03,007]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:03,017]: Epoch: 001, Loss:1.6326 Train: 0.7250, Val:0.5875, Test: 0.7647, Time(s/epoch):0.0064
[2025-04-01 02:43:03,025]: Epoch: 002, Loss:0.9025 Train: 0.8417, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:03,031]: Epoch: 003, Loss:0.6313 Train: 0.8833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:43:03,039]: Epoch: 004, Loss:0.4665 Train: 0.9083, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:03,047]: Epoch: 005, Loss:0.3955 Train: 0.9083, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:03,054]: Epoch: 006, Loss:0.3771 Train: 0.9167, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0069
[2025-04-01 02:43:03,062]: Epoch: 007, Loss:0.3634 Train: 0.9333, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:03,070]: Epoch: 008, Loss:0.3108 Train: 0.9250, Val:0.6000, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:03,078]: Epoch: 009, Loss:0.2611 Train: 0.9333, Val:0.6125, Test: 0.7647, Time(s/epoch):0.0075
[2025-04-01 02:43:03,085]: Epoch: 010, Loss:0.2298 Train: 0.9333, Val:0.6250, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:03,092]: Epoch: 011, Loss:0.2701 Train: 0.9417, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0061
[2025-04-01 02:43:03,101]: Epoch: 012, Loss:0.2445 Train: 0.9500, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0087
[2025-04-01 02:43:03,107]: Epoch: 013, Loss:0.2128 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0067
[2025-04-01 02:43:03,114]: Epoch: 014, Loss:0.2000 Train: 0.9583, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0069
[2025-04-01 02:43:03,123]: Epoch: 015, Loss:0.1581 Train: 0.9500, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0079
[2025-04-01 02:43:03,130]: Epoch: 016, Loss:0.2089 Train: 0.9583, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0072
[2025-04-01 02:43:03,137]: Epoch: 017, Loss:0.1955 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:03,145]: Epoch: 018, Loss:0.1396 Train: 0.9583, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0069
[2025-04-01 02:43:03,152]: Epoch: 019, Loss:0.1945 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:03,159]: Epoch: 020, Loss:0.1520 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:03,167]: Epoch: 021, Loss:0.1676 Train: 0.9583, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0085
[2025-04-01 02:43:03,176]: Epoch: 022, Loss:0.1620 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0084
[2025-04-01 02:43:03,184]: Epoch: 023, Loss:0.2140 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0083
[2025-04-01 02:43:03,191]: Epoch: 024, Loss:0.1333 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0069
[2025-04-01 02:43:03,199]: Epoch: 025, Loss:0.1803 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:03,207]: Epoch: 026, Loss:0.1464 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0083
[2025-04-01 02:43:03,214]: Epoch: 027, Loss:0.1154 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:03,222]: Epoch: 028, Loss:0.1231 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:03,229]: Epoch: 029, Loss:0.1279 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0072
[2025-04-01 02:43:03,237]: Epoch: 030, Loss:0.1514 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:03,245]: Epoch: 031, Loss:0.1461 Train: 0.9917, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0075
[2025-04-01 02:43:03,253]: Epoch: 032, Loss:0.1302 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0082
[2025-04-01 02:43:03,259]: Epoch: 033, Loss:0.1262 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0059
[2025-04-01 02:43:03,267]: Epoch: 034, Loss:0.1295 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:03,274]: Epoch: 035, Loss:0.1556 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:03,280]: Epoch: 036, Loss:0.1268 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0062
[2025-04-01 02:43:03,288]: Epoch: 037, Loss:0.1259 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0071
[2025-04-01 02:43:03,294]: Epoch: 038, Loss:0.1303 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:03,301]: Epoch: 039, Loss:0.1176 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0067
[2025-04-01 02:43:03,308]: Epoch: 040, Loss:0.1199 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0067
[2025-04-01 02:43:03,315]: Epoch: 041, Loss:0.1314 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0070
[2025-04-01 02:43:03,323]: Epoch: 042, Loss:0.1466 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:03,329]: Epoch: 043, Loss:0.1092 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:03,336]: Epoch: 044, Loss:0.1222 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:03,344]: Epoch: 045, Loss:0.1574 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:03,352]: Epoch: 046, Loss:0.1699 Train: 0.9667, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0081
[2025-04-01 02:43:03,360]: Epoch: 047, Loss:0.1132 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0073
[2025-04-01 02:43:03,367]: Epoch: 048, Loss:0.1173 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:03,375]: Epoch: 049, Loss:0.1271 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:03,383]: Epoch: 050, Loss:0.1287 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:03,390]: Epoch: 051, Loss:0.1322 Train: 0.9750, Val:0.6250, Test: 0.7843, Time(s/epoch):0.0061
[2025-04-01 02:43:03,398]: Epoch: 052, Loss:0.1687 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:03,405]: Epoch: 053, Loss:0.1405 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:03,413]: Epoch: 054, Loss:0.1574 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:03,419]: Epoch: 055, Loss:0.1378 Train: 0.9750, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0058
[2025-04-01 02:43:03,427]: Epoch: 056, Loss:0.1410 Train: 0.9833, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0078
[2025-04-01 02:43:03,434]: Epoch: 057, Loss:0.1289 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0071
[2025-04-01 02:43:03,442]: Epoch: 058, Loss:0.1310 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:03,448]: Epoch: 059, Loss:0.1328 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0059
[2025-04-01 02:43:03,455]: Epoch: 060, Loss:0.1379 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0072
[2025-04-01 02:43:03,462]: Epoch: 061, Loss:0.1233 Train: 0.9750, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0068
[2025-04-01 02:43:03,478]: Epoch: 062, Loss:0.1166 Train: 0.9833, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0162
[2025-04-01 02:43:03,495]: Epoch: 063, Loss:0.1171 Train: 0.9833, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0165
[2025-04-01 02:43:03,504]: Epoch: 064, Loss:0.1214 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0086
[2025-04-01 02:43:03,512]: Epoch: 065, Loss:0.1083 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:03,520]: Epoch: 066, Loss:0.1521 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:03,528]: Epoch: 067, Loss:0.1492 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:03,536]: Epoch: 068, Loss:0.1094 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:03,543]: Epoch: 069, Loss:0.1013 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0070
[2025-04-01 02:43:03,552]: Epoch: 070, Loss:0.0953 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0085
[2025-04-01 02:43:03,560]: Epoch: 071, Loss:0.1449 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:03,568]: Epoch: 072, Loss:0.1262 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:03,576]: Epoch: 073, Loss:0.1051 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:03,584]: Epoch: 074, Loss:0.1756 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:03,592]: Epoch: 075, Loss:0.0989 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:03,601]: Epoch: 076, Loss:0.1160 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0085
[2025-04-01 02:43:03,609]: Epoch: 077, Loss:0.1375 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:03,618]: Epoch: 078, Loss:0.1064 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0087
[2025-04-01 02:43:03,626]: Epoch: 079, Loss:0.1131 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0082
[2025-04-01 02:43:03,633]: Epoch: 080, Loss:0.1201 Train: 0.9583, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0062
[2025-04-01 02:43:03,639]: Epoch: 081, Loss:0.1343 Train: 0.9667, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0059
[2025-04-01 02:43:03,646]: Epoch: 082, Loss:0.1692 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0069
[2025-04-01 02:43:03,653]: Epoch: 083, Loss:0.1583 Train: 0.9833, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:03,661]: Epoch: 084, Loss:0.1279 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:03,668]: Epoch: 085, Loss:0.1272 Train: 0.9750, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0074
[2025-04-01 02:43:03,677]: Epoch: 086, Loss:0.1422 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:03,685]: Epoch: 087, Loss:0.1302 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0083
[2025-04-01 02:43:03,693]: Epoch: 088, Loss:0.1211 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0079
[2025-04-01 02:43:03,699]: Epoch: 089, Loss:0.0914 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0055
[2025-04-01 02:43:03,706]: Epoch: 090, Loss:0.1142 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0070
[2025-04-01 02:43:03,713]: Epoch: 091, Loss:0.0967 Train: 0.9667, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0069
[2025-04-01 02:43:03,720]: Epoch: 092, Loss:0.1181 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:03,727]: Epoch: 093, Loss:0.1144 Train: 0.9750, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0070
[2025-04-01 02:43:03,735]: Epoch: 094, Loss:0.1283 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:03,742]: Epoch: 095, Loss:0.0951 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0060
[2025-04-01 02:43:03,748]: Epoch: 096, Loss:0.1009 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:03,756]: Epoch: 097, Loss:0.1325 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:03,764]: Epoch: 098, Loss:0.0992 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:03,771]: Epoch: 099, Loss:0.1260 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0069
[2025-04-01 02:43:03,778]: Epoch: 100, Loss:0.0878 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0070
[2025-04-01 02:43:03,786]: Epoch: 101, Loss:0.1237 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0078
[2025-04-01 02:43:03,793]: Epoch: 102, Loss:0.1312 Train: 0.9583, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:03,801]: Epoch: 103, Loss:0.1666 Train: 0.9833, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:03,808]: Epoch: 104, Loss:0.0854 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0077
[2025-04-01 02:43:03,815]: Epoch: 105, Loss:0.0871 Train: 0.9833, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0064
[2025-04-01 02:43:03,822]: Epoch: 106, Loss:0.0966 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:03,829]: Epoch: 107, Loss:0.1140 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:03,836]: Epoch: 108, Loss:0.1061 Train: 0.9833, Val:0.6750, Test: 0.7647, Time(s/epoch):0.0073
[2025-04-01 02:43:03,844]: Epoch: 109, Loss:0.1139 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:03,851]: Epoch: 110, Loss:0.0958 Train: 0.9750, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0064
[2025-04-01 02:43:03,859]: Epoch: 111, Loss:0.0908 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0080
[2025-04-01 02:43:03,866]: Epoch: 112, Loss:0.1187 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:03,873]: Epoch: 113, Loss:0.1062 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:03,881]: Epoch: 114, Loss:0.1286 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:03,889]: Epoch: 115, Loss:0.0846 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:03,896]: Epoch: 116, Loss:0.1169 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0063
[2025-04-01 02:43:03,903]: Epoch: 117, Loss:0.1118 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0069
[2025-04-01 02:43:03,909]: Epoch: 118, Loss:0.1317 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:03,917]: Epoch: 119, Loss:0.0958 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:03,923]: Epoch: 120, Loss:0.1272 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0062
[2025-04-01 02:43:03,929]: Epoch: 121, Loss:0.0887 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0059
[2025-04-01 02:43:03,937]: Epoch: 122, Loss:0.0911 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0080
[2025-04-01 02:43:03,945]: Epoch: 123, Loss:0.1039 Train: 0.9833, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:03,954]: Epoch: 124, Loss:0.1087 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:03,960]: Epoch: 125, Loss:0.1193 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:03,967]: Epoch: 126, Loss:0.1206 Train: 0.9583, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0065
[2025-04-01 02:43:03,975]: Epoch: 127, Loss:0.1209 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:03,982]: Epoch: 128, Loss:0.1122 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:03,989]: Epoch: 129, Loss:0.1559 Train: 0.9833, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0071
[2025-04-01 02:43:03,997]: Epoch: 130, Loss:0.1060 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:04,005]: Epoch: 131, Loss:0.0856 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:04,012]: Epoch: 132, Loss:0.1093 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:04,019]: Epoch: 133, Loss:0.1263 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:04,026]: Epoch: 134, Loss:0.1112 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0072
[2025-04-01 02:43:04,033]: Epoch: 135, Loss:0.0916 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0069
[2025-04-01 02:43:04,041]: Epoch: 136, Loss:0.1350 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:04,049]: Epoch: 137, Loss:0.1205 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:04,056]: Epoch: 138, Loss:0.1135 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:04,063]: Epoch: 139, Loss:0.1003 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:04,070]: Epoch: 140, Loss:0.1146 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:04,078]: Epoch: 141, Loss:0.0996 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:04,085]: Epoch: 142, Loss:0.1042 Train: 0.9833, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:04,093]: Epoch: 143, Loss:0.1255 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:04,100]: Epoch: 144, Loss:0.1086 Train: 0.9750, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:04,108]: Epoch: 145, Loss:0.1130 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:04,115]: Epoch: 146, Loss:0.0867 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:04,121]: Epoch: 147, Loss:0.1328 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0061
[2025-04-01 02:43:04,128]: Epoch: 148, Loss:0.1078 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:04,135]: Epoch: 149, Loss:0.1154 Train: 0.9667, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:04,145]: Epoch: 150, Loss:0.1467 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0093
[2025-04-01 02:43:04,153]: Epoch: 151, Loss:0.1475 Train: 0.9667, Val:0.7000, Test: 0.8235, Time(s/epoch):0.0081
[2025-04-01 02:43:04,159]: Epoch: 152, Loss:0.1191 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0061
[2025-04-01 02:43:04,168]: Epoch: 153, Loss:0.1008 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0086
[2025-04-01 02:43:04,179]: Epoch: 154, Loss:0.1250 Train: 0.9750, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0102
[2025-04-01 02:43:04,188]: Epoch: 155, Loss:0.1684 Train: 0.9750, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0093
[2025-04-01 02:43:04,196]: Epoch: 156, Loss:0.0959 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:04,204]: Epoch: 157, Loss:0.1049 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0083
[2025-04-01 02:43:04,213]: Epoch: 158, Loss:0.1270 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0084
[2025-04-01 02:43:04,221]: Epoch: 159, Loss:0.1665 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:04,228]: Epoch: 160, Loss:0.1168 Train: 0.9417, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:04,235]: Epoch: 161, Loss:0.1975 Train: 0.9583, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0063
[2025-04-01 02:43:04,246]: Epoch: 162, Loss:0.1301 Train: 0.9583, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0109
[2025-04-01 02:43:04,267]: Epoch: 163, Loss:0.1334 Train: 0.9833, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0205
[2025-04-01 02:43:04,276]: Epoch: 164, Loss:0.1452 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0092
[2025-04-01 02:43:04,285]: Epoch: 165, Loss:0.1184 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0090
[2025-04-01 02:43:04,294]: Epoch: 166, Loss:0.0931 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0089
[2025-04-01 02:43:04,303]: Epoch: 167, Loss:0.2013 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0084
[2025-04-01 02:43:04,311]: Epoch: 168, Loss:0.1109 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0079
[2025-04-01 02:43:04,319]: Epoch: 169, Loss:0.0994 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:04,325]: Epoch: 170, Loss:0.1066 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0066
[2025-04-01 02:43:04,333]: Epoch: 171, Loss:0.1190 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0072
[2025-04-01 02:43:04,341]: Epoch: 172, Loss:0.1052 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0079
[2025-04-01 02:43:04,348]: Epoch: 173, Loss:0.0946 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:04,357]: Epoch: 174, Loss:0.1280 Train: 0.9750, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:04,363]: Epoch: 175, Loss:0.1760 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0065
[2025-04-01 02:43:04,369]: Epoch: 176, Loss:0.0905 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0059
[2025-04-01 02:43:04,376]: Epoch: 177, Loss:0.1042 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0065
[2025-04-01 02:43:04,383]: Epoch: 178, Loss:0.1584 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:04,390]: Epoch: 179, Loss:0.1452 Train: 0.9583, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:04,398]: Epoch: 180, Loss:0.1396 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:04,406]: Epoch: 181, Loss:0.1085 Train: 0.9667, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:04,414]: Epoch: 182, Loss:0.1509 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:04,421]: Epoch: 183, Loss:0.1444 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:04,429]: Epoch: 184, Loss:0.1316 Train: 0.9750, Val:0.7000, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:04,437]: Epoch: 185, Loss:0.0998 Train: 0.9750, Val:0.7000, Test: 0.7647, Time(s/epoch):0.0075
[2025-04-01 02:43:04,445]: Epoch: 186, Loss:0.1092 Train: 0.9833, Val:0.7000, Test: 0.7451, Time(s/epoch):0.0082
[2025-04-01 02:43:04,453]: Epoch: 187, Loss:0.1163 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:04,459]: Epoch: 188, Loss:0.1107 Train: 0.9750, Val:0.6250, Test: 0.7451, Time(s/epoch):0.0062
[2025-04-01 02:43:04,468]: Epoch: 189, Loss:0.0881 Train: 0.9667, Val:0.6125, Test: 0.7451, Time(s/epoch):0.0088
[2025-04-01 02:43:04,476]: Epoch: 190, Loss:0.1400 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0075
[2025-04-01 02:43:04,482]: Epoch: 191, Loss:0.1288 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0063
[2025-04-01 02:43:04,489]: Epoch: 192, Loss:0.1333 Train: 0.9833, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:04,497]: Epoch: 193, Loss:0.0984 Train: 0.9667, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:04,504]: Epoch: 194, Loss:0.1268 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:04,510]: Epoch: 195, Loss:0.1164 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0061
[2025-04-01 02:43:04,518]: Epoch: 196, Loss:0.1001 Train: 0.9667, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0077
[2025-04-01 02:43:04,525]: Epoch: 197, Loss:0.1234 Train: 0.9667, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0063
[2025-04-01 02:43:04,531]: Epoch: 198, Loss:0.1359 Train: 0.9750, Val:0.6375, Test: 0.7451, Time(s/epoch):0.0064
[2025-04-01 02:43:04,539]: Epoch: 199, Loss:0.1089 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:04,546]: Epoch: 200, Loss:0.1147 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:04,546]: [Run-2 score] {'train': 0.9666666666666667, 'val': 0.7, 'test': 0.8235294117647058}
[2025-04-01 02:43:04,546]: repeat 3/3
[2025-04-01 02:43:04,546]: Manual random seed:0
[2025-04-01 02:43:04,546]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:04,551]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:04,562]: Epoch: 001, Loss:1.7117 Train: 0.7500, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0083
[2025-04-01 02:43:04,568]: Epoch: 002, Loss:1.0454 Train: 0.8333, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0056
[2025-04-01 02:43:04,576]: Epoch: 003, Loss:0.7307 Train: 0.8750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:04,584]: Epoch: 004, Loss:0.5088 Train: 0.9083, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:04,591]: Epoch: 005, Loss:0.4301 Train: 0.8917, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:04,599]: Epoch: 006, Loss:0.3530 Train: 0.9083, Val:0.6250, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:04,607]: Epoch: 007, Loss:0.2839 Train: 0.9333, Val:0.6250, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:04,616]: Epoch: 008, Loss:0.2331 Train: 0.9417, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:04,624]: Epoch: 009, Loss:0.2903 Train: 0.9500, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0082
[2025-04-01 02:43:04,632]: Epoch: 010, Loss:0.2252 Train: 0.9417, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0077
[2025-04-01 02:43:04,640]: Epoch: 011, Loss:0.2244 Train: 0.9417, Val:0.6750, Test: 0.7647, Time(s/epoch):0.0083
[2025-04-01 02:43:04,648]: Epoch: 012, Loss:0.2052 Train: 0.9417, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:04,657]: Epoch: 013, Loss:0.2144 Train: 0.9417, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:04,665]: Epoch: 014, Loss:0.2301 Train: 0.9500, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:04,674]: Epoch: 015, Loss:0.2180 Train: 0.9500, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0088
[2025-04-01 02:43:04,682]: Epoch: 016, Loss:0.1979 Train: 0.9583, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:04,689]: Epoch: 017, Loss:0.1526 Train: 0.9583, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0063
[2025-04-01 02:43:04,696]: Epoch: 018, Loss:0.1727 Train: 0.9417, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:04,705]: Epoch: 019, Loss:0.1981 Train: 0.9583, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0087
[2025-04-01 02:43:04,711]: Epoch: 020, Loss:0.1667 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0062
[2025-04-01 02:43:04,719]: Epoch: 021, Loss:0.1664 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:04,728]: Epoch: 022, Loss:0.1308 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0087
[2025-04-01 02:43:04,735]: Epoch: 023, Loss:0.2023 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:04,744]: Epoch: 024, Loss:0.1305 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:04,752]: Epoch: 025, Loss:0.1477 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:04,759]: Epoch: 026, Loss:0.1708 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:04,767]: Epoch: 027, Loss:0.1649 Train: 0.9667, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:04,774]: Epoch: 028, Loss:0.1486 Train: 0.9750, Val:0.7000, Test: 0.8039, Time(s/epoch):0.0067
[2025-04-01 02:43:04,783]: Epoch: 029, Loss:0.1320 Train: 0.9667, Val:0.7000, Test: 0.7647, Time(s/epoch):0.0086
[2025-04-01 02:43:04,791]: Epoch: 030, Loss:0.1097 Train: 0.9667, Val:0.7000, Test: 0.7451, Time(s/epoch):0.0085
[2025-04-01 02:43:04,798]: Epoch: 031, Loss:0.1944 Train: 0.9667, Val:0.7000, Test: 0.7647, Time(s/epoch):0.0067
[2025-04-01 02:43:04,805]: Epoch: 032, Loss:0.1473 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0067
[2025-04-01 02:43:04,812]: Epoch: 033, Loss:0.1234 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:04,819]: Epoch: 034, Loss:0.1875 Train: 0.9583, Val:0.6750, Test: 0.8235, Time(s/epoch):0.0062
[2025-04-01 02:43:04,826]: Epoch: 035, Loss:0.1324 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:04,834]: Epoch: 036, Loss:0.1182 Train: 0.9667, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:04,840]: Epoch: 037, Loss:0.1241 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0057
[2025-04-01 02:43:04,847]: Epoch: 038, Loss:0.1266 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:04,854]: Epoch: 039, Loss:0.1603 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0063
[2025-04-01 02:43:04,861]: Epoch: 040, Loss:0.1645 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:04,868]: Epoch: 041, Loss:0.1136 Train: 0.9667, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0067
[2025-04-01 02:43:04,876]: Epoch: 042, Loss:0.1640 Train: 0.9667, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:04,884]: Epoch: 043, Loss:0.1780 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:04,891]: Epoch: 044, Loss:0.1330 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:04,899]: Epoch: 045, Loss:0.1570 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0076
[2025-04-01 02:43:04,905]: Epoch: 046, Loss:0.1285 Train: 0.9833, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0060
[2025-04-01 02:43:04,912]: Epoch: 047, Loss:0.1198 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:04,921]: Epoch: 048, Loss:0.1306 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:04,930]: Epoch: 049, Loss:0.1218 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0087
[2025-04-01 02:43:04,938]: Epoch: 050, Loss:0.1436 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:04,944]: Epoch: 051, Loss:0.1311 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0062
[2025-04-01 02:43:04,953]: Epoch: 052, Loss:0.1070 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0090
[2025-04-01 02:43:04,963]: Epoch: 053, Loss:0.1206 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0094
[2025-04-01 02:43:04,968]: Epoch: 054, Loss:0.1671 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0053
[2025-04-01 02:43:04,975]: Epoch: 055, Loss:0.1585 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:04,983]: Epoch: 056, Loss:0.1534 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0071
[2025-04-01 02:43:04,989]: Epoch: 057, Loss:0.1215 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0059
[2025-04-01 02:43:04,997]: Epoch: 058, Loss:0.1210 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:05,003]: Epoch: 059, Loss:0.1374 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0066
[2025-04-01 02:43:05,012]: Epoch: 060, Loss:0.1697 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0083
[2025-04-01 02:43:05,020]: Epoch: 061, Loss:0.1242 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0082
[2025-04-01 02:43:05,029]: Epoch: 062, Loss:0.1567 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:05,037]: Epoch: 063, Loss:0.1881 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:05,045]: Epoch: 064, Loss:0.1093 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,053]: Epoch: 065, Loss:0.1299 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0076
[2025-04-01 02:43:05,058]: Epoch: 066, Loss:0.1195 Train: 0.9667, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0057
[2025-04-01 02:43:05,066]: Epoch: 067, Loss:0.1296 Train: 0.9667, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0078
[2025-04-01 02:43:05,073]: Epoch: 068, Loss:0.1274 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:05,080]: Epoch: 069, Loss:0.1292 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0065
[2025-04-01 02:43:05,087]: Epoch: 070, Loss:0.1328 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0067
[2025-04-01 02:43:05,094]: Epoch: 071, Loss:0.1466 Train: 0.9583, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0070
[2025-04-01 02:43:05,102]: Epoch: 072, Loss:0.1184 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,110]: Epoch: 073, Loss:0.1104 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:05,119]: Epoch: 074, Loss:0.1346 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0086
[2025-04-01 02:43:05,126]: Epoch: 075, Loss:0.1431 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,133]: Epoch: 076, Loss:0.1066 Train: 0.9750, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0064
[2025-04-01 02:43:05,140]: Epoch: 077, Loss:0.1273 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0065
[2025-04-01 02:43:05,147]: Epoch: 078, Loss:0.1436 Train: 0.9750, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0071
[2025-04-01 02:43:05,155]: Epoch: 079, Loss:0.1963 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:05,163]: Epoch: 080, Loss:0.1339 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0084
[2025-04-01 02:43:05,172]: Epoch: 081, Loss:0.1298 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0089
[2025-04-01 02:43:05,181]: Epoch: 082, Loss:0.1086 Train: 0.9583, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:05,189]: Epoch: 083, Loss:0.1319 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:05,195]: Epoch: 084, Loss:0.1024 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:05,204]: Epoch: 085, Loss:0.1416 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:05,212]: Epoch: 086, Loss:0.1144 Train: 0.9667, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0084
[2025-04-01 02:43:05,219]: Epoch: 087, Loss:0.0885 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0067
[2025-04-01 02:43:05,228]: Epoch: 088, Loss:0.1696 Train: 0.9750, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0081
[2025-04-01 02:43:05,236]: Epoch: 089, Loss:0.1200 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:05,244]: Epoch: 090, Loss:0.1387 Train: 0.9750, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0082
[2025-04-01 02:43:05,252]: Epoch: 091, Loss:0.1600 Train: 0.9667, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0079
[2025-04-01 02:43:05,260]: Epoch: 092, Loss:0.1813 Train: 0.9750, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0076
[2025-04-01 02:43:05,268]: Epoch: 093, Loss:0.1361 Train: 0.9750, Val:0.6875, Test: 0.8039, Time(s/epoch):0.0073
[2025-04-01 02:43:05,276]: Epoch: 094, Loss:0.1221 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,283]: Epoch: 095, Loss:0.1503 Train: 0.9833, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0075
[2025-04-01 02:43:05,291]: Epoch: 096, Loss:0.1379 Train: 0.9833, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0079
[2025-04-01 02:43:05,298]: Epoch: 097, Loss:0.1144 Train: 0.9833, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0062
[2025-04-01 02:43:05,304]: Epoch: 098, Loss:0.1256 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0065
[2025-04-01 02:43:05,311]: Epoch: 099, Loss:0.1228 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0062
[2025-04-01 02:43:05,321]: Epoch: 100, Loss:0.1271 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0097
[2025-04-01 02:43:05,328]: Epoch: 101, Loss:0.0964 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:05,336]: Epoch: 102, Loss:0.1099 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:05,345]: Epoch: 103, Loss:0.1022 Train: 0.9833, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:05,353]: Epoch: 104, Loss:0.1251 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0078
[2025-04-01 02:43:05,360]: Epoch: 105, Loss:0.1281 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:05,367]: Epoch: 106, Loss:0.0981 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0068
[2025-04-01 02:43:05,374]: Epoch: 107, Loss:0.1130 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0070
[2025-04-01 02:43:05,383]: Epoch: 108, Loss:0.1140 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0086
[2025-04-01 02:43:05,391]: Epoch: 109, Loss:0.1082 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0077
[2025-04-01 02:43:05,399]: Epoch: 110, Loss:0.0970 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0074
[2025-04-01 02:43:05,405]: Epoch: 111, Loss:0.0973 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0063
[2025-04-01 02:43:05,414]: Epoch: 112, Loss:0.0825 Train: 0.9833, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0083
[2025-04-01 02:43:05,423]: Epoch: 113, Loss:0.0959 Train: 0.9833, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0088
[2025-04-01 02:43:05,429]: Epoch: 114, Loss:0.1192 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0057
[2025-04-01 02:43:05,437]: Epoch: 115, Loss:0.1150 Train: 0.9833, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0080
[2025-04-01 02:43:05,443]: Epoch: 116, Loss:0.1044 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0063
[2025-04-01 02:43:05,450]: Epoch: 117, Loss:0.1040 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0062
[2025-04-01 02:43:05,458]: Epoch: 118, Loss:0.1171 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:05,466]: Epoch: 119, Loss:0.1124 Train: 0.9917, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0080
[2025-04-01 02:43:05,474]: Epoch: 120, Loss:0.1324 Train: 0.9833, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0078
[2025-04-01 02:43:05,482]: Epoch: 121, Loss:0.1126 Train: 0.9667, Val:0.6875, Test: 0.8431, Time(s/epoch):0.0076
[2025-04-01 02:43:05,490]: Epoch: 122, Loss:0.2005 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0078
[2025-04-01 02:43:05,498]: Epoch: 123, Loss:0.1048 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0083
[2025-04-01 02:43:05,507]: Epoch: 124, Loss:0.1170 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:05,514]: Epoch: 125, Loss:0.1229 Train: 0.9667, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0077
[2025-04-01 02:43:05,521]: Epoch: 126, Loss:0.1285 Train: 0.9750, Val:0.6375, Test: 0.7647, Time(s/epoch):0.0067
[2025-04-01 02:43:05,528]: Epoch: 127, Loss:0.1221 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0067
[2025-04-01 02:43:05,536]: Epoch: 128, Loss:0.1150 Train: 0.9750, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0082
[2025-04-01 02:43:05,544]: Epoch: 129, Loss:0.1000 Train: 0.9750, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0075
[2025-04-01 02:43:05,553]: Epoch: 130, Loss:0.1151 Train: 0.9833, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0091
[2025-04-01 02:43:05,562]: Epoch: 131, Loss:0.1367 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:05,569]: Epoch: 132, Loss:0.1185 Train: 0.9833, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,578]: Epoch: 133, Loss:0.1501 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:05,586]: Epoch: 134, Loss:0.1167 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0083
[2025-04-01 02:43:05,593]: Epoch: 135, Loss:0.0910 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0069
[2025-04-01 02:43:05,600]: Epoch: 136, Loss:0.1074 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0066
[2025-04-01 02:43:05,608]: Epoch: 137, Loss:0.1170 Train: 0.9667, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:05,616]: Epoch: 138, Loss:0.1179 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0078
[2025-04-01 02:43:05,624]: Epoch: 139, Loss:0.0950 Train: 0.9583, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:05,632]: Epoch: 140, Loss:0.1483 Train: 0.9667, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,639]: Epoch: 141, Loss:0.1072 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:05,647]: Epoch: 142, Loss:0.1006 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0085
[2025-04-01 02:43:05,654]: Epoch: 143, Loss:0.1119 Train: 0.9833, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0064
[2025-04-01 02:43:05,664]: Epoch: 144, Loss:0.1129 Train: 0.9750, Val:0.6250, Test: 0.8039, Time(s/epoch):0.0098
[2025-04-01 02:43:05,672]: Epoch: 145, Loss:0.1192 Train: 0.9667, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0082
[2025-04-01 02:43:05,680]: Epoch: 146, Loss:0.1379 Train: 0.9750, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0075
[2025-04-01 02:43:05,688]: Epoch: 147, Loss:0.1623 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:05,697]: Epoch: 148, Loss:0.0953 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0087
[2025-04-01 02:43:05,705]: Epoch: 149, Loss:0.1029 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0081
[2025-04-01 02:43:05,713]: Epoch: 150, Loss:0.1158 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0074
[2025-04-01 02:43:05,719]: Epoch: 151, Loss:0.1208 Train: 0.9833, Val:0.7000, Test: 0.7843, Time(s/epoch):0.0057
[2025-04-01 02:43:05,726]: Epoch: 152, Loss:0.0865 Train: 0.9667, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0075
[2025-04-01 02:43:05,735]: Epoch: 153, Loss:0.1267 Train: 0.9750, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0081
[2025-04-01 02:43:05,743]: Epoch: 154, Loss:0.1136 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0085
[2025-04-01 02:43:05,750]: Epoch: 155, Loss:0.1020 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0063
[2025-04-01 02:43:05,758]: Epoch: 156, Loss:0.1110 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:05,766]: Epoch: 157, Loss:0.1084 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0076
[2025-04-01 02:43:05,773]: Epoch: 158, Loss:0.1118 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0068
[2025-04-01 02:43:05,781]: Epoch: 159, Loss:0.1198 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,790]: Epoch: 160, Loss:0.1053 Train: 0.9667, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0091
[2025-04-01 02:43:05,798]: Epoch: 161, Loss:0.1257 Train: 0.9750, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0077
[2025-04-01 02:43:05,805]: Epoch: 162, Loss:0.0843 Train: 0.9833, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0071
[2025-04-01 02:43:05,813]: Epoch: 163, Loss:0.1017 Train: 0.9833, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0082
[2025-04-01 02:43:05,820]: Epoch: 164, Loss:0.1114 Train: 0.9750, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0060
[2025-04-01 02:43:05,826]: Epoch: 165, Loss:0.1310 Train: 0.9833, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0066
[2025-04-01 02:43:05,833]: Epoch: 166, Loss:0.1119 Train: 0.9917, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0067
[2025-04-01 02:43:05,841]: Epoch: 167, Loss:0.0987 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0075
[2025-04-01 02:43:05,849]: Epoch: 168, Loss:0.1381 Train: 0.9750, Val:0.6500, Test: 0.8039, Time(s/epoch):0.0079
[2025-04-01 02:43:05,857]: Epoch: 169, Loss:0.1190 Train: 0.9833, Val:0.6750, Test: 0.8039, Time(s/epoch):0.0080
[2025-04-01 02:43:05,865]: Epoch: 170, Loss:0.1451 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0080
[2025-04-01 02:43:05,873]: Epoch: 171, Loss:0.1117 Train: 0.9583, Val:0.6750, Test: 0.7451, Time(s/epoch):0.0077
[2025-04-01 02:43:05,881]: Epoch: 172, Loss:0.1281 Train: 0.9750, Val:0.6750, Test: 0.7451, Time(s/epoch):0.0076
[2025-04-01 02:43:05,889]: Epoch: 173, Loss:0.1372 Train: 0.9833, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0080
[2025-04-01 02:43:05,896]: Epoch: 174, Loss:0.0743 Train: 0.9750, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:05,904]: Epoch: 175, Loss:0.0873 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0078
[2025-04-01 02:43:05,911]: Epoch: 176, Loss:0.1140 Train: 0.9667, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0073
[2025-04-01 02:43:05,918]: Epoch: 177, Loss:0.1123 Train: 0.9750, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:05,925]: Epoch: 178, Loss:0.1184 Train: 0.9833, Val:0.7000, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:05,932]: Epoch: 179, Loss:0.1178 Train: 0.9583, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0070
[2025-04-01 02:43:05,940]: Epoch: 180, Loss:0.1324 Train: 0.9833, Val:0.6750, Test: 0.7843, Time(s/epoch):0.0084
[2025-04-01 02:43:05,949]: Epoch: 181, Loss:0.1219 Train: 0.9833, Val:0.6375, Test: 0.8039, Time(s/epoch):0.0085
[2025-04-01 02:43:05,958]: Epoch: 182, Loss:0.1098 Train: 0.9750, Val:0.6375, Test: 0.8235, Time(s/epoch):0.0085
[2025-04-01 02:43:05,966]: Epoch: 183, Loss:0.1049 Train: 0.9750, Val:0.6250, Test: 0.8039, Time(s/epoch):0.0083
[2025-04-01 02:43:05,974]: Epoch: 184, Loss:0.1052 Train: 0.9750, Val:0.6250, Test: 0.7843, Time(s/epoch):0.0079
[2025-04-01 02:43:05,983]: Epoch: 185, Loss:0.1524 Train: 0.9750, Val:0.6375, Test: 0.7843, Time(s/epoch):0.0083
[2025-04-01 02:43:05,991]: Epoch: 186, Loss:0.1267 Train: 0.9750, Val:0.6500, Test: 0.7647, Time(s/epoch):0.0078
[2025-04-01 02:43:05,998]: Epoch: 187, Loss:0.1050 Train: 0.9667, Val:0.6625, Test: 0.7843, Time(s/epoch):0.0074
[2025-04-01 02:43:06,005]: Epoch: 188, Loss:0.1056 Train: 0.9833, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0064
[2025-04-01 02:43:06,012]: Epoch: 189, Loss:0.1084 Train: 0.9750, Val:0.6875, Test: 0.7843, Time(s/epoch):0.0065
[2025-04-01 02:43:06,018]: Epoch: 190, Loss:0.1170 Train: 0.9750, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0064
[2025-04-01 02:43:06,026]: Epoch: 191, Loss:0.0977 Train: 0.9833, Val:0.6500, Test: 0.8235, Time(s/epoch):0.0079
[2025-04-01 02:43:06,033]: Epoch: 192, Loss:0.0934 Train: 0.9833, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0068
[2025-04-01 02:43:06,040]: Epoch: 193, Loss:0.1101 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0064
[2025-04-01 02:43:06,048]: Epoch: 194, Loss:0.1047 Train: 0.9750, Val:0.6625, Test: 0.7647, Time(s/epoch):0.0087
[2025-04-01 02:43:06,057]: Epoch: 195, Loss:0.1053 Train: 0.9750, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0081
[2025-04-01 02:43:06,064]: Epoch: 196, Loss:0.1192 Train: 0.9667, Val:0.6500, Test: 0.7843, Time(s/epoch):0.0067
[2025-04-01 02:43:06,071]: Epoch: 197, Loss:0.0930 Train: 0.9667, Val:0.6625, Test: 0.8235, Time(s/epoch):0.0077
[2025-04-01 02:43:06,079]: Epoch: 198, Loss:0.1224 Train: 0.9750, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0070
[2025-04-01 02:43:06,085]: Epoch: 199, Loss:0.1085 Train: 0.9833, Val:0.6875, Test: 0.8235, Time(s/epoch):0.0068
[2025-04-01 02:43:06,093]: Epoch: 200, Loss:0.1001 Train: 0.9833, Val:0.6625, Test: 0.8039, Time(s/epoch):0.0072
[2025-04-01 02:43:06,093]: [Run-3 score] {'train': 0.975, 'val': 0.7, 'test': 0.803921568627451}
[2025-04-01 02:43:06,093]: repeat 1/3
[2025-04-01 02:43:06,093]: Manual random seed:0
[2025-04-01 02:43:06,094]: auto fixed data split seed to 0, model init seed to 0

Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0070
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0085
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0100
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0094
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0092
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0097
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0094
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0094
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0088
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0075
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0084
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0097
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0097
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0097
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0098
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0087
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0069
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0078
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0095
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0084
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0068
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0072
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0075
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0080
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0097
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0080
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0070
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0075
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0073
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0070
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0072
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0069
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0072
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0073
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0091
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0097
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0074
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0070
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0092
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0101
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0099
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0101
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0100
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0095
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0088
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0072
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0078
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0075
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0080
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0076
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0076
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0076
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0079
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0077
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0086
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0082
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0092
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0097
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0097
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.4767 Time(s/epoch):0.0065
Epoch: 002, Supervised Loss:1.4063 Time(s/epoch):0.0050
Epoch: 003, Supervised Loss:1.3290 Time(s/epoch):0.0047
Epoch: 004, Supervised Loss:1.2835 Time(s/epoch):0.0050
Epoch: 005, Supervised Loss:1.2698 Time(s/epoch):0.0063
Epoch: 006, Supervised Loss:1.2669 Time(s/epoch):0.0064
Epoch: 007, Supervised Loss:1.2634 Time(s/epoch):0.0059
Epoch: 008, Supervised Loss:1.2551 Time(s/epoch):0.0047
Epoch: 009, Supervised Loss:1.2445 Time(s/epoch):0.0051
Epoch: 010, Supervised Loss:1.2385 Time(s/epoch):0.0050
Epoch: 011, Supervised Loss:1.2376 Time(s/epoch):0.0052
Epoch: 012, Supervised Loss:1.2369 Time(s/epoch):0.0054
Epoch: 013, Supervised Loss:1.2349 Time(s/epoch):0.0055
Epoch: 014, Supervised Loss:1.2347 Time(s/epoch):0.0050
Epoch: 015, Supervised Loss:1.2395 Time(s/epoch):0.0053
Epoch: 016, Supervised Loss:1.2480 Time(s/epoch):0.0051
Epoch: 017, Supervised Loss:1.2562 Time(s/epoch):0.0055
Epoch: 018, Supervised Loss:1.2616 Time(s/epoch):0.0054
Epoch: 019, Supervised Loss:1.2642 Time(s/epoch):0.0052
Epoch: 020, Supervised Loss:1.2650 Time(s/epoch):0.0058
Epoch: 021, Supervised Loss:1.2651 Time(s/epoch):0.0060
Epoch: 022, Supervised Loss:1.2653 Time(s/epoch):0.0056
Epoch: 023, Supervised Loss:1.2661 Time(s/epoch):0.0056
Epoch: 024, Supervised Loss:1.2675 Time(s/epoch):0.0053
Epoch: 025, Supervised Loss:1.2695 Time(s/epoch):0.0057
Epoch: 026, Supervised Loss:1.2714 Time(s/epoch):0.0060
Epoch: 027, Supervised Loss:1.2723 Time(s/epoch):0.0057
Epoch: 028, Supervised Loss:1.2711 Time(s/epoch):0.0060
Epoch: 029, Supervised Loss:1.2673 Time(s/epoch):0.0061
Epoch: 030, Supervised Loss:1.2615 Time(s/epoch):0.0054
0.8911468 515
Add 405 edges.
Prune 367 edges from torch.Size([2, 515]) to torch.Size([2, 148])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 547], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.3937 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 547], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5397 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 547], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5437 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(18)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0153
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0104
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0124
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0095
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0123
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0112
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0101
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0098
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0090
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0097
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0119
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0094
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0079
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0086
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0087
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0080
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0077
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0099
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0087
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0099
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0098
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0097
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0076
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0074
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0074
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0096
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0100
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0099
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0095
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0093
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0096
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0094
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0094
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0093
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0079
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0069
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0089
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0096
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0098
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0095
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0095
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0078
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0074
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0071
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0071
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0079
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0073
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0073
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0072
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0073
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0072
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0076
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0097
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0088
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0071
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0081
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0093
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0092
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0092
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0093
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0094
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0093
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0094
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0093
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0094
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0086
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0072
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0072
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0070
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0072
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0075
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0074
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0078
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0071
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0099
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0094
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0069
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0071
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0077
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0074
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0075
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0072
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0073
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0074
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0075
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0070
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0075
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0070
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0089
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0096
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0093
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0093
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0093
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0094
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0095
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0095
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0098
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0099
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0101
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0104
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0097
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0095
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0095
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0073
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0070
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0073
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0073
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0087
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0097
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0097
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0096
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0098
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0099
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0096
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0074
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0076
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0075
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0077
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0097
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0093
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0094
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0099
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0076
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0077
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0074
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0087
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0077
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0095
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0096
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0097
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0097
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0099
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0100
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0098
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0097
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0102
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0100
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0098
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0097
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0098
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0097
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0093
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0092
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0095
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0094
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0094
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0094
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0095
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0094
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0084
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0068
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0070
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0074
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0076
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0102
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0100
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0104
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0103
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0098
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0094
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0093[2025-04-01 02:43:08,055]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:08,064]: Epoch: 001, Loss:1.8023 Train: 0.4833, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:43:08,072]: Epoch: 002, Loss:1.3588 Train: 0.7583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:08,078]: Epoch: 003, Loss:0.7791 Train: 0.8833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:08,085]: Epoch: 004, Loss:0.6068 Train: 0.9000, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:08,093]: Epoch: 005, Loss:0.5555 Train: 0.8917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:08,099]: Epoch: 006, Loss:0.4415 Train: 0.8917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:08,107]: Epoch: 007, Loss:0.3796 Train: 0.9167, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:08,114]: Epoch: 008, Loss:0.3216 Train: 0.9167, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:08,123]: Epoch: 009, Loss:0.3196 Train: 0.9250, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:43:08,131]: Epoch: 010, Loss:0.3085 Train: 0.9250, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:08,138]: Epoch: 011, Loss:0.2979 Train: 0.9250, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:08,144]: Epoch: 012, Loss:0.2392 Train: 0.9250, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:08,153]: Epoch: 013, Loss:0.2608 Train: 0.9333, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:08,159]: Epoch: 014, Loss:0.2608 Train: 0.9250, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0057
[2025-04-01 02:43:08,168]: Epoch: 015, Loss:0.2302 Train: 0.9167, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0097
[2025-04-01 02:43:08,177]: Epoch: 016, Loss:0.2004 Train: 0.9250, Val:0.7250, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:08,184]: Epoch: 017, Loss:0.2760 Train: 0.9500, Val:0.7250, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:08,192]: Epoch: 018, Loss:0.2371 Train: 0.9500, Val:0.7250, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:08,201]: Epoch: 019, Loss:0.2074 Train: 0.9583, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:08,207]: Epoch: 020, Loss:0.2119 Train: 0.9667, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:43:08,215]: Epoch: 021, Loss:0.1503 Train: 0.9500, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:08,223]: Epoch: 022, Loss:0.2093 Train: 0.9583, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:08,231]: Epoch: 023, Loss:0.1771 Train: 0.9583, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:43:08,237]: Epoch: 024, Loss:0.1968 Train: 0.9583, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:08,245]: Epoch: 025, Loss:0.1897 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:08,251]: Epoch: 026, Loss:0.1392 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:08,259]: Epoch: 027, Loss:0.1987 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:08,267]: Epoch: 028, Loss:0.1650 Train: 0.9750, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0074
[2025-04-01 02:43:08,274]: Epoch: 029, Loss:0.1612 Train: 0.9750, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:08,280]: Epoch: 030, Loss:0.1528 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:08,288]: Epoch: 031, Loss:0.1819 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:08,296]: Epoch: 032, Loss:0.1501 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:43:08,306]: Epoch: 033, Loss:0.1411 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0091
[2025-04-01 02:43:08,314]: Epoch: 034, Loss:0.1460 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:08,323]: Epoch: 035, Loss:0.1304 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0089
[2025-04-01 02:43:08,330]: Epoch: 036, Loss:0.1595 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:08,339]: Epoch: 037, Loss:0.1783 Train: 0.9500, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0090
[2025-04-01 02:43:08,347]: Epoch: 038, Loss:0.1645 Train: 0.9500, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:08,355]: Epoch: 039, Loss:0.1810 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:08,364]: Epoch: 040, Loss:0.1410 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0088
[2025-04-01 02:43:08,371]: Epoch: 041, Loss:0.1205 Train: 0.9667, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:43:08,379]: Epoch: 042, Loss:0.1858 Train: 0.9667, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:08,387]: Epoch: 043, Loss:0.1502 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:08,394]: Epoch: 044, Loss:0.1368 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:08,400]: Epoch: 045, Loss:0.1486 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:08,408]: Epoch: 046, Loss:0.1417 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:08,414]: Epoch: 047, Loss:0.1612 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:08,422]: Epoch: 048, Loss:0.1172 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:08,429]: Epoch: 049, Loss:0.1600 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:08,436]: Epoch: 050, Loss:0.1459 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:08,444]: Epoch: 051, Loss:0.1468 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:08,452]: Epoch: 052, Loss:0.1653 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:08,461]: Epoch: 053, Loss:0.1552 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0087
[2025-04-01 02:43:08,469]: Epoch: 054, Loss:0.1102 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:08,475]: Epoch: 055, Loss:0.1527 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:08,482]: Epoch: 056, Loss:0.1878 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:08,488]: Epoch: 057, Loss:0.1754 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:08,497]: Epoch: 058, Loss:0.1429 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:08,505]: Epoch: 059, Loss:0.1255 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:08,513]: Epoch: 060, Loss:0.1165 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:08,520]: Epoch: 061, Loss:0.1351 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:08,528]: Epoch: 062, Loss:0.1282 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:08,535]: Epoch: 063, Loss:0.1373 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:08,546]: Epoch: 064, Loss:0.1259 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0108
[2025-04-01 02:43:08,564]: Epoch: 065, Loss:0.1332 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0176
[2025-04-01 02:43:08,578]: Epoch: 066, Loss:0.1245 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0133
[2025-04-01 02:43:08,591]: Epoch: 067, Loss:0.1256 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0129
[2025-04-01 02:43:08,602]: Epoch: 068, Loss:0.1493 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0113
[2025-04-01 02:43:08,612]: Epoch: 069, Loss:0.1179 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0092
[2025-04-01 02:43:08,620]: Epoch: 070, Loss:0.1235 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:08,628]: Epoch: 071, Loss:0.1194 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:08,636]: Epoch: 072, Loss:0.1240 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:08,645]: Epoch: 073, Loss:0.1243 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:08,652]: Epoch: 074, Loss:0.1245 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:08,660]: Epoch: 075, Loss:0.1193 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:08,667]: Epoch: 076, Loss:0.1269 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:08,675]: Epoch: 077, Loss:0.1425 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:08,683]: Epoch: 078, Loss:0.1161 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:08,689]: Epoch: 079, Loss:0.1234 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0059
[2025-04-01 02:43:08,695]: Epoch: 080, Loss:0.1474 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:43:08,703]: Epoch: 081, Loss:0.1174 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:08,711]: Epoch: 082, Loss:0.1478 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0072
[2025-04-01 02:43:08,719]: Epoch: 083, Loss:0.1464 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:08,726]: Epoch: 084, Loss:0.1018 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:08,734]: Epoch: 085, Loss:0.1212 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:08,742]: Epoch: 086, Loss:0.1102 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:08,750]: Epoch: 087, Loss:0.1234 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:08,756]: Epoch: 088, Loss:0.1547 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:08,764]: Epoch: 089, Loss:0.1069 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:08,772]: Epoch: 090, Loss:0.1220 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:08,778]: Epoch: 091, Loss:0.1224 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:08,786]: Epoch: 092, Loss:0.1511 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:08,794]: Epoch: 093, Loss:0.1080 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:08,801]: Epoch: 094, Loss:0.1121 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:08,809]: Epoch: 095, Loss:0.1325 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:08,816]: Epoch: 096, Loss:0.1434 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:08,823]: Epoch: 097, Loss:0.1186 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:08,829]: Epoch: 098, Loss:0.1115 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:08,836]: Epoch: 099, Loss:0.1116 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:08,844]: Epoch: 100, Loss:0.1302 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:08,852]: Epoch: 101, Loss:0.1256 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:08,860]: Epoch: 102, Loss:0.1187 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:08,867]: Epoch: 103, Loss:0.1309 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:08,874]: Epoch: 104, Loss:0.1030 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:08,882]: Epoch: 105, Loss:0.1088 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:08,890]: Epoch: 106, Loss:0.1243 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:08,896]: Epoch: 107, Loss:0.1261 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:08,905]: Epoch: 108, Loss:0.1286 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:08,912]: Epoch: 109, Loss:0.1270 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:08,919]: Epoch: 110, Loss:0.1312 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:08,925]: Epoch: 111, Loss:0.0965 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:43:08,934]: Epoch: 112, Loss:0.1527 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:08,941]: Epoch: 113, Loss:0.1058 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:08,949]: Epoch: 114, Loss:0.1196 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:08,956]: Epoch: 115, Loss:0.1309 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:08,965]: Epoch: 116, Loss:0.1002 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:08,973]: Epoch: 117, Loss:0.0963 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:08,984]: Epoch: 118, Loss:0.1077 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0106
[2025-04-01 02:43:08,990]: Epoch: 119, Loss:0.1161 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:08,998]: Epoch: 120, Loss:0.1248 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:09,007]: Epoch: 121, Loss:0.1322 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:43:09,016]: Epoch: 122, Loss:0.0896 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0088
[2025-04-01 02:43:09,025]: Epoch: 123, Loss:0.1169 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0088
[2025-04-01 02:43:09,033]: Epoch: 124, Loss:0.1123 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:09,041]: Epoch: 125, Loss:0.1034 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:09,048]: Epoch: 126, Loss:0.1520 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:09,054]: Epoch: 127, Loss:0.1462 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:09,063]: Epoch: 128, Loss:0.1263 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:09,070]: Epoch: 129, Loss:0.1151 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:09,078]: Epoch: 130, Loss:0.1262 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:09,086]: Epoch: 131, Loss:0.1381 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:09,094]: Epoch: 132, Loss:0.1794 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:09,101]: Epoch: 133, Loss:0.1217 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:09,107]: Epoch: 134, Loss:0.1310 Train: 0.9500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:09,115]: Epoch: 135, Loss:0.1187 Train: 0.9667, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:09,121]: Epoch: 136, Loss:0.1488 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:09,128]: Epoch: 137, Loss:0.1396 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:09,136]: Epoch: 138, Loss:0.1184 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:09,142]: Epoch: 139, Loss:0.1340 Train: 0.9750, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:43:09,149]: Epoch: 140, Loss:0.1296 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:09,156]: Epoch: 141, Loss:0.1178 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:09,165]: Epoch: 142, Loss:0.1119 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0089
[2025-04-01 02:43:09,174]: Epoch: 143, Loss:0.1123 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:43:09,181]: Epoch: 144, Loss:0.1384 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:09,189]: Epoch: 145, Loss:0.1342 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:09,196]: Epoch: 146, Loss:0.1388 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:09,203]: Epoch: 147, Loss:0.1074 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:09,210]: Epoch: 148, Loss:0.1059 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:09,216]: Epoch: 149, Loss:0.1126 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:09,223]: Epoch: 150, Loss:0.1126 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:09,229]: Epoch: 151, Loss:0.1121 Train: 0.9667, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0061
[2025-04-01 02:43:09,236]: Epoch: 152, Loss:0.1150 Train: 0.9583, Val:0.7250, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:09,243]: Epoch: 153, Loss:0.1425 Train: 0.9750, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:09,250]: Epoch: 154, Loss:0.1077 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:09,258]: Epoch: 155, Loss:0.1415 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:09,264]: Epoch: 156, Loss:0.1645 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:09,271]: Epoch: 157, Loss:0.1353 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:09,279]: Epoch: 158, Loss:0.1436 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:09,287]: Epoch: 159, Loss:0.1393 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:09,294]: Epoch: 160, Loss:0.0946 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:09,301]: Epoch: 161, Loss:0.1417 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:09,308]: Epoch: 162, Loss:0.1176 Train: 0.9583, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:09,316]: Epoch: 163, Loss:0.1350 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:09,323]: Epoch: 164, Loss:0.1714 Train: 0.9583, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:43:09,330]: Epoch: 165, Loss:0.1143 Train: 0.9750, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:09,337]: Epoch: 166, Loss:0.1293 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:09,344]: Epoch: 167, Loss:0.1242 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0061
[2025-04-01 02:43:09,351]: Epoch: 168, Loss:0.1081 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:09,359]: Epoch: 169, Loss:0.1739 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,367]: Epoch: 170, Loss:0.1250 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,374]: Epoch: 171, Loss:0.1480 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,382]: Epoch: 172, Loss:0.1027 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,390]: Epoch: 173, Loss:0.1293 Train: 0.9667, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:43:09,398]: Epoch: 174, Loss:0.1185 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:09,405]: Epoch: 175, Loss:0.1690 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:09,412]: Epoch: 176, Loss:0.1110 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:09,419]: Epoch: 177, Loss:0.1332 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:09,426]: Epoch: 178, Loss:0.1238 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:09,434]: Epoch: 179, Loss:0.1124 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:09,441]: Epoch: 180, Loss:0.1123 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:09,449]: Epoch: 181, Loss:0.1017 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:09,456]: Epoch: 182, Loss:0.1311 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:09,463]: Epoch: 183, Loss:0.1284 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:09,470]: Epoch: 184, Loss:0.1395 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:09,479]: Epoch: 185, Loss:0.1248 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:09,486]: Epoch: 186, Loss:0.0843 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:09,492]: Epoch: 187, Loss:0.1070 Train: 0.9583, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:09,499]: Epoch: 188, Loss:0.1125 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:09,506]: Epoch: 189, Loss:0.2134 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:09,514]: Epoch: 190, Loss:0.1501 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:09,520]: Epoch: 191, Loss:0.0999 Train: 0.9750, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0060
[2025-04-01 02:43:09,528]: Epoch: 192, Loss:0.1168 Train: 0.9667, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:09,535]: Epoch: 193, Loss:0.1078 Train: 0.9583, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:09,541]: Epoch: 194, Loss:0.1549 Train: 0.9583, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:09,548]: Epoch: 195, Loss:0.1356 Train: 0.9667, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:09,554]: Epoch: 196, Loss:0.1260 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:09,561]: Epoch: 197, Loss:0.1132 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:09,568]: Epoch: 198, Loss:0.1078 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:09,575]: Epoch: 199, Loss:0.1263 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:09,582]: Epoch: 200, Loss:0.1004 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:09,582]: [Run-1 score] {'train': 0.925, 'val': 0.725, 'test': 0.5686274509803921}
[2025-04-01 02:43:09,582]: repeat 2/3
[2025-04-01 02:43:09,582]: Manual random seed:0
[2025-04-01 02:43:09,582]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:09,587]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:09,597]: Epoch: 001, Loss:1.6309 Train: 0.6667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:09,604]: Epoch: 002, Loss:1.0371 Train: 0.8167, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:09,612]: Epoch: 003, Loss:0.6001 Train: 0.8667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:09,619]: Epoch: 004, Loss:0.5013 Train: 0.8667, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:09,626]: Epoch: 005, Loss:0.4355 Train: 0.9000, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:09,634]: Epoch: 006, Loss:0.4204 Train: 0.8833, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:09,640]: Epoch: 007, Loss:0.3885 Train: 0.9083, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0056
[2025-04-01 02:43:09,648]: Epoch: 008, Loss:0.2743 Train: 0.9250, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,656]: Epoch: 009, Loss:0.2734 Train: 0.9083, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:09,664]: Epoch: 010, Loss:0.3002 Train: 0.9000, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:09,671]: Epoch: 011, Loss:0.2415 Train: 0.9167, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:09,679]: Epoch: 012, Loss:0.2797 Train: 0.9333, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:09,686]: Epoch: 013, Loss:0.2152 Train: 0.9333, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:09,693]: Epoch: 014, Loss:0.2394 Train: 0.9417, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:09,700]: Epoch: 015, Loss:0.2098 Train: 0.9333, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:09,708]: Epoch: 016, Loss:0.1925 Train: 0.9500, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:09,715]: Epoch: 017, Loss:0.2438 Train: 0.9333, Val:0.7250, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:09,724]: Epoch: 018, Loss:0.2157 Train: 0.9167, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:09,731]: Epoch: 019, Loss:0.2320 Train: 0.9500, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:09,740]: Epoch: 020, Loss:0.1944 Train: 0.9417, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:09,748]: Epoch: 021, Loss:0.1897 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:09,755]: Epoch: 022, Loss:0.1974 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:09,764]: Epoch: 023, Loss:0.2581 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:43:09,770]: Epoch: 024, Loss:0.1947 Train: 0.9500, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0059
[2025-04-01 02:43:09,779]: Epoch: 025, Loss:0.1577 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0088
[2025-04-01 02:43:09,787]: Epoch: 026, Loss:0.2067 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:09,796]: Epoch: 027, Loss:0.1416 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:09,804]: Epoch: 028, Loss:0.1796 Train: 0.9500, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:09,813]: Epoch: 029, Loss:0.1638 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:43:09,820]: Epoch: 030, Loss:0.1654 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:09,828]: Epoch: 031, Loss:0.1612 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:09,835]: Epoch: 032, Loss:0.1448 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:09,843]: Epoch: 033, Loss:0.2513 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:09,853]: Epoch: 034, Loss:0.1875 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0096
[2025-04-01 02:43:09,861]: Epoch: 035, Loss:0.1775 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:09,868]: Epoch: 036, Loss:0.1519 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:09,875]: Epoch: 037, Loss:0.1637 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:09,884]: Epoch: 038, Loss:0.1633 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:09,891]: Epoch: 039, Loss:0.1914 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:09,898]: Epoch: 040, Loss:0.1479 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:09,905]: Epoch: 041, Loss:0.1435 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:09,912]: Epoch: 042, Loss:0.1536 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:09,921]: Epoch: 043, Loss:0.1856 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:09,929]: Epoch: 044, Loss:0.1651 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:09,937]: Epoch: 045, Loss:0.1936 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:09,943]: Epoch: 046, Loss:0.1508 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:09,951]: Epoch: 047, Loss:0.1864 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:09,959]: Epoch: 048, Loss:0.1835 Train: 0.9500, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:43:09,968]: Epoch: 049, Loss:0.1984 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0087
[2025-04-01 02:43:09,976]: Epoch: 050, Loss:0.1802 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:09,983]: Epoch: 051, Loss:0.1659 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:09,989]: Epoch: 052, Loss:0.1788 Train: 0.9417, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:09,997]: Epoch: 053, Loss:0.1699 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:10,004]: Epoch: 054, Loss:0.1753 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,011]: Epoch: 055, Loss:0.1361 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:10,018]: Epoch: 056, Loss:0.1328 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,026]: Epoch: 057, Loss:0.1530 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,034]: Epoch: 058, Loss:0.1420 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:10,040]: Epoch: 059, Loss:0.1433 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:10,047]: Epoch: 060, Loss:0.1506 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:10,055]: Epoch: 061, Loss:0.1281 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:10,062]: Epoch: 062, Loss:0.1569 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:10,068]: Epoch: 063, Loss:0.1487 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:10,076]: Epoch: 064, Loss:0.1751 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:10,085]: Epoch: 065, Loss:0.1754 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0089
[2025-04-01 02:43:10,093]: Epoch: 066, Loss:0.1463 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:10,101]: Epoch: 067, Loss:0.1634 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:10,107]: Epoch: 068, Loss:0.1631 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,114]: Epoch: 069, Loss:0.1228 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:10,121]: Epoch: 070, Loss:0.1427 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:10,128]: Epoch: 071, Loss:0.1589 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:10,136]: Epoch: 072, Loss:0.1259 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,142]: Epoch: 073, Loss:0.1290 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,150]: Epoch: 074, Loss:0.1362 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:10,158]: Epoch: 075, Loss:0.1381 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,166]: Epoch: 076, Loss:0.1366 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:10,174]: Epoch: 077, Loss:0.1117 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:10,182]: Epoch: 078, Loss:0.1411 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:10,189]: Epoch: 079, Loss:0.1417 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:10,197]: Epoch: 080, Loss:0.1217 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:10,203]: Epoch: 081, Loss:0.1635 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:10,210]: Epoch: 082, Loss:0.1426 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:10,219]: Epoch: 083, Loss:0.1308 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:10,225]: Epoch: 084, Loss:0.1354 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:10,232]: Epoch: 085, Loss:0.1356 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:10,238]: Epoch: 086, Loss:0.1381 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:10,246]: Epoch: 087, Loss:0.1080 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:10,254]: Epoch: 088, Loss:0.1640 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:10,263]: Epoch: 089, Loss:0.1863 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:10,270]: Epoch: 090, Loss:0.1464 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:10,277]: Epoch: 091, Loss:0.1212 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:10,284]: Epoch: 092, Loss:0.1361 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:10,292]: Epoch: 093, Loss:0.1558 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:43:10,298]: Epoch: 094, Loss:0.1203 Train: 0.9583, Val:0.7125, Test: 0.5490, Time(s/epoch):0.0059
[2025-04-01 02:43:10,307]: Epoch: 095, Loss:0.1230 Train: 0.9583, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:10,313]: Epoch: 096, Loss:0.1421 Train: 0.9583, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:43:10,320]: Epoch: 097, Loss:0.1292 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:10,328]: Epoch: 098, Loss:0.1054 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,336]: Epoch: 099, Loss:0.1717 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:10,344]: Epoch: 100, Loss:0.1380 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,352]: Epoch: 101, Loss:0.1483 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:10,359]: Epoch: 102, Loss:0.1068 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:10,366]: Epoch: 103, Loss:0.1366 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:10,374]: Epoch: 104, Loss:0.1419 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:10,379]: Epoch: 105, Loss:0.1352 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0056
[2025-04-01 02:43:10,387]: Epoch: 106, Loss:0.1475 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:10,396]: Epoch: 107, Loss:0.1307 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0093
[2025-04-01 02:43:10,403]: Epoch: 108, Loss:0.1178 Train: 0.9583, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:10,412]: Epoch: 109, Loss:0.1187 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:10,419]: Epoch: 110, Loss:0.1052 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:10,428]: Epoch: 111, Loss:0.1333 Train: 0.9667, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:10,435]: Epoch: 112, Loss:0.1331 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,444]: Epoch: 113, Loss:0.1406 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:43:10,452]: Epoch: 114, Loss:0.1369 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,460]: Epoch: 115, Loss:0.1024 Train: 0.9583, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:10,468]: Epoch: 116, Loss:0.1563 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,476]: Epoch: 117, Loss:0.1548 Train: 0.9667, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:10,483]: Epoch: 118, Loss:0.1244 Train: 0.9667, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:10,491]: Epoch: 119, Loss:0.1448 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:43:10,498]: Epoch: 120, Loss:0.1193 Train: 0.9750, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:10,504]: Epoch: 121, Loss:0.1356 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:10,512]: Epoch: 122, Loss:0.1618 Train: 0.9750, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:10,519]: Epoch: 123, Loss:0.1421 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:10,526]: Epoch: 124, Loss:0.0953 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:10,533]: Epoch: 125, Loss:0.1150 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,540]: Epoch: 126, Loss:0.1404 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:10,547]: Epoch: 127, Loss:0.1198 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:10,554]: Epoch: 128, Loss:0.1015 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:10,561]: Epoch: 129, Loss:0.1395 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:10,569]: Epoch: 130, Loss:0.1184 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:10,577]: Epoch: 131, Loss:0.1008 Train: 0.9500, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:10,585]: Epoch: 132, Loss:0.1367 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,594]: Epoch: 133, Loss:0.1169 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0090
[2025-04-01 02:43:10,600]: Epoch: 134, Loss:0.1110 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:10,608]: Epoch: 135, Loss:0.1373 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,616]: Epoch: 136, Loss:0.1066 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:10,623]: Epoch: 137, Loss:0.1047 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:10,629]: Epoch: 138, Loss:0.1291 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:10,637]: Epoch: 139, Loss:0.1451 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:10,644]: Epoch: 140, Loss:0.1171 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:10,651]: Epoch: 141, Loss:0.1135 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:10,659]: Epoch: 142, Loss:0.1611 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:10,665]: Epoch: 143, Loss:0.1805 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0058
[2025-04-01 02:43:10,672]: Epoch: 144, Loss:0.1767 Train: 0.9833, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:10,680]: Epoch: 145, Loss:0.1307 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:10,686]: Epoch: 146, Loss:0.1843 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,693]: Epoch: 147, Loss:0.1192 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:10,700]: Epoch: 148, Loss:0.1292 Train: 0.9583, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:10,708]: Epoch: 149, Loss:0.1695 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,716]: Epoch: 150, Loss:0.1238 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:10,724]: Epoch: 151, Loss:0.1131 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,731]: Epoch: 152, Loss:0.1334 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:10,737]: Epoch: 153, Loss:0.1248 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,744]: Epoch: 154, Loss:0.1063 Train: 0.9583, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,750]: Epoch: 155, Loss:0.1590 Train: 0.9583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:10,757]: Epoch: 156, Loss:0.1494 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:10,764]: Epoch: 157, Loss:0.1620 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:10,771]: Epoch: 158, Loss:0.1063 Train: 0.9583, Val:0.7250, Test: 0.5686, Time(s/epoch):0.0061
[2025-04-01 02:43:10,777]: Epoch: 159, Loss:0.1131 Train: 0.9667, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:43:10,786]: Epoch: 160, Loss:0.1739 Train: 0.9750, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:10,794]: Epoch: 161, Loss:0.1656 Train: 0.9667, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:10,801]: Epoch: 162, Loss:0.1334 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:10,809]: Epoch: 163, Loss:0.1619 Train: 0.9583, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:10,817]: Epoch: 164, Loss:0.1438 Train: 0.9667, Val:0.6250, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:10,825]: Epoch: 165, Loss:0.1152 Train: 0.9667, Val:0.6250, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:10,834]: Epoch: 166, Loss:0.1154 Train: 0.9750, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0087
[2025-04-01 02:43:10,840]: Epoch: 167, Loss:0.1764 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0060
[2025-04-01 02:43:10,847]: Epoch: 168, Loss:0.1395 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:10,854]: Epoch: 169, Loss:0.1022 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:10,861]: Epoch: 170, Loss:0.1566 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:10,869]: Epoch: 171, Loss:0.1171 Train: 0.9750, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:10,877]: Epoch: 172, Loss:0.0994 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:10,883]: Epoch: 173, Loss:0.1524 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:10,891]: Epoch: 174, Loss:0.1399 Train: 0.9583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:10,899]: Epoch: 175, Loss:0.1558 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:10,906]: Epoch: 176, Loss:0.1600 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:10,914]: Epoch: 177, Loss:0.1254 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,922]: Epoch: 178, Loss:0.1139 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:10,929]: Epoch: 179, Loss:0.1616 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:10,938]: Epoch: 180, Loss:0.1305 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:10,945]: Epoch: 181, Loss:0.1326 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:10,952]: Epoch: 182, Loss:0.1129 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:10,959]: Epoch: 183, Loss:0.1083 Train: 0.9750, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:10,967]: Epoch: 184, Loss:0.1018 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:10,975]: Epoch: 185, Loss:0.1170 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:10,981]: Epoch: 186, Loss:0.1470 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:10,988]: Epoch: 187, Loss:0.1698 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:10,996]: Epoch: 188, Loss:0.1250 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:11,004]: Epoch: 189, Loss:0.1071 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:11,013]: Epoch: 190, Loss:0.1140 Train: 0.9500, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:11,020]: Epoch: 191, Loss:0.1258 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:11,028]: Epoch: 192, Loss:0.1581 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:11,036]: Epoch: 193, Loss:0.1004 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:11,044]: Epoch: 194, Loss:0.1360 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:11,052]: Epoch: 195, Loss:0.1104 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:11,059]: Epoch: 196, Loss:0.1404 Train: 0.9583, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:11,065]: Epoch: 197, Loss:0.1455 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:11,073]: Epoch: 198, Loss:0.0984 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:11,081]: Epoch: 199, Loss:0.1292 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:11,088]: Epoch: 200, Loss:0.1278 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:11,088]: [Run-2 score] {'train': 0.9, 'val': 0.725, 'test': 0.6274509803921569}
[2025-04-01 02:43:11,088]: repeat 3/3
[2025-04-01 02:43:11,088]: Manual random seed:0
[2025-04-01 02:43:11,088]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:11,093]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:11,103]: Epoch: 001, Loss:1.6958 Train: 0.6833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:11,111]: Epoch: 002, Loss:1.0909 Train: 0.7583, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:11,117]: Epoch: 003, Loss:0.7568 Train: 0.8167, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:11,125]: Epoch: 004, Loss:0.5613 Train: 0.8667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:11,132]: Epoch: 005, Loss:0.4173 Train: 0.8917, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:11,139]: Epoch: 006, Loss:0.3936 Train: 0.8917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:11,146]: Epoch: 007, Loss:0.3786 Train: 0.9250, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:11,153]: Epoch: 008, Loss:0.3260 Train: 0.9417, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:11,162]: Epoch: 009, Loss:0.3383 Train: 0.9250, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0088
[2025-04-01 02:43:11,172]: Epoch: 010, Loss:0.2607 Train: 0.9250, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0094
[2025-04-01 02:43:11,180]: Epoch: 011, Loss:0.2628 Train: 0.9333, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:11,188]: Epoch: 012, Loss:0.2769 Train: 0.9417, Val:0.7125, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:11,196]: Epoch: 013, Loss:0.3032 Train: 0.9167, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:11,204]: Epoch: 014, Loss:0.2404 Train: 0.9167, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:11,212]: Epoch: 015, Loss:0.2290 Train: 0.9417, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:11,219]: Epoch: 016, Loss:0.2047 Train: 0.9500, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:11,227]: Epoch: 017, Loss:0.1842 Train: 0.9500, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:11,234]: Epoch: 018, Loss:0.2178 Train: 0.9500, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:11,241]: Epoch: 019, Loss:0.2416 Train: 0.9417, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:11,248]: Epoch: 020, Loss:0.2128 Train: 0.9417, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:11,256]: Epoch: 021, Loss:0.1788 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:11,264]: Epoch: 022, Loss:0.1557 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:11,271]: Epoch: 023, Loss:0.2113 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:11,278]: Epoch: 024, Loss:0.2058 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:11,286]: Epoch: 025, Loss:0.1718 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:11,293]: Epoch: 026, Loss:0.1696 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:11,301]: Epoch: 027, Loss:0.1732 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:11,307]: Epoch: 028, Loss:0.1759 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:11,316]: Epoch: 029, Loss:0.1779 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:11,322]: Epoch: 030, Loss:0.1429 Train: 0.9667, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:11,330]: Epoch: 031, Loss:0.1391 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:11,338]: Epoch: 032, Loss:0.1708 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:11,344]: Epoch: 033, Loss:0.1873 Train: 0.9583, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:11,352]: Epoch: 034, Loss:0.1503 Train: 0.9667, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:11,360]: Epoch: 035, Loss:0.1684 Train: 0.9750, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:11,368]: Epoch: 036, Loss:0.1569 Train: 0.9667, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:11,375]: Epoch: 037, Loss:0.1381 Train: 0.9667, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:11,383]: Epoch: 038, Loss:0.1762 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:11,391]: Epoch: 039, Loss:0.1570 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:11,397]: Epoch: 040, Loss:0.1701 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:43:11,405]: Epoch: 041, Loss:0.1405 Train: 0.9583, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:11,412]: Epoch: 042, Loss:0.1321 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:11,419]: Epoch: 043, Loss:0.1404 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:11,426]: Epoch: 044, Loss:0.1847 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,434]: Epoch: 045, Loss:0.1757 Train: 0.9417, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:11,442]: Epoch: 046, Loss:0.1670 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:11,450]: Epoch: 047, Loss:0.1797 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:11,457]: Epoch: 048, Loss:0.1527 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:11,464]: Epoch: 049, Loss:0.1229 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:11,471]: Epoch: 050, Loss:0.1480 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:11,479]: Epoch: 051, Loss:0.1380 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:11,486]: Epoch: 052, Loss:0.1418 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:11,492]: Epoch: 053, Loss:0.1547 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0058
[2025-04-01 02:43:11,499]: Epoch: 054, Loss:0.1481 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:11,507]: Epoch: 055, Loss:0.1285 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:11,513]: Epoch: 056, Loss:0.1906 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:11,519]: Epoch: 057, Loss:0.1143 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:11,527]: Epoch: 058, Loss:0.1313 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,534]: Epoch: 059, Loss:0.1649 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,540]: Epoch: 060, Loss:0.1398 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:11,548]: Epoch: 061, Loss:0.1428 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:11,556]: Epoch: 062, Loss:0.1342 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:11,564]: Epoch: 063, Loss:0.1407 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:11,572]: Epoch: 064, Loss:0.1249 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:11,580]: Epoch: 065, Loss:0.1327 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:11,587]: Epoch: 066, Loss:0.1357 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:11,595]: Epoch: 067, Loss:0.1214 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:11,601]: Epoch: 068, Loss:0.1450 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0055
[2025-04-01 02:43:11,608]: Epoch: 069, Loss:0.1498 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,615]: Epoch: 070, Loss:0.1454 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:11,621]: Epoch: 071, Loss:0.1670 Train: 0.9667, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:43:11,629]: Epoch: 072, Loss:0.1573 Train: 0.9667, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:11,637]: Epoch: 073, Loss:0.1513 Train: 0.9667, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:11,645]: Epoch: 074, Loss:0.1632 Train: 0.9583, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:11,653]: Epoch: 075, Loss:0.1469 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:11,659]: Epoch: 076, Loss:0.1141 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0058
[2025-04-01 02:43:11,668]: Epoch: 077, Loss:0.1212 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:11,675]: Epoch: 078, Loss:0.1254 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:11,683]: Epoch: 079, Loss:0.1250 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:11,691]: Epoch: 080, Loss:0.1556 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:11,699]: Epoch: 081, Loss:0.1428 Train: 0.9667, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:11,705]: Epoch: 082, Loss:0.1734 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:11,713]: Epoch: 083, Loss:0.1309 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:11,719]: Epoch: 084, Loss:0.1042 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:11,726]: Epoch: 085, Loss:0.1371 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:11,733]: Epoch: 086, Loss:0.1112 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:11,739]: Epoch: 087, Loss:0.1172 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:43:11,746]: Epoch: 088, Loss:0.1747 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:11,754]: Epoch: 089, Loss:0.1128 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:11,761]: Epoch: 090, Loss:0.0996 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:11,768]: Epoch: 091, Loss:0.1192 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:11,775]: Epoch: 092, Loss:0.1815 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,782]: Epoch: 093, Loss:0.1614 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:11,791]: Epoch: 094, Loss:0.1686 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:11,799]: Epoch: 095, Loss:0.1117 Train: 0.9667, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:11,806]: Epoch: 096, Loss:0.1533 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:11,815]: Epoch: 097, Loss:0.1170 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:11,823]: Epoch: 098, Loss:0.1419 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:11,833]: Epoch: 099, Loss:0.1448 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0097
[2025-04-01 02:43:11,841]: Epoch: 100, Loss:0.1843 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:11,850]: Epoch: 101, Loss:0.1051 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0089
[2025-04-01 02:43:11,858]: Epoch: 102, Loss:0.1329 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:11,867]: Epoch: 103, Loss:0.1329 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:11,874]: Epoch: 104, Loss:0.1513 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:11,882]: Epoch: 105, Loss:0.1134 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:43:11,888]: Epoch: 106, Loss:0.1474 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:11,897]: Epoch: 107, Loss:0.1123 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:43:11,905]: Epoch: 108, Loss:0.1388 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:11,912]: Epoch: 109, Loss:0.1324 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:11,921]: Epoch: 110, Loss:0.1152 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:11,930]: Epoch: 111, Loss:0.1092 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0087
[2025-04-01 02:43:11,937]: Epoch: 112, Loss:0.1277 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:11,945]: Epoch: 113, Loss:0.1185 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:11,952]: Epoch: 114, Loss:0.1198 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:11,960]: Epoch: 115, Loss:0.1164 Train: 0.9583, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:11,968]: Epoch: 116, Loss:0.1081 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:11,975]: Epoch: 117, Loss:0.1315 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:11,983]: Epoch: 118, Loss:0.1299 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:11,990]: Epoch: 119, Loss:0.1489 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:11,997]: Epoch: 120, Loss:0.1547 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:12,004]: Epoch: 121, Loss:0.1281 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:12,011]: Epoch: 122, Loss:0.1422 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:12,019]: Epoch: 123, Loss:0.1212 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:12,027]: Epoch: 124, Loss:0.1441 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:12,034]: Epoch: 125, Loss:0.1142 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:12,041]: Epoch: 126, Loss:0.1306 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:12,048]: Epoch: 127, Loss:0.1104 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:12,056]: Epoch: 128, Loss:0.1114 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:12,062]: Epoch: 129, Loss:0.1108 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:12,069]: Epoch: 130, Loss:0.1513 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:12,077]: Epoch: 131, Loss:0.1166 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:12,084]: Epoch: 132, Loss:0.1141 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:12,092]: Epoch: 133, Loss:0.1034 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:12,100]: Epoch: 134, Loss:0.0965 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:12,107]: Epoch: 135, Loss:0.1121 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:12,114]: Epoch: 136, Loss:0.1526 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:12,123]: Epoch: 137, Loss:0.1371 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0088
[2025-04-01 02:43:12,129]: Epoch: 138, Loss:0.1199 Train: 0.9667, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0057
[2025-04-01 02:43:12,136]: Epoch: 139, Loss:0.1396 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:12,143]: Epoch: 140, Loss:0.1210 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:12,149]: Epoch: 141, Loss:0.1618 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:12,156]: Epoch: 142, Loss:0.1519 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:12,163]: Epoch: 143, Loss:0.1218 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:12,171]: Epoch: 144, Loss:0.1509 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:12,181]: Epoch: 145, Loss:0.1430 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0098
[2025-04-01 02:43:12,189]: Epoch: 146, Loss:0.1226 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:12,198]: Epoch: 147, Loss:0.1246 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:12,204]: Epoch: 148, Loss:0.1766 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:43:12,213]: Epoch: 149, Loss:0.1279 Train: 0.9583, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:12,220]: Epoch: 150, Loss:0.1385 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:12,228]: Epoch: 151, Loss:0.1141 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:12,236]: Epoch: 152, Loss:0.1444 Train: 0.9667, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:12,242]: Epoch: 153, Loss:0.1722 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:12,250]: Epoch: 154, Loss:0.1003 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:12,257]: Epoch: 155, Loss:0.1350 Train: 0.9750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:12,266]: Epoch: 156, Loss:0.1258 Train: 0.9583, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:12,274]: Epoch: 157, Loss:0.1835 Train: 0.9667, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:12,280]: Epoch: 158, Loss:0.1562 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0058
[2025-04-01 02:43:12,288]: Epoch: 159, Loss:0.1085 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:12,295]: Epoch: 160, Loss:0.0987 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:12,303]: Epoch: 161, Loss:0.1180 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:12,311]: Epoch: 162, Loss:0.1603 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:12,318]: Epoch: 163, Loss:0.1368 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:12,325]: Epoch: 164, Loss:0.1258 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:12,332]: Epoch: 165, Loss:0.1116 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:12,339]: Epoch: 166, Loss:0.1135 Train: 0.9667, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:12,347]: Epoch: 167, Loss:0.1175 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:12,354]: Epoch: 168, Loss:0.1433 Train: 0.9750, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:12,362]: Epoch: 169, Loss:0.1155 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:12,368]: Epoch: 170, Loss:0.1235 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:43:12,377]: Epoch: 171, Loss:0.1280 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:12,384]: Epoch: 172, Loss:0.1434 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:12,392]: Epoch: 173, Loss:0.1052 Train: 0.9750, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:12,399]: Epoch: 174, Loss:0.1289 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:12,406]: Epoch: 175, Loss:0.1183 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:12,412]: Epoch: 176, Loss:0.1318 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:12,419]: Epoch: 177, Loss:0.1150 Train: 0.9583, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:12,426]: Epoch: 178, Loss:0.1382 Train: 0.9583, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:12,433]: Epoch: 179, Loss:0.1376 Train: 0.9583, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:12,439]: Epoch: 180, Loss:0.1586 Train: 0.9750, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:12,447]: Epoch: 181, Loss:0.1287 Train: 0.9750, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:12,454]: Epoch: 182, Loss:0.1201 Train: 0.9750, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:43:12,460]: Epoch: 183, Loss:0.0966 Train: 0.9667, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:12,468]: Epoch: 184, Loss:0.0993 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:12,476]: Epoch: 185, Loss:0.1410 Train: 0.9667, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:12,485]: Epoch: 186, Loss:0.2096 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:12,492]: Epoch: 187, Loss:0.1076 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:12,500]: Epoch: 188, Loss:0.1626 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:12,509]: Epoch: 189, Loss:0.1209 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:12,517]: Epoch: 190, Loss:0.1065 Train: 0.9583, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:12,525]: Epoch: 191, Loss:0.1419 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:12,534]: Epoch: 192, Loss:0.1776 Train: 0.9583, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:12,540]: Epoch: 193, Loss:0.1598 Train: 0.9583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:12,548]: Epoch: 194, Loss:0.1405 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:12,555]: Epoch: 195, Loss:0.1444 Train: 0.9583, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:12,563]: Epoch: 196, Loss:0.1231 Train: 0.9583, Val:0.6375, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:12,571]: Epoch: 197, Loss:0.1428 Train: 0.9667, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:43:12,578]: Epoch: 198, Loss:0.1301 Train: 0.9667, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:12,584]: Epoch: 199, Loss:0.1357 Train: 0.9667, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:12,592]: Epoch: 200, Loss:0.1090 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:12,592]: [Run-3 score] {'train': 0.975, 'val': 0.725, 'test': 0.6078431372549019}
[2025-04-01 02:43:12,592]: repeat 1/3
[2025-04-01 02:43:12,592]: Manual random seed:0
[2025-04-01 02:43:12,593]: auto fixed data split seed to 0, model init seed to 0

Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0099
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0100
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0090
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0080
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0089
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0095
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0097
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0096
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0097
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0094
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0092
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0099
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0100
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0101
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0101
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0100
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0099
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0098
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0095
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0099
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0099
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0099
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0099
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0100
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0078
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0099
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0107
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0100
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0083
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0070
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0078
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0075
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0084
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0097
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0096
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0094
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0074
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0071
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0075
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.6090 Time(s/epoch):0.0050
Epoch: 002, Supervised Loss:1.5131 Time(s/epoch):0.0049
Epoch: 003, Supervised Loss:1.4074 Time(s/epoch):0.0050
Epoch: 004, Supervised Loss:1.3568 Time(s/epoch):0.0047
Epoch: 005, Supervised Loss:1.3883 Time(s/epoch):0.0051
Epoch: 006, Supervised Loss:1.4524 Time(s/epoch):0.0048
Epoch: 007, Supervised Loss:1.4790 Time(s/epoch):0.0050
Epoch: 008, Supervised Loss:1.4697 Time(s/epoch):0.0047
Epoch: 009, Supervised Loss:1.4632 Time(s/epoch):0.0052
Epoch: 010, Supervised Loss:1.4711 Time(s/epoch):0.0048
Epoch: 011, Supervised Loss:1.4785 Time(s/epoch):0.0049
Epoch: 012, Supervised Loss:1.4765 Time(s/epoch):0.0051
Epoch: 013, Supervised Loss:1.4683 Time(s/epoch):0.0050
Epoch: 014, Supervised Loss:1.4616 Time(s/epoch):0.0050
Epoch: 015, Supervised Loss:1.4609 Time(s/epoch):0.0066
Epoch: 016, Supervised Loss:1.4640 Time(s/epoch):0.0061
Epoch: 017, Supervised Loss:1.4650 Time(s/epoch):0.0065
Epoch: 018, Supervised Loss:1.4611 Time(s/epoch):0.0065
Epoch: 019, Supervised Loss:1.4542 Time(s/epoch):0.0060
Epoch: 020, Supervised Loss:1.4473 Time(s/epoch):0.0047
Epoch: 021, Supervised Loss:1.4417 Time(s/epoch):0.0046
Epoch: 022, Supervised Loss:1.4374 Time(s/epoch):0.0047
Epoch: 023, Supervised Loss:1.4344 Time(s/epoch):0.0047
Epoch: 024, Supervised Loss:1.4331 Time(s/epoch):0.0047
Epoch: 025, Supervised Loss:1.4334 Time(s/epoch):0.0045
Epoch: 026, Supervised Loss:1.4343 Time(s/epoch):0.0054
Epoch: 027, Supervised Loss:1.4346 Time(s/epoch):0.0062
Epoch: 028, Supervised Loss:1.4338 Time(s/epoch):0.0063
Epoch: 029, Supervised Loss:1.4325 Time(s/epoch):0.0054
Epoch: 030, Supervised Loss:1.4312 Time(s/epoch):0.0047
0.9250624 515
Add 374 edges.
Prune 371 edges from torch.Size([2, 515]) to torch.Size([2, 144])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.706
Data(x=[251, 1703], edge_index=[2, 513], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.4860 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.706
Data(x=[251, 1703], edge_index=[2, 513], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5026 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.706
Data(x=[251, 1703], edge_index=[2, 513], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5014 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(18)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0178
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0106
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0114
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0079
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0095
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0075
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0080
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0071
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0092
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0115
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0114
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0097
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0095
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0093
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0094
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0094
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0094
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0096
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0095
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0099
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0100
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0096
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0090
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0071
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0087
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0091
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0099
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0100
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0104
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0099
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0100
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0093
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0094
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0092
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0094
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0091
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0070
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0075
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0096
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0095
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0094
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0095
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0090
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0071
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0075
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0107
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0101
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0097
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0094
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0087
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0072
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0075
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0078
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0075
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0097
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0094
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0072
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0070
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0070
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0085
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0097
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0094
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0075
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0085
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0081
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0098
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0088
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0069
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0070
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0072
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0076
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0070
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0071
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0078
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0071
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0092
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0097
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0095
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0078
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0076
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0075
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0074
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0083
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0093
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0094
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0093
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0070
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0069
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0096
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0088
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0069
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0070
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0070
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0069
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0074
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0073
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0077
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0076
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0077
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0072
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0085
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0099
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0100
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0090
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0073
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0073
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0070
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0070
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0073
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0072
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0102
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0089
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0074
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0081
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0096
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0100
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0093
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0092
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0088
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0073
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0073
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0072
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0076
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0096
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0095
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0091
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0081
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0084
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0089
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0095
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0072
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0074
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0071
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0070
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0079
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0072
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0091
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0097
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0077
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0077
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0076
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0090
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0100
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0084
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0084
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0095
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0098
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0088
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0085
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0097
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0093
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0080
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0080
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0079
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0098
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0096
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0102
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0108
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0089
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0100
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0097
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0091
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0096
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0092
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0079
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0078
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0076
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0078
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0074
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0077
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0097
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0099
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0099
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0098
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0099
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0098
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0098
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0101
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0108
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0107
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0095[2025-04-01 02:43:14,539]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:14,548]: Epoch: 001, Loss:1.7021 Train: 0.5250, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:43:14,556]: Epoch: 002, Loss:1.3047 Train: 0.7583, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:14,563]: Epoch: 003, Loss:0.8534 Train: 0.7833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:14,569]: Epoch: 004, Loss:0.7035 Train: 0.8250, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:14,578]: Epoch: 005, Loss:0.5803 Train: 0.8667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:14,586]: Epoch: 006, Loss:0.5281 Train: 0.8917, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:14,595]: Epoch: 007, Loss:0.4680 Train: 0.9083, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:14,601]: Epoch: 008, Loss:0.4175 Train: 0.9167, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:14,609]: Epoch: 009, Loss:0.4134 Train: 0.9083, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:14,618]: Epoch: 010, Loss:0.3944 Train: 0.9250, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:14,626]: Epoch: 011, Loss:0.4072 Train: 0.9500, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:14,634]: Epoch: 012, Loss:0.3323 Train: 0.9417, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:14,641]: Epoch: 013, Loss:0.3673 Train: 0.9500, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:14,650]: Epoch: 014, Loss:0.3247 Train: 0.9500, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:14,656]: Epoch: 015, Loss:0.3734 Train: 0.9500, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:14,664]: Epoch: 016, Loss:0.2646 Train: 0.9333, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:14,671]: Epoch: 017, Loss:0.3359 Train: 0.9417, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:14,679]: Epoch: 018, Loss:0.2830 Train: 0.9500, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:14,686]: Epoch: 019, Loss:0.2564 Train: 0.9500, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:14,694]: Epoch: 020, Loss:0.2497 Train: 0.9500, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:14,700]: Epoch: 021, Loss:0.2296 Train: 0.9583, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:14,708]: Epoch: 022, Loss:0.2388 Train: 0.9583, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:14,716]: Epoch: 023, Loss:0.2790 Train: 0.9583, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:14,724]: Epoch: 024, Loss:0.2544 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:14,732]: Epoch: 025, Loss:0.1986 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:14,739]: Epoch: 026, Loss:0.2075 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:14,747]: Epoch: 027, Loss:0.2251 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:14,754]: Epoch: 028, Loss:0.2122 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:14,761]: Epoch: 029, Loss:0.2655 Train: 0.9500, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:14,769]: Epoch: 030, Loss:0.1750 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:14,777]: Epoch: 031, Loss:0.1887 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:14,783]: Epoch: 032, Loss:0.2255 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:14,792]: Epoch: 033, Loss:0.2479 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:14,798]: Epoch: 034, Loss:0.2175 Train: 0.9583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:14,805]: Epoch: 035, Loss:0.2222 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:14,813]: Epoch: 036, Loss:0.2498 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:14,819]: Epoch: 037, Loss:0.2295 Train: 0.9750, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0055
[2025-04-01 02:43:14,827]: Epoch: 038, Loss:0.2064 Train: 0.9833, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:14,835]: Epoch: 039, Loss:0.2140 Train: 0.9583, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:14,843]: Epoch: 040, Loss:0.2020 Train: 0.9500, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:14,850]: Epoch: 041, Loss:0.2392 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:14,859]: Epoch: 042, Loss:0.2113 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:14,867]: Epoch: 043, Loss:0.2307 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:14,874]: Epoch: 044, Loss:0.2225 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:14,883]: Epoch: 045, Loss:0.2256 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:14,889]: Epoch: 046, Loss:0.1982 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:14,896]: Epoch: 047, Loss:0.2140 Train: 0.9583, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:14,904]: Epoch: 048, Loss:0.2023 Train: 0.9667, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:14,909]: Epoch: 049, Loss:0.1741 Train: 0.9667, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0054
[2025-04-01 02:43:14,917]: Epoch: 050, Loss:0.2309 Train: 0.9583, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:14,924]: Epoch: 051, Loss:0.2108 Train: 0.9583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:14,933]: Epoch: 052, Loss:0.2958 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:14,940]: Epoch: 053, Loss:0.2520 Train: 0.9750, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:14,947]: Epoch: 054, Loss:0.2289 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:14,955]: Epoch: 055, Loss:0.1947 Train: 0.9833, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:14,964]: Epoch: 056, Loss:0.1735 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:14,969]: Epoch: 057, Loss:0.2043 Train: 0.9667, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0058
[2025-04-01 02:43:14,976]: Epoch: 058, Loss:0.2149 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:14,985]: Epoch: 059, Loss:0.1917 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:14,993]: Epoch: 060, Loss:0.2085 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:14,999]: Epoch: 061, Loss:0.1957 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:15,006]: Epoch: 062, Loss:0.2033 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:15,014]: Epoch: 063, Loss:0.2006 Train: 0.9833, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:15,020]: Epoch: 064, Loss:0.1596 Train: 0.9833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:15,028]: Epoch: 065, Loss:0.1772 Train: 0.9833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:15,036]: Epoch: 066, Loss:0.1843 Train: 0.9750, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:15,044]: Epoch: 067, Loss:0.1928 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:15,050]: Epoch: 068, Loss:0.1799 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0055
[2025-04-01 02:43:15,057]: Epoch: 069, Loss:0.1688 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:15,064]: Epoch: 070, Loss:0.1894 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:15,071]: Epoch: 071, Loss:0.1625 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:15,078]: Epoch: 072, Loss:0.1794 Train: 0.9583, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:15,085]: Epoch: 073, Loss:0.1856 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:15,094]: Epoch: 074, Loss:0.1367 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:15,100]: Epoch: 075, Loss:0.1534 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:15,108]: Epoch: 076, Loss:0.2049 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:15,114]: Epoch: 077, Loss:0.1737 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:15,121]: Epoch: 078, Loss:0.1909 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:15,128]: Epoch: 079, Loss:0.1794 Train: 0.9833, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:15,135]: Epoch: 080, Loss:0.1862 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:15,141]: Epoch: 081, Loss:0.1757 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:15,148]: Epoch: 082, Loss:0.1864 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:15,156]: Epoch: 083, Loss:0.1589 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:15,163]: Epoch: 084, Loss:0.1556 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:15,170]: Epoch: 085, Loss:0.1705 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:15,177]: Epoch: 086, Loss:0.1406 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:15,187]: Epoch: 087, Loss:0.1387 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0098
[2025-04-01 02:43:15,196]: Epoch: 088, Loss:0.1962 Train: 0.9583, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:15,204]: Epoch: 089, Loss:0.1929 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:15,210]: Epoch: 090, Loss:0.1702 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:15,217]: Epoch: 091, Loss:0.1377 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:15,225]: Epoch: 092, Loss:0.1399 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:15,233]: Epoch: 093, Loss:0.1931 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:15,241]: Epoch: 094, Loss:0.1661 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:15,249]: Epoch: 095, Loss:0.1764 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,257]: Epoch: 096, Loss:0.1250 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:15,265]: Epoch: 097, Loss:0.1748 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:15,272]: Epoch: 098, Loss:0.1617 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:15,281]: Epoch: 099, Loss:0.1953 Train: 0.9583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0087
[2025-04-01 02:43:15,288]: Epoch: 100, Loss:0.1909 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:15,294]: Epoch: 101, Loss:0.1371 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:15,302]: Epoch: 102, Loss:0.1778 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:15,308]: Epoch: 103, Loss:0.1666 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:15,315]: Epoch: 104, Loss:0.1433 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:15,323]: Epoch: 105, Loss:0.1601 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:15,329]: Epoch: 106, Loss:0.1397 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:43:15,337]: Epoch: 107, Loss:0.1642 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:15,343]: Epoch: 108, Loss:0.1340 Train: 0.9667, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:15,349]: Epoch: 109, Loss:0.1666 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0057
[2025-04-01 02:43:15,356]: Epoch: 110, Loss:0.1594 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:15,363]: Epoch: 111, Loss:0.1700 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:15,370]: Epoch: 112, Loss:0.1888 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:15,377]: Epoch: 113, Loss:0.1552 Train: 0.9583, Val:0.6250, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:15,385]: Epoch: 114, Loss:0.1805 Train: 0.9667, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:15,391]: Epoch: 115, Loss:0.2228 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:15,397]: Epoch: 116, Loss:0.1704 Train: 0.9583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:15,404]: Epoch: 117, Loss:0.1783 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:15,411]: Epoch: 118, Loss:0.2581 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:15,419]: Epoch: 119, Loss:0.2074 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:15,426]: Epoch: 120, Loss:0.2167 Train: 0.9750, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:15,434]: Epoch: 121, Loss:0.1514 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:15,443]: Epoch: 122, Loss:0.1655 Train: 0.9500, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:15,450]: Epoch: 123, Loss:0.2425 Train: 0.9500, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:15,457]: Epoch: 124, Loss:0.1847 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:15,465]: Epoch: 125, Loss:0.2072 Train: 0.9583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:15,473]: Epoch: 126, Loss:0.1835 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:15,479]: Epoch: 127, Loss:0.1669 Train: 0.9667, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0060
[2025-04-01 02:43:15,486]: Epoch: 128, Loss:0.1839 Train: 0.9750, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:15,494]: Epoch: 129, Loss:0.1423 Train: 0.9750, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:15,500]: Epoch: 130, Loss:0.2231 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:15,506]: Epoch: 131, Loss:0.1724 Train: 0.9750, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:15,514]: Epoch: 132, Loss:0.1452 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:15,521]: Epoch: 133, Loss:0.2081 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:15,529]: Epoch: 134, Loss:0.1759 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:15,536]: Epoch: 135, Loss:0.1730 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:15,543]: Epoch: 136, Loss:0.1747 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:15,549]: Epoch: 137, Loss:0.1962 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:15,557]: Epoch: 138, Loss:0.1634 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:15,565]: Epoch: 139, Loss:0.1439 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:15,573]: Epoch: 140, Loss:0.1719 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:15,579]: Epoch: 141, Loss:0.1598 Train: 0.9833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:15,586]: Epoch: 142, Loss:0.1652 Train: 0.9833, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:15,594]: Epoch: 143, Loss:0.1242 Train: 0.9833, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:15,600]: Epoch: 144, Loss:0.1629 Train: 0.9833, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:15,607]: Epoch: 145, Loss:0.1547 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:15,614]: Epoch: 146, Loss:0.1634 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:15,621]: Epoch: 147, Loss:0.2253 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:15,628]: Epoch: 148, Loss:0.1896 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,636]: Epoch: 149, Loss:0.1610 Train: 0.9667, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:15,644]: Epoch: 150, Loss:0.1606 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:15,652]: Epoch: 151, Loss:0.1628 Train: 0.9667, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:15,659]: Epoch: 152, Loss:0.1940 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:15,665]: Epoch: 153, Loss:0.1723 Train: 0.9750, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:15,672]: Epoch: 154, Loss:0.1564 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:15,679]: Epoch: 155, Loss:0.1741 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:15,686]: Epoch: 156, Loss:0.1536 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,693]: Epoch: 157, Loss:0.2003 Train: 0.9583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:15,702]: Epoch: 158, Loss:0.1648 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:15,710]: Epoch: 159, Loss:0.1614 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:15,718]: Epoch: 160, Loss:0.1562 Train: 0.9667, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,724]: Epoch: 161, Loss:0.2151 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:15,730]: Epoch: 162, Loss:0.1811 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:15,738]: Epoch: 163, Loss:0.1258 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:15,746]: Epoch: 164, Loss:0.1654 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,754]: Epoch: 165, Loss:0.1996 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:15,762]: Epoch: 166, Loss:0.2069 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:15,768]: Epoch: 167, Loss:0.1395 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:15,775]: Epoch: 168, Loss:0.1320 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:15,783]: Epoch: 169, Loss:0.1364 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:15,791]: Epoch: 170, Loss:0.1334 Train: 0.9667, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:15,798]: Epoch: 171, Loss:0.2064 Train: 0.9833, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:15,807]: Epoch: 172, Loss:0.1625 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:15,815]: Epoch: 173, Loss:0.1888 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:15,822]: Epoch: 174, Loss:0.1663 Train: 0.9583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:15,830]: Epoch: 175, Loss:0.1795 Train: 0.9667, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:15,838]: Epoch: 176, Loss:0.1739 Train: 0.9667, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:15,846]: Epoch: 177, Loss:0.1889 Train: 0.9750, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:15,855]: Epoch: 178, Loss:0.1575 Train: 0.9667, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0086
[2025-04-01 02:43:15,864]: Epoch: 179, Loss:0.1691 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:15,872]: Epoch: 180, Loss:0.1555 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:15,879]: Epoch: 181, Loss:0.1466 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:15,887]: Epoch: 182, Loss:0.1558 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:15,895]: Epoch: 183, Loss:0.1689 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:15,903]: Epoch: 184, Loss:0.1656 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:15,911]: Epoch: 185, Loss:0.1247 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:15,918]: Epoch: 186, Loss:0.1522 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:15,926]: Epoch: 187, Loss:0.1481 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:15,935]: Epoch: 188, Loss:0.1914 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:15,943]: Epoch: 189, Loss:0.1175 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:15,951]: Epoch: 190, Loss:0.1463 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0080
[2025-04-01 02:43:15,960]: Epoch: 191, Loss:0.1593 Train: 0.9667, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:15,968]: Epoch: 192, Loss:0.1973 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:15,986]: Epoch: 193, Loss:0.1779 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0182
[2025-04-01 02:43:16,016]: Epoch: 194, Loss:0.1385 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0295
[2025-04-01 02:43:16,026]: Epoch: 195, Loss:0.1840 Train: 0.9667, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0104
[2025-04-01 02:43:16,040]: Epoch: 196, Loss:0.1883 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0137
[2025-04-01 02:43:16,048]: Epoch: 197, Loss:0.1631 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:16,056]: Epoch: 198, Loss:0.1495 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:16,063]: Epoch: 199, Loss:0.2983 Train: 0.9750, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:16,072]: Epoch: 200, Loss:0.2480 Train: 0.9750, Val:0.6875, Test: 0.7647, Time(s/epoch):0.0083
[2025-04-01 02:43:16,072]: [Run-1 score] {'train': 0.8666666666666667, 'val': 0.7125, 'test': 0.6862745098039216}
[2025-04-01 02:43:16,072]: repeat 2/3
[2025-04-01 02:43:16,072]: Manual random seed:0
[2025-04-01 02:43:16,072]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:16,077]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:16,088]: Epoch: 001, Loss:1.5249 Train: 0.5667, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:43:16,095]: Epoch: 002, Loss:1.1888 Train: 0.7583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:16,104]: Epoch: 003, Loss:0.8522 Train: 0.8083, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:16,112]: Epoch: 004, Loss:0.6451 Train: 0.8583, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:16,118]: Epoch: 005, Loss:0.5861 Train: 0.8333, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:16,125]: Epoch: 006, Loss:0.5403 Train: 0.8500, Val:0.5750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:16,133]: Epoch: 007, Loss:0.5422 Train: 0.8500, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:16,140]: Epoch: 008, Loss:0.4027 Train: 0.8750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:16,148]: Epoch: 009, Loss:0.4414 Train: 0.9250, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:16,155]: Epoch: 010, Loss:0.3612 Train: 0.9000, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:16,164]: Epoch: 011, Loss:0.3890 Train: 0.8917, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:16,170]: Epoch: 012, Loss:0.3188 Train: 0.9000, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:16,178]: Epoch: 013, Loss:0.3021 Train: 0.9083, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:16,188]: Epoch: 014, Loss:0.4100 Train: 0.9333, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0098
[2025-04-01 02:43:16,196]: Epoch: 015, Loss:0.3682 Train: 0.9583, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:16,204]: Epoch: 016, Loss:0.2885 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:16,212]: Epoch: 017, Loss:0.2808 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:16,220]: Epoch: 018, Loss:0.2575 Train: 0.9750, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:16,226]: Epoch: 019, Loss:0.3057 Train: 0.9750, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:16,234]: Epoch: 020, Loss:0.2355 Train: 0.9750, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:16,241]: Epoch: 021, Loss:0.2360 Train: 0.9750, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:16,249]: Epoch: 022, Loss:0.2216 Train: 0.9667, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:16,257]: Epoch: 023, Loss:0.2468 Train: 0.9583, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:16,265]: Epoch: 024, Loss:0.2387 Train: 0.9667, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:16,272]: Epoch: 025, Loss:0.3210 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:16,279]: Epoch: 026, Loss:0.2968 Train: 0.9833, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:16,287]: Epoch: 027, Loss:0.1784 Train: 0.9833, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:16,294]: Epoch: 028, Loss:0.2014 Train: 0.9583, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:16,300]: Epoch: 029, Loss:0.2633 Train: 0.9583, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:16,308]: Epoch: 030, Loss:0.2588 Train: 0.9583, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:16,316]: Epoch: 031, Loss:0.2352 Train: 0.9667, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:16,324]: Epoch: 032, Loss:0.2026 Train: 0.9833, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:16,331]: Epoch: 033, Loss:0.2303 Train: 0.9750, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:16,338]: Epoch: 034, Loss:0.2244 Train: 0.9750, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:16,345]: Epoch: 035, Loss:0.1988 Train: 0.9667, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:16,352]: Epoch: 036, Loss:0.2246 Train: 0.9750, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:16,359]: Epoch: 037, Loss:0.1967 Train: 0.9750, Val:0.5875, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:16,366]: Epoch: 038, Loss:0.2413 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:16,372]: Epoch: 039, Loss:0.2792 Train: 0.9667, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:16,379]: Epoch: 040, Loss:0.2024 Train: 0.9583, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:16,386]: Epoch: 041, Loss:0.2252 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:16,395]: Epoch: 042, Loss:0.2022 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:16,402]: Epoch: 043, Loss:0.1894 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:16,409]: Epoch: 044, Loss:0.1926 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:16,417]: Epoch: 045, Loss:0.2689 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:16,425]: Epoch: 046, Loss:0.2604 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:16,431]: Epoch: 047, Loss:0.1838 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0055
[2025-04-01 02:43:16,438]: Epoch: 048, Loss:0.1637 Train: 0.9750, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:16,445]: Epoch: 049, Loss:0.2092 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:16,453]: Epoch: 050, Loss:0.1926 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0070
[2025-04-01 02:43:16,460]: Epoch: 051, Loss:0.1835 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:16,469]: Epoch: 052, Loss:0.1964 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:16,476]: Epoch: 053, Loss:0.1759 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:16,482]: Epoch: 054, Loss:0.1841 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:16,490]: Epoch: 055, Loss:0.1870 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:16,498]: Epoch: 056, Loss:0.2155 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:16,506]: Epoch: 057, Loss:0.1931 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:16,514]: Epoch: 058, Loss:0.1788 Train: 0.9667, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:16,520]: Epoch: 059, Loss:0.1841 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0056
[2025-04-01 02:43:16,526]: Epoch: 060, Loss:0.1912 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:16,534]: Epoch: 061, Loss:0.1587 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:16,542]: Epoch: 062, Loss:0.2103 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:16,550]: Epoch: 063, Loss:0.1716 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:16,557]: Epoch: 064, Loss:0.1901 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:16,564]: Epoch: 065, Loss:0.1957 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:16,570]: Epoch: 066, Loss:0.1884 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:16,578]: Epoch: 067, Loss:0.1879 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:16,584]: Epoch: 068, Loss:0.1929 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:16,593]: Epoch: 069, Loss:0.1860 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0087
[2025-04-01 02:43:16,599]: Epoch: 070, Loss:0.1711 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:43:16,606]: Epoch: 071, Loss:0.2753 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:16,613]: Epoch: 072, Loss:0.1764 Train: 0.9917, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:16,619]: Epoch: 073, Loss:0.1888 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:16,628]: Epoch: 074, Loss:0.1893 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:16,637]: Epoch: 075, Loss:0.1963 Train: 0.9833, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0089
[2025-04-01 02:43:16,645]: Epoch: 076, Loss:0.1658 Train: 0.9833, Val:0.6125, Test: 0.7255, Time(s/epoch):0.0085
[2025-04-01 02:43:16,654]: Epoch: 077, Loss:0.1823 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:16,661]: Epoch: 078, Loss:0.2185 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:16,670]: Epoch: 079, Loss:0.2036 Train: 0.9833, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:16,677]: Epoch: 080, Loss:0.1465 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:16,684]: Epoch: 081, Loss:0.1888 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:16,693]: Epoch: 082, Loss:0.1841 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0092
[2025-04-01 02:43:16,701]: Epoch: 083, Loss:0.2013 Train: 0.9833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:16,709]: Epoch: 084, Loss:0.1833 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:16,717]: Epoch: 085, Loss:0.1873 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:16,725]: Epoch: 086, Loss:0.1951 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:16,731]: Epoch: 087, Loss:0.1529 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:43:16,739]: Epoch: 088, Loss:0.1952 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:16,747]: Epoch: 089, Loss:0.2001 Train: 0.9667, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:16,753]: Epoch: 090, Loss:0.1867 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:16,760]: Epoch: 091, Loss:0.1713 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:16,768]: Epoch: 092, Loss:0.1410 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:16,775]: Epoch: 093, Loss:0.1975 Train: 0.9667, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:16,782]: Epoch: 094, Loss:0.1461 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:16,790]: Epoch: 095, Loss:0.1578 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:16,798]: Epoch: 096, Loss:0.1469 Train: 0.9833, Val:0.5500, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:16,805]: Epoch: 097, Loss:0.1740 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:16,813]: Epoch: 098, Loss:0.1757 Train: 0.9750, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:16,821]: Epoch: 099, Loss:0.2153 Train: 0.9833, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:16,828]: Epoch: 100, Loss:0.1650 Train: 0.9667, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:16,836]: Epoch: 101, Loss:0.1441 Train: 0.9583, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:16,843]: Epoch: 102, Loss:0.2193 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:16,849]: Epoch: 103, Loss:0.1979 Train: 0.9750, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:16,856]: Epoch: 104, Loss:0.1712 Train: 0.9750, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:16,863]: Epoch: 105, Loss:0.1859 Train: 0.9833, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0070
[2025-04-01 02:43:16,871]: Epoch: 106, Loss:0.1619 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:16,878]: Epoch: 107, Loss:0.1676 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:16,886]: Epoch: 108, Loss:0.1667 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:16,893]: Epoch: 109, Loss:0.1693 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:16,899]: Epoch: 110, Loss:0.1685 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:43:16,907]: Epoch: 111, Loss:0.1570 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:16,915]: Epoch: 112, Loss:0.2009 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:16,923]: Epoch: 113, Loss:0.2038 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:16,929]: Epoch: 114, Loss:0.1458 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:16,937]: Epoch: 115, Loss:0.1883 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:16,944]: Epoch: 116, Loss:0.1648 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:16,951]: Epoch: 117, Loss:0.1230 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:16,958]: Epoch: 118, Loss:0.1914 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:16,966]: Epoch: 119, Loss:0.1265 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:16,973]: Epoch: 120, Loss:0.1446 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:16,981]: Epoch: 121, Loss:0.1467 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:16,988]: Epoch: 122, Loss:0.1946 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:16,995]: Epoch: 123, Loss:0.1546 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:17,002]: Epoch: 124, Loss:0.1730 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:17,010]: Epoch: 125, Loss:0.1967 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:17,018]: Epoch: 126, Loss:0.1433 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:17,025]: Epoch: 127, Loss:0.1555 Train: 0.9583, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:17,031]: Epoch: 128, Loss:0.2099 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:17,038]: Epoch: 129, Loss:0.1614 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:17,045]: Epoch: 130, Loss:0.1126 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:17,053]: Epoch: 131, Loss:0.1258 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:17,061]: Epoch: 132, Loss:0.1830 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:17,068]: Epoch: 133, Loss:0.1955 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:17,076]: Epoch: 134, Loss:0.1615 Train: 0.9917, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:17,083]: Epoch: 135, Loss:0.1470 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:17,090]: Epoch: 136, Loss:0.1901 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:17,098]: Epoch: 137, Loss:0.1474 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:17,106]: Epoch: 138, Loss:0.1450 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:17,112]: Epoch: 139, Loss:0.1426 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:17,119]: Epoch: 140, Loss:0.1219 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:17,126]: Epoch: 141, Loss:0.1658 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:17,134]: Epoch: 142, Loss:0.1821 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:17,143]: Epoch: 143, Loss:0.1352 Train: 0.9667, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0086
[2025-04-01 02:43:17,150]: Epoch: 144, Loss:0.1779 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:17,158]: Epoch: 145, Loss:0.1675 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:17,164]: Epoch: 146, Loss:0.1337 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:17,171]: Epoch: 147, Loss:0.1678 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:17,179]: Epoch: 148, Loss:0.1832 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:17,189]: Epoch: 149, Loss:0.2417 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0093
[2025-04-01 02:43:17,197]: Epoch: 150, Loss:0.2220 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:17,204]: Epoch: 151, Loss:0.2405 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:17,212]: Epoch: 152, Loss:0.1816 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:17,219]: Epoch: 153, Loss:0.1815 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:17,227]: Epoch: 154, Loss:0.2494 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:17,234]: Epoch: 155, Loss:0.1681 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:17,240]: Epoch: 156, Loss:0.1944 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:17,247]: Epoch: 157, Loss:0.1878 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:17,255]: Epoch: 158, Loss:0.1415 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:17,263]: Epoch: 159, Loss:0.1472 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:17,270]: Epoch: 160, Loss:0.1504 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:17,277]: Epoch: 161, Loss:0.3629 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:17,285]: Epoch: 162, Loss:0.2388 Train: 0.9833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:17,293]: Epoch: 163, Loss:0.1498 Train: 0.9833, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:17,299]: Epoch: 164, Loss:0.1801 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:17,307]: Epoch: 165, Loss:0.1826 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:17,315]: Epoch: 166, Loss:0.1920 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:17,321]: Epoch: 167, Loss:0.1463 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:17,328]: Epoch: 168, Loss:0.1346 Train: 0.9833, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0070
[2025-04-01 02:43:17,335]: Epoch: 169, Loss:0.1489 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:17,341]: Epoch: 170, Loss:0.1949 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:17,348]: Epoch: 171, Loss:0.1661 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:17,355]: Epoch: 172, Loss:0.1516 Train: 0.9833, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:17,362]: Epoch: 173, Loss:0.1292 Train: 0.9833, Val:0.6125, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:17,369]: Epoch: 174, Loss:0.1339 Train: 0.9833, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:17,377]: Epoch: 175, Loss:0.1772 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,385]: Epoch: 176, Loss:0.1604 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:17,393]: Epoch: 177, Loss:0.1602 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:17,399]: Epoch: 178, Loss:0.1587 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:17,406]: Epoch: 179, Loss:0.1819 Train: 0.9917, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:17,413]: Epoch: 180, Loss:0.1773 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:17,419]: Epoch: 181, Loss:0.1966 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:17,428]: Epoch: 182, Loss:0.1679 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:17,436]: Epoch: 183, Loss:0.2457 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:17,443]: Epoch: 184, Loss:0.1486 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,451]: Epoch: 185, Loss:0.1721 Train: 0.9667, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:17,459]: Epoch: 186, Loss:0.1441 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:17,465]: Epoch: 187, Loss:0.1579 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:17,473]: Epoch: 188, Loss:0.1687 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:17,479]: Epoch: 189, Loss:0.1849 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:17,486]: Epoch: 190, Loss:0.1501 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:17,493]: Epoch: 191, Loss:0.1214 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:17,501]: Epoch: 192, Loss:0.1646 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:17,508]: Epoch: 193, Loss:0.1260 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:17,516]: Epoch: 194, Loss:0.1810 Train: 0.9833, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:17,524]: Epoch: 195, Loss:0.1189 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:17,531]: Epoch: 196, Loss:0.1679 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:17,538]: Epoch: 197, Loss:0.1739 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:17,545]: Epoch: 198, Loss:0.1316 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:17,552]: Epoch: 199, Loss:0.1755 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:17,559]: Epoch: 200, Loss:0.1462 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:17,559]: [Run-2 score] {'train': 0.9, 'val': 0.7, 'test': 0.6862745098039216}
[2025-04-01 02:43:17,559]: repeat 3/3
[2025-04-01 02:43:17,559]: Manual random seed:0
[2025-04-01 02:43:17,560]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:17,563]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:17,573]: Epoch: 001, Loss:1.7047 Train: 0.6833, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:43:17,579]: Epoch: 002, Loss:1.0950 Train: 0.7750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:17,586]: Epoch: 003, Loss:0.9655 Train: 0.8333, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:17,593]: Epoch: 004, Loss:0.6501 Train: 0.8750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:17,601]: Epoch: 005, Loss:0.5634 Train: 0.8833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:17,608]: Epoch: 006, Loss:0.5212 Train: 0.8750, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:17,614]: Epoch: 007, Loss:0.4628 Train: 0.8917, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:17,621]: Epoch: 008, Loss:0.4864 Train: 0.9000, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:17,627]: Epoch: 009, Loss:0.4344 Train: 0.9000, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:17,635]: Epoch: 010, Loss:0.3911 Train: 0.9167, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:17,643]: Epoch: 011, Loss:0.3774 Train: 0.9333, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:17,649]: Epoch: 012, Loss:0.4572 Train: 0.9417, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0058
[2025-04-01 02:43:17,657]: Epoch: 013, Loss:0.3452 Train: 0.9250, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:17,665]: Epoch: 014, Loss:0.3284 Train: 0.9333, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,673]: Epoch: 015, Loss:0.3451 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:17,680]: Epoch: 016, Loss:0.3049 Train: 0.9500, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:17,687]: Epoch: 017, Loss:0.3056 Train: 0.9333, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:17,694]: Epoch: 018, Loss:0.3461 Train: 0.9333, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:17,701]: Epoch: 019, Loss:0.2890 Train: 0.9417, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:17,708]: Epoch: 020, Loss:0.2729 Train: 0.9417, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:17,714]: Epoch: 021, Loss:0.2837 Train: 0.9500, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:17,722]: Epoch: 022, Loss:0.2562 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:17,729]: Epoch: 023, Loss:0.2843 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:17,737]: Epoch: 024, Loss:0.2555 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:17,745]: Epoch: 025, Loss:0.2784 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:17,751]: Epoch: 026, Loss:0.2626 Train: 0.9583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0056
[2025-04-01 02:43:17,759]: Epoch: 027, Loss:0.2088 Train: 0.9583, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:17,767]: Epoch: 028, Loss:0.2484 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:17,774]: Epoch: 029, Loss:0.2374 Train: 0.9833, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:17,781]: Epoch: 030, Loss:0.2375 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:17,790]: Epoch: 031, Loss:0.2090 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0087
[2025-04-01 02:43:17,796]: Epoch: 032, Loss:0.2784 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:17,803]: Epoch: 033, Loss:0.2414 Train: 0.9583, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:17,811]: Epoch: 034, Loss:0.2307 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,819]: Epoch: 035, Loss:0.2597 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,826]: Epoch: 036, Loss:0.2164 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:17,834]: Epoch: 037, Loss:0.2349 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:17,841]: Epoch: 038, Loss:0.2386 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:17,849]: Epoch: 039, Loss:0.2191 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:17,857]: Epoch: 040, Loss:0.2272 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:17,865]: Epoch: 041, Loss:0.2247 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:17,872]: Epoch: 042, Loss:0.2182 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:17,880]: Epoch: 043, Loss:0.2041 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:17,887]: Epoch: 044, Loss:0.2534 Train: 0.9750, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:17,893]: Epoch: 045, Loss:0.1868 Train: 0.9500, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:17,901]: Epoch: 046, Loss:0.2307 Train: 0.9583, Val:0.5875, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:17,909]: Epoch: 047, Loss:0.2667 Train: 0.9583, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:17,915]: Epoch: 048, Loss:0.2583 Train: 0.9750, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:17,923]: Epoch: 049, Loss:0.1947 Train: 0.9667, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:17,931]: Epoch: 050, Loss:0.2238 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:17,938]: Epoch: 051, Loss:0.2675 Train: 0.9667, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:17,946]: Epoch: 052, Loss:0.1910 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:17,952]: Epoch: 053, Loss:0.1950 Train: 0.9833, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:17,960]: Epoch: 054, Loss:0.2375 Train: 0.9750, Val:0.5875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:17,967]: Epoch: 055, Loss:0.2016 Train: 0.9667, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:17,975]: Epoch: 056, Loss:0.2320 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:17,984]: Epoch: 057, Loss:0.2206 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:17,989]: Epoch: 058, Loss:0.1664 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0056
[2025-04-01 02:43:17,997]: Epoch: 059, Loss:0.1953 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:18,003]: Epoch: 060, Loss:0.1983 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:18,011]: Epoch: 061, Loss:0.2045 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:18,018]: Epoch: 062, Loss:0.1938 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:18,025]: Epoch: 063, Loss:0.2254 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:18,032]: Epoch: 064, Loss:0.1985 Train: 0.9667, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:18,040]: Epoch: 065, Loss:0.1755 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:18,048]: Epoch: 066, Loss:0.2159 Train: 0.9667, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:18,056]: Epoch: 067, Loss:0.2717 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:18,064]: Epoch: 068, Loss:0.1936 Train: 0.9750, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:18,073]: Epoch: 069, Loss:0.2268 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0087
[2025-04-01 02:43:18,081]: Epoch: 070, Loss:0.1862 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:18,089]: Epoch: 071, Loss:0.1749 Train: 0.9750, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:18,096]: Epoch: 072, Loss:0.1594 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:18,105]: Epoch: 073, Loss:0.2082 Train: 0.9833, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:18,114]: Epoch: 074, Loss:0.1929 Train: 0.9833, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0091
[2025-04-01 02:43:18,123]: Epoch: 075, Loss:0.1832 Train: 0.9833, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:18,130]: Epoch: 076, Loss:0.2023 Train: 0.9833, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:18,137]: Epoch: 077, Loss:0.1866 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:18,146]: Epoch: 078, Loss:0.1985 Train: 0.9833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:18,154]: Epoch: 079, Loss:0.1699 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:18,160]: Epoch: 080, Loss:0.1934 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:18,169]: Epoch: 081, Loss:0.1739 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:18,178]: Epoch: 082, Loss:0.1772 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0089
[2025-04-01 02:43:18,188]: Epoch: 083, Loss:0.1735 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0101
[2025-04-01 02:43:18,197]: Epoch: 084, Loss:0.1956 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:43:18,204]: Epoch: 085, Loss:0.2042 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:18,210]: Epoch: 086, Loss:0.1773 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:18,219]: Epoch: 087, Loss:0.1693 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0088
[2025-04-01 02:43:18,227]: Epoch: 088, Loss:0.2406 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:18,235]: Epoch: 089, Loss:0.2097 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:18,243]: Epoch: 090, Loss:0.2073 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:18,250]: Epoch: 091, Loss:0.2205 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:18,256]: Epoch: 092, Loss:0.1859 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:18,264]: Epoch: 093, Loss:0.2160 Train: 0.9750, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:18,270]: Epoch: 094, Loss:0.2089 Train: 0.9750, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0058
[2025-04-01 02:43:18,277]: Epoch: 095, Loss:0.1741 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:18,285]: Epoch: 096, Loss:0.2068 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:18,294]: Epoch: 097, Loss:0.1572 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:18,300]: Epoch: 098, Loss:0.2305 Train: 0.9667, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:18,307]: Epoch: 099, Loss:0.1926 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:18,314]: Epoch: 100, Loss:0.1790 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:18,322]: Epoch: 101, Loss:0.2391 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:18,328]: Epoch: 102, Loss:0.2401 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:18,334]: Epoch: 103, Loss:0.1874 Train: 0.9500, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:18,341]: Epoch: 104, Loss:0.2115 Train: 0.9583, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:18,349]: Epoch: 105, Loss:0.2144 Train: 0.9583, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:18,357]: Epoch: 106, Loss:0.1784 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:18,365]: Epoch: 107, Loss:0.1824 Train: 0.9583, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:18,372]: Epoch: 108, Loss:0.1781 Train: 0.9583, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:18,379]: Epoch: 109, Loss:0.1857 Train: 0.9667, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:18,387]: Epoch: 110, Loss:0.1538 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:18,396]: Epoch: 111, Loss:0.1759 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:18,403]: Epoch: 112, Loss:0.1830 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:18,409]: Epoch: 113, Loss:0.1833 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:18,417]: Epoch: 114, Loss:0.1482 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:18,424]: Epoch: 115, Loss:0.2316 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:18,432]: Epoch: 116, Loss:0.2160 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:18,439]: Epoch: 117, Loss:0.2220 Train: 0.9667, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:18,446]: Epoch: 118, Loss:0.1931 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:18,454]: Epoch: 119, Loss:0.2567 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:18,462]: Epoch: 120, Loss:0.2332 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:18,469]: Epoch: 121, Loss:0.1785 Train: 0.9583, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:18,476]: Epoch: 122, Loss:0.2077 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:18,483]: Epoch: 123, Loss:0.2536 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:18,489]: Epoch: 124, Loss:0.1788 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:18,497]: Epoch: 125, Loss:0.1901 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:18,505]: Epoch: 126, Loss:0.1307 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:18,513]: Epoch: 127, Loss:0.1856 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:18,519]: Epoch: 128, Loss:0.2609 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:18,526]: Epoch: 129, Loss:0.2076 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:18,532]: Epoch: 130, Loss:0.1958 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:18,539]: Epoch: 131, Loss:0.2156 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:18,547]: Epoch: 132, Loss:0.1471 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:18,555]: Epoch: 133, Loss:0.1339 Train: 0.9583, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:18,561]: Epoch: 134, Loss:0.2031 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:18,569]: Epoch: 135, Loss:0.1794 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:18,578]: Epoch: 136, Loss:0.1678 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:18,585]: Epoch: 137, Loss:0.1887 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:18,594]: Epoch: 138, Loss:0.1655 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:18,601]: Epoch: 139, Loss:0.1612 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:18,609]: Epoch: 140, Loss:0.1699 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:18,616]: Epoch: 141, Loss:0.1613 Train: 0.9667, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:18,624]: Epoch: 142, Loss:0.2185 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:18,629]: Epoch: 143, Loss:0.1578 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0055
[2025-04-01 02:43:18,636]: Epoch: 144, Loss:0.1465 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:18,644]: Epoch: 145, Loss:0.1774 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:18,651]: Epoch: 146, Loss:0.1806 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:18,659]: Epoch: 147, Loss:0.1388 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:18,665]: Epoch: 148, Loss:0.1590 Train: 0.9667, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:18,672]: Epoch: 149, Loss:0.1485 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:18,679]: Epoch: 150, Loss:0.1608 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:18,686]: Epoch: 151, Loss:0.1377 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:18,693]: Epoch: 152, Loss:0.1549 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:18,701]: Epoch: 153, Loss:0.1645 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:18,709]: Epoch: 154, Loss:0.1315 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:18,718]: Epoch: 155, Loss:0.1583 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:18,725]: Epoch: 156, Loss:0.1980 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:18,734]: Epoch: 157, Loss:0.1826 Train: 0.9833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:43:18,741]: Epoch: 158, Loss:0.1738 Train: 0.9833, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:18,749]: Epoch: 159, Loss:0.1839 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:18,758]: Epoch: 160, Loss:0.1602 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:18,765]: Epoch: 161, Loss:0.1439 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:18,774]: Epoch: 162, Loss:0.1879 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:18,782]: Epoch: 163, Loss:0.1494 Train: 0.9667, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:18,790]: Epoch: 164, Loss:0.1594 Train: 0.9667, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:18,798]: Epoch: 165, Loss:0.2307 Train: 0.9667, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:18,805]: Epoch: 166, Loss:0.1970 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:18,813]: Epoch: 167, Loss:0.1443 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:18,819]: Epoch: 168, Loss:0.1970 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0056
[2025-04-01 02:43:18,826]: Epoch: 169, Loss:0.1416 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:18,833]: Epoch: 170, Loss:0.1613 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:18,841]: Epoch: 171, Loss:0.1648 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:18,848]: Epoch: 172, Loss:0.2105 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:18,856]: Epoch: 173, Loss:0.1370 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:18,863]: Epoch: 174, Loss:0.1522 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:18,870]: Epoch: 175, Loss:0.1732 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:18,877]: Epoch: 176, Loss:0.1526 Train: 0.9833, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:18,885]: Epoch: 177, Loss:0.1367 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:18,891]: Epoch: 178, Loss:0.1460 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:18,898]: Epoch: 179, Loss:0.1314 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:18,905]: Epoch: 180, Loss:0.2014 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:18,911]: Epoch: 181, Loss:0.1782 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:18,918]: Epoch: 182, Loss:0.1429 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:18,924]: Epoch: 183, Loss:0.1615 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:18,931]: Epoch: 184, Loss:0.1247 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:18,938]: Epoch: 185, Loss:0.1738 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:18,945]: Epoch: 186, Loss:0.1801 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:18,952]: Epoch: 187, Loss:0.1587 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:18,960]: Epoch: 188, Loss:0.1292 Train: 0.9833, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:43:18,968]: Epoch: 189, Loss:0.2239 Train: 0.9917, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:18,976]: Epoch: 190, Loss:0.1563 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:18,982]: Epoch: 191, Loss:0.1325 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:18,989]: Epoch: 192, Loss:0.1396 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:18,996]: Epoch: 193, Loss:0.2201 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:19,004]: Epoch: 194, Loss:0.1345 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:19,012]: Epoch: 195, Loss:0.1535 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:19,019]: Epoch: 196, Loss:0.2147 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:19,026]: Epoch: 197, Loss:0.1648 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:19,032]: Epoch: 198, Loss:0.1900 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:19,040]: Epoch: 199, Loss:0.2094 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:19,048]: Epoch: 200, Loss:0.1886 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:19,048]: [Run-3 score] {'train': 0.8333333333333334, 'val': 0.725, 'test': 0.6470588235294118}
[2025-04-01 02:43:19,048]: repeat 1/3
[2025-04-01 02:43:19,048]: Manual random seed:0
[2025-04-01 02:43:19,049]: auto fixed data split seed to 0, model init seed to 0

Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0115
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0098
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0080
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0091
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0078
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0096
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0092
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0077
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0092
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0095
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0094
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0101
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0098
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0098
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0100
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0097
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0095
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0093
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0074
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.9632 Time(s/epoch):0.0049
Epoch: 002, Supervised Loss:1.8779 Time(s/epoch):0.0061
Epoch: 003, Supervised Loss:1.7838 Time(s/epoch):0.0064
Epoch: 004, Supervised Loss:1.7203 Time(s/epoch):0.0062
Epoch: 005, Supervised Loss:1.7009 Time(s/epoch):0.0062
Epoch: 006, Supervised Loss:1.7308 Time(s/epoch):0.0062
Epoch: 007, Supervised Loss:1.7744 Time(s/epoch):0.0062
Epoch: 008, Supervised Loss:1.7948 Time(s/epoch):0.0063
Epoch: 009, Supervised Loss:1.7975 Time(s/epoch):0.0064
Epoch: 010, Supervised Loss:1.8031 Time(s/epoch):0.0064
Epoch: 011, Supervised Loss:1.8209 Time(s/epoch):0.0062
Epoch: 012, Supervised Loss:1.8441 Time(s/epoch):0.0051
Epoch: 013, Supervised Loss:1.8630 Time(s/epoch):0.0048
Epoch: 014, Supervised Loss:1.8752 Time(s/epoch):0.0046
Epoch: 015, Supervised Loss:1.8838 Time(s/epoch):0.0056
Epoch: 016, Supervised Loss:1.8911 Time(s/epoch):0.0064
Epoch: 017, Supervised Loss:1.8964 Time(s/epoch):0.0062
Epoch: 018, Supervised Loss:1.8997 Time(s/epoch):0.0056
Epoch: 019, Supervised Loss:1.9019 Time(s/epoch):0.0054
Epoch: 020, Supervised Loss:1.9032 Time(s/epoch):0.0057
Epoch: 021, Supervised Loss:1.9025 Time(s/epoch):0.0050
Epoch: 022, Supervised Loss:1.8993 Time(s/epoch):0.0048
Epoch: 023, Supervised Loss:1.8951 Time(s/epoch):0.0047
Epoch: 024, Supervised Loss:1.8917 Time(s/epoch):0.0068
Epoch: 025, Supervised Loss:1.8901 Time(s/epoch):0.0063
Epoch: 026, Supervised Loss:1.8905 Time(s/epoch):0.0054
Epoch: 027, Supervised Loss:1.8929 Time(s/epoch):0.0049
Epoch: 028, Supervised Loss:1.8971 Time(s/epoch):0.0048
Epoch: 029, Supervised Loss:1.9025 Time(s/epoch):0.0056
Epoch: 030, Supervised Loss:1.9075 Time(s/epoch):0.0061
0.95983636 515
Add 331 edges.
Prune 355 edges from torch.Size([2, 515]) to torch.Size([2, 160])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.663
Data(x=[251, 1703], edge_index=[2, 487], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.4760 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.663
Data(x=[251, 1703], edge_index=[2, 487], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4838 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.663
Data(x=[251, 1703], edge_index=[2, 487], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4857 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(33)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0137
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0080
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0110
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0085
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0094
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0082
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0073
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0083
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0094
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0095
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0094
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0079
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0074
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0088
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0106
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0100
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0100
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0087
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0075
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0077
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0076
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0092
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0098
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0076
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0078
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0075
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0083
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0085
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0084
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0077
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0078
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0075
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0077
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0075
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0085
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0075
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0075
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0073
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0079
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0099
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0095
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0095
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0093
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0095
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0093
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0093
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0093
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0093
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0091
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0073
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0074
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0076
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0076
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0072
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0072
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0076
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0070
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0101
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0098
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0096
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0080
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0076
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0076
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0073
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0075
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0089
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0098
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0103
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0096
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0096
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0093
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0097
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0092
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0073
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0080
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0093
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0081
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0074
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0078
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0071
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0086
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0096
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0096
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0095
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0095
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0095
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0078
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0078
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0074
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0075
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0074
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0075
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0072
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0074
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0073
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0074
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0073
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0076
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0076
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0087
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0098
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0107
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0100
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0075
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0078
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0074
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0082
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0094
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0078
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0071
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0075
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0077
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0078
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0075
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0082
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0074
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0073
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0075
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0082
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0097
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0101
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0095
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0078
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0072
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0093
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0099
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0104
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0086
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0083
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0096
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0100
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0094
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0112
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0088
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0077
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0088
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0082
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0077
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0078
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0084
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0100
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0100
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0093
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0075
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0069
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0094
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0095
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0077
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0074
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0073
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0070
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0074
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0073
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0077
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0074
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0074
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0073
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0072
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0073
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0073
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0071
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0072
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0071
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0072
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0073
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0074
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0076
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0075
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0077
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0073
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0074
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0074
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0075
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0075
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0076
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0075
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0070
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0070
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0074
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0075
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0081
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0094
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0081
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0075
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0077
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0075
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0082
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0096
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0085
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0074
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0073
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0066
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0072
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0070
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0071
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0073
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0077
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0080
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0078
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0133
[Train] best epoch:11 | min reconstruction loss:0.0756[2025-04-01 02:43:20,921]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:20,931]: Epoch: 001, Loss:1.7209 Train: 0.4833, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:43:20,939]: Epoch: 002, Loss:1.3159 Train: 0.7667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:20,946]: Epoch: 003, Loss:0.7654 Train: 0.8333, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:20,953]: Epoch: 004, Loss:0.6061 Train: 0.8417, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:20,961]: Epoch: 005, Loss:0.6167 Train: 0.8667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:20,968]: Epoch: 006, Loss:0.4875 Train: 0.8833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:20,975]: Epoch: 007, Loss:0.4313 Train: 0.9250, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:20,983]: Epoch: 008, Loss:0.3616 Train: 0.9333, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:20,989]: Epoch: 009, Loss:0.3959 Train: 0.9250, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:20,997]: Epoch: 010, Loss:0.3261 Train: 0.9167, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:21,006]: Epoch: 011, Loss:0.3377 Train: 0.9250, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:21,014]: Epoch: 012, Loss:0.2932 Train: 0.9250, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:21,020]: Epoch: 013, Loss:0.3118 Train: 0.9333, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:21,028]: Epoch: 014, Loss:0.2967 Train: 0.9333, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,034]: Epoch: 015, Loss:0.2424 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:21,041]: Epoch: 016, Loss:0.2104 Train: 0.9500, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:21,049]: Epoch: 017, Loss:0.2121 Train: 0.9417, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:21,057]: Epoch: 018, Loss:0.3057 Train: 0.9583, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:21,065]: Epoch: 019, Loss:0.2518 Train: 0.9500, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:21,073]: Epoch: 020, Loss:0.2128 Train: 0.9167, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0086
[2025-04-01 02:43:21,080]: Epoch: 021, Loss:0.2203 Train: 0.9250, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:21,087]: Epoch: 022, Loss:0.2600 Train: 0.9500, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:21,094]: Epoch: 023, Loss:0.3033 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:21,101]: Epoch: 024, Loss:0.2037 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:21,109]: Epoch: 025, Loss:0.2308 Train: 0.9417, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:21,117]: Epoch: 026, Loss:0.1745 Train: 0.9417, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:21,124]: Epoch: 027, Loss:0.1932 Train: 0.9333, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:21,130]: Epoch: 028, Loss:0.2181 Train: 0.9417, Val:0.5500, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:21,137]: Epoch: 029, Loss:0.2153 Train: 0.9583, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:21,146]: Epoch: 030, Loss:0.1896 Train: 0.9667, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0086
[2025-04-01 02:43:21,155]: Epoch: 031, Loss:0.1441 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:21,161]: Epoch: 032, Loss:0.1767 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:21,168]: Epoch: 033, Loss:0.1857 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:21,174]: Epoch: 034, Loss:0.1541 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:21,181]: Epoch: 035, Loss:0.1516 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:21,191]: Epoch: 036, Loss:0.1357 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0097
[2025-04-01 02:43:21,198]: Epoch: 037, Loss:0.1540 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:21,206]: Epoch: 038, Loss:0.1462 Train: 0.9583, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:21,214]: Epoch: 039, Loss:0.1382 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:21,221]: Epoch: 040, Loss:0.1498 Train: 0.9583, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:21,229]: Epoch: 041, Loss:0.1500 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:21,237]: Epoch: 042, Loss:0.1422 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:21,244]: Epoch: 043, Loss:0.1591 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:21,251]: Epoch: 044, Loss:0.1275 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:21,260]: Epoch: 045, Loss:0.1387 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:43:21,267]: Epoch: 046, Loss:0.1741 Train: 0.9833, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:21,275]: Epoch: 047, Loss:0.1321 Train: 0.9833, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:21,283]: Epoch: 048, Loss:0.1421 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:21,291]: Epoch: 049, Loss:0.1121 Train: 0.9750, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:21,299]: Epoch: 050, Loss:0.1468 Train: 0.9667, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:21,308]: Epoch: 051, Loss:0.1033 Train: 0.9500, Val:0.5500, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:21,316]: Epoch: 052, Loss:0.1591 Train: 0.9750, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:21,323]: Epoch: 053, Loss:0.1156 Train: 0.9750, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:21,333]: Epoch: 054, Loss:0.1355 Train: 0.9750, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0093
[2025-04-01 02:43:21,338]: Epoch: 055, Loss:0.1212 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0054
[2025-04-01 02:43:21,346]: Epoch: 056, Loss:0.1306 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:21,354]: Epoch: 057, Loss:0.1711 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:21,361]: Epoch: 058, Loss:0.1228 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:21,369]: Epoch: 059, Loss:0.1419 Train: 0.9750, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:21,376]: Epoch: 060, Loss:0.1661 Train: 0.9833, Val:0.5375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:21,384]: Epoch: 061, Loss:0.1263 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:21,390]: Epoch: 062, Loss:0.1638 Train: 0.9750, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:21,397]: Epoch: 063, Loss:0.1362 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:21,405]: Epoch: 064, Loss:0.1429 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,413]: Epoch: 065, Loss:0.1096 Train: 0.9917, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,419]: Epoch: 066, Loss:0.1087 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0056
[2025-04-01 02:43:21,427]: Epoch: 067, Loss:0.1088 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:21,434]: Epoch: 068, Loss:0.1258 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:21,442]: Epoch: 069, Loss:0.1368 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,448]: Epoch: 070, Loss:0.1266 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:43:21,456]: Epoch: 071, Loss:0.1105 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:21,464]: Epoch: 072, Loss:0.1068 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:21,471]: Epoch: 073, Loss:0.1200 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:21,478]: Epoch: 074, Loss:0.1249 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:21,485]: Epoch: 075, Loss:0.1171 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:21,491]: Epoch: 076, Loss:0.1097 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:21,500]: Epoch: 077, Loss:0.1105 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:21,508]: Epoch: 078, Loss:0.1235 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:21,515]: Epoch: 079, Loss:0.1252 Train: 0.9667, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:21,523]: Epoch: 080, Loss:0.1137 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:21,531]: Epoch: 081, Loss:0.1000 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,538]: Epoch: 082, Loss:0.1255 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:21,544]: Epoch: 083, Loss:0.1116 Train: 0.9750, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:21,553]: Epoch: 084, Loss:0.1093 Train: 0.9750, Val:0.5375, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:21,559]: Epoch: 085, Loss:0.1012 Train: 0.9917, Val:0.5375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:21,566]: Epoch: 086, Loss:0.0989 Train: 0.9917, Val:0.5375, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:21,574]: Epoch: 087, Loss:0.1166 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:21,582]: Epoch: 088, Loss:0.1486 Train: 0.9917, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:21,590]: Epoch: 089, Loss:0.1261 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:21,597]: Epoch: 090, Loss:0.1135 Train: 0.9750, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:21,605]: Epoch: 091, Loss:0.1345 Train: 0.9750, Val:0.5375, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:21,614]: Epoch: 092, Loss:0.1206 Train: 0.9833, Val:0.5375, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:21,619]: Epoch: 093, Loss:0.1102 Train: 0.9750, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0056
[2025-04-01 02:43:21,627]: Epoch: 094, Loss:0.1188 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,635]: Epoch: 095, Loss:0.1263 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,643]: Epoch: 096, Loss:0.1055 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:21,651]: Epoch: 097, Loss:0.1190 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:21,659]: Epoch: 098, Loss:0.1310 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:21,666]: Epoch: 099, Loss:0.1309 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:21,675]: Epoch: 100, Loss:0.1723 Train: 0.9833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:21,683]: Epoch: 101, Loss:0.1027 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:21,690]: Epoch: 102, Loss:0.1653 Train: 1.0000, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:21,697]: Epoch: 103, Loss:0.1119 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:21,704]: Epoch: 104, Loss:0.1182 Train: 0.9833, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:21,711]: Epoch: 105, Loss:0.1080 Train: 0.9833, Val:0.5500, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:21,718]: Epoch: 106, Loss:0.1192 Train: 0.9750, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:21,725]: Epoch: 107, Loss:0.0994 Train: 0.9583, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:21,733]: Epoch: 108, Loss:0.1093 Train: 0.9500, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:21,739]: Epoch: 109, Loss:0.1510 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:21,746]: Epoch: 110, Loss:0.0934 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:21,753]: Epoch: 111, Loss:0.1108 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:21,759]: Epoch: 112, Loss:0.1246 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:21,766]: Epoch: 113, Loss:0.0803 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:21,773]: Epoch: 114, Loss:0.1563 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:21,781]: Epoch: 115, Loss:0.1065 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:21,788]: Epoch: 116, Loss:0.1113 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:21,796]: Epoch: 117, Loss:0.1133 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:21,802]: Epoch: 118, Loss:0.1374 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:21,810]: Epoch: 119, Loss:0.1423 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:21,817]: Epoch: 120, Loss:0.1275 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:21,824]: Epoch: 121, Loss:0.1016 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:21,831]: Epoch: 122, Loss:0.1048 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:21,838]: Epoch: 123, Loss:0.1194 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:21,845]: Epoch: 124, Loss:0.1138 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:21,854]: Epoch: 125, Loss:0.1144 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:21,862]: Epoch: 126, Loss:0.1110 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:21,869]: Epoch: 127, Loss:0.1131 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:21,877]: Epoch: 128, Loss:0.1110 Train: 1.0000, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:21,885]: Epoch: 129, Loss:0.0996 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,893]: Epoch: 130, Loss:0.1614 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:21,903]: Epoch: 131, Loss:0.0986 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0090
[2025-04-01 02:43:21,911]: Epoch: 132, Loss:0.1368 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:21,919]: Epoch: 133, Loss:0.1081 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:21,927]: Epoch: 134, Loss:0.1318 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,935]: Epoch: 135, Loss:0.1046 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,942]: Epoch: 136, Loss:0.1107 Train: 0.9667, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:21,949]: Epoch: 137, Loss:0.1014 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:21,957]: Epoch: 138, Loss:0.0883 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:21,966]: Epoch: 139, Loss:0.0933 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:21,974]: Epoch: 140, Loss:0.0983 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:21,982]: Epoch: 141, Loss:0.1164 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:21,990]: Epoch: 142, Loss:0.1052 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:21,998]: Epoch: 143, Loss:0.0785 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:22,006]: Epoch: 144, Loss:0.0768 Train: 0.9917, Val:0.5500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:22,013]: Epoch: 145, Loss:0.1076 Train: 0.9917, Val:0.5375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:22,021]: Epoch: 146, Loss:0.0659 Train: 1.0000, Val:0.5375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:22,027]: Epoch: 147, Loss:0.0841 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:22,036]: Epoch: 148, Loss:0.0762 Train: 0.9917, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:22,044]: Epoch: 149, Loss:0.0887 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:22,052]: Epoch: 150, Loss:0.0905 Train: 0.9917, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:22,060]: Epoch: 151, Loss:0.1187 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:22,067]: Epoch: 152, Loss:0.0807 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:22,074]: Epoch: 153, Loss:0.1124 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:22,080]: Epoch: 154, Loss:0.0717 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:22,088]: Epoch: 155, Loss:0.1050 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:22,094]: Epoch: 156, Loss:0.0762 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:22,102]: Epoch: 157, Loss:0.0846 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:22,109]: Epoch: 158, Loss:0.0948 Train: 1.0000, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:22,118]: Epoch: 159, Loss:0.0895 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:22,125]: Epoch: 160, Loss:0.0858 Train: 0.9917, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:22,134]: Epoch: 161, Loss:0.0952 Train: 0.9917, Val:0.5500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:22,141]: Epoch: 162, Loss:0.0848 Train: 0.9917, Val:0.5500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:22,149]: Epoch: 163, Loss:0.0982 Train: 0.9667, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:22,157]: Epoch: 164, Loss:0.0992 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:22,164]: Epoch: 165, Loss:0.1282 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:22,172]: Epoch: 166, Loss:0.0689 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:22,180]: Epoch: 167, Loss:0.0721 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:22,190]: Epoch: 168, Loss:0.1162 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0099
[2025-04-01 02:43:22,199]: Epoch: 169, Loss:0.1526 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:22,208]: Epoch: 170, Loss:0.1041 Train: 1.0000, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0090
[2025-04-01 02:43:22,216]: Epoch: 171, Loss:0.1113 Train: 0.9833, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:22,225]: Epoch: 172, Loss:0.1156 Train: 0.9917, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:22,231]: Epoch: 173, Loss:0.1154 Train: 0.9917, Val:0.5500, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:22,238]: Epoch: 174, Loss:0.0857 Train: 0.9917, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:22,247]: Epoch: 175, Loss:0.1078 Train: 0.9917, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:22,256]: Epoch: 176, Loss:0.1200 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:22,266]: Epoch: 177, Loss:0.1001 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0096
[2025-04-01 02:43:22,275]: Epoch: 178, Loss:0.0912 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0093
[2025-04-01 02:43:22,282]: Epoch: 179, Loss:0.0857 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:22,291]: Epoch: 180, Loss:0.1113 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:22,299]: Epoch: 181, Loss:0.1042 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:22,308]: Epoch: 182, Loss:0.0798 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:43:22,316]: Epoch: 183, Loss:0.1048 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:22,325]: Epoch: 184, Loss:0.0884 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:22,334]: Epoch: 185, Loss:0.1089 Train: 0.9917, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:22,342]: Epoch: 186, Loss:0.0815 Train: 0.9917, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:22,350]: Epoch: 187, Loss:0.0734 Train: 0.9917, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:22,358]: Epoch: 188, Loss:0.0785 Train: 0.9917, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:22,366]: Epoch: 189, Loss:0.0906 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:22,372]: Epoch: 190, Loss:0.1344 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:22,379]: Epoch: 191, Loss:0.0896 Train: 0.9833, Val:0.5375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:22,387]: Epoch: 192, Loss:0.0804 Train: 0.9833, Val:0.5375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:22,395]: Epoch: 193, Loss:0.1370 Train: 0.9917, Val:0.5375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:22,403]: Epoch: 194, Loss:0.1232 Train: 0.9917, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:22,409]: Epoch: 195, Loss:0.0940 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:22,417]: Epoch: 196, Loss:0.0811 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:22,424]: Epoch: 197, Loss:0.0856 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:22,431]: Epoch: 198, Loss:0.0755 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:22,438]: Epoch: 199, Loss:0.1127 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:22,445]: Epoch: 200, Loss:0.0852 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:22,445]: [Run-1 score] {'train': 0.9416666666666667, 'val': 0.6875, 'test': 0.6862745098039216}
[2025-04-01 02:43:22,445]: repeat 2/3
[2025-04-01 02:43:22,445]: Manual random seed:0
[2025-04-01 02:43:22,446]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:22,450]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:22,459]: Epoch: 001, Loss:1.6053 Train: 0.6333, Val:0.5875, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:22,466]: Epoch: 002, Loss:1.1257 Train: 0.7833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:22,474]: Epoch: 003, Loss:0.7300 Train: 0.8250, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:22,482]: Epoch: 004, Loss:0.5915 Train: 0.8667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:22,489]: Epoch: 005, Loss:0.4931 Train: 0.8750, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:22,497]: Epoch: 006, Loss:0.5234 Train: 0.8667, Val:0.5625, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:22,505]: Epoch: 007, Loss:0.4910 Train: 0.8833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:22,513]: Epoch: 008, Loss:0.3657 Train: 0.9167, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:22,519]: Epoch: 009, Loss:0.3512 Train: 0.9167, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0055
[2025-04-01 02:43:22,526]: Epoch: 010, Loss:0.3489 Train: 0.9250, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:22,533]: Epoch: 011, Loss:0.3167 Train: 0.9333, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:22,539]: Epoch: 012, Loss:0.3032 Train: 0.9333, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:22,546]: Epoch: 013, Loss:0.3305 Train: 0.9500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:22,553]: Epoch: 014, Loss:0.2652 Train: 0.9500, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:22,561]: Epoch: 015, Loss:0.2560 Train: 0.9500, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:22,570]: Epoch: 016, Loss:0.2440 Train: 0.9500, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:22,577]: Epoch: 017, Loss:0.2316 Train: 0.9583, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:22,584]: Epoch: 018, Loss:0.1953 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:22,593]: Epoch: 019, Loss:0.2205 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:22,600]: Epoch: 020, Loss:0.1866 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:22,608]: Epoch: 021, Loss:0.2241 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:22,616]: Epoch: 022, Loss:0.1868 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:22,624]: Epoch: 023, Loss:0.1582 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:22,630]: Epoch: 024, Loss:0.1409 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:43:22,638]: Epoch: 025, Loss:0.1618 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:22,646]: Epoch: 026, Loss:0.1518 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:22,652]: Epoch: 027, Loss:0.1360 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:22,659]: Epoch: 028, Loss:0.1669 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:22,668]: Epoch: 029, Loss:0.1383 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:22,674]: Epoch: 030, Loss:0.1489 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:22,681]: Epoch: 031, Loss:0.1836 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:22,688]: Epoch: 032, Loss:0.1309 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:22,695]: Epoch: 033, Loss:0.1711 Train: 0.9833, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:22,704]: Epoch: 034, Loss:0.1758 Train: 0.9667, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:22,710]: Epoch: 035, Loss:0.2195 Train: 0.9667, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:22,719]: Epoch: 036, Loss:0.1406 Train: 0.9750, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0087
[2025-04-01 02:43:22,725]: Epoch: 037, Loss:0.1113 Train: 0.9583, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:22,734]: Epoch: 038, Loss:0.1132 Train: 0.9583, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:22,740]: Epoch: 039, Loss:0.1744 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:22,748]: Epoch: 040, Loss:0.1298 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:22,754]: Epoch: 041, Loss:0.1313 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:22,763]: Epoch: 042, Loss:0.1419 Train: 0.9667, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:22,769]: Epoch: 043, Loss:0.1498 Train: 0.9750, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0057
[2025-04-01 02:43:22,776]: Epoch: 044, Loss:0.1552 Train: 0.9750, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:22,783]: Epoch: 045, Loss:0.1391 Train: 0.9583, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:22,791]: Epoch: 046, Loss:0.1571 Train: 0.9583, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:22,797]: Epoch: 047, Loss:0.1491 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:22,805]: Epoch: 048, Loss:0.1493 Train: 0.9750, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:22,812]: Epoch: 049, Loss:0.1427 Train: 0.9583, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:22,819]: Epoch: 050, Loss:0.1574 Train: 0.9667, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:22,827]: Epoch: 051, Loss:0.1466 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:22,835]: Epoch: 052, Loss:0.0861 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:22,841]: Epoch: 053, Loss:0.1118 Train: 0.9750, Val:0.5250, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:22,848]: Epoch: 054, Loss:0.1753 Train: 0.9750, Val:0.5500, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:22,856]: Epoch: 055, Loss:0.1330 Train: 0.9750, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:22,863]: Epoch: 056, Loss:0.1415 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:22,870]: Epoch: 057, Loss:0.1267 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:22,880]: Epoch: 058, Loss:0.1821 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0092
[2025-04-01 02:43:22,888]: Epoch: 059, Loss:0.1543 Train: 0.9667, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:22,896]: Epoch: 060, Loss:0.0981 Train: 0.9583, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:22,905]: Epoch: 061, Loss:0.1191 Train: 0.9583, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:22,914]: Epoch: 062, Loss:0.1482 Train: 0.9667, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:22,920]: Epoch: 063, Loss:0.1019 Train: 0.9667, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:22,929]: Epoch: 064, Loss:0.1324 Train: 0.9667, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:22,937]: Epoch: 065, Loss:0.1184 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:22,945]: Epoch: 066, Loss:0.1719 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:22,953]: Epoch: 067, Loss:0.1067 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:22,961]: Epoch: 068, Loss:0.1346 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:22,967]: Epoch: 069, Loss:0.1290 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:22,974]: Epoch: 070, Loss:0.1301 Train: 1.0000, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:22,981]: Epoch: 071, Loss:0.1278 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:22,989]: Epoch: 072, Loss:0.1170 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:22,997]: Epoch: 073, Loss:0.1250 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:23,004]: Epoch: 074, Loss:0.0867 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:23,013]: Epoch: 075, Loss:0.1261 Train: 0.9750, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:23,019]: Epoch: 076, Loss:0.1002 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:23,028]: Epoch: 077, Loss:0.1239 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:23,035]: Epoch: 078, Loss:0.1520 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:23,041]: Epoch: 079, Loss:0.1185 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:23,049]: Epoch: 080, Loss:0.1255 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:23,057]: Epoch: 081, Loss:0.0985 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:23,064]: Epoch: 082, Loss:0.1686 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:23,073]: Epoch: 083, Loss:0.1251 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:23,081]: Epoch: 084, Loss:0.0987 Train: 1.0000, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:23,089]: Epoch: 085, Loss:0.1100 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:23,096]: Epoch: 086, Loss:0.1034 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:23,103]: Epoch: 087, Loss:0.1049 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:23,109]: Epoch: 088, Loss:0.0946 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:23,116]: Epoch: 089, Loss:0.1269 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:23,124]: Epoch: 090, Loss:0.1031 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:23,132]: Epoch: 091, Loss:0.1180 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:23,139]: Epoch: 092, Loss:0.1165 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:23,146]: Epoch: 093, Loss:0.1055 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:23,153]: Epoch: 094, Loss:0.1217 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:23,160]: Epoch: 095, Loss:0.1063 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:23,167]: Epoch: 096, Loss:0.0897 Train: 0.9750, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:23,175]: Epoch: 097, Loss:0.0980 Train: 0.9583, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:23,185]: Epoch: 098, Loss:0.0957 Train: 0.9583, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0102
[2025-04-01 02:43:23,195]: Epoch: 099, Loss:0.1323 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0093
[2025-04-01 02:43:23,203]: Epoch: 100, Loss:0.0869 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:23,209]: Epoch: 101, Loss:0.0946 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:23,217]: Epoch: 102, Loss:0.0844 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:23,225]: Epoch: 103, Loss:0.0950 Train: 0.9833, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:23,233]: Epoch: 104, Loss:0.1004 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,241]: Epoch: 105, Loss:0.0821 Train: 0.9833, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:23,250]: Epoch: 106, Loss:0.0919 Train: 0.9917, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:23,258]: Epoch: 107, Loss:0.1352 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:23,266]: Epoch: 108, Loss:0.0802 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:23,275]: Epoch: 109, Loss:0.0635 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:23,283]: Epoch: 110, Loss:0.1001 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:23,290]: Epoch: 111, Loss:0.1300 Train: 1.0000, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:23,297]: Epoch: 112, Loss:0.1087 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:23,306]: Epoch: 113, Loss:0.1059 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:23,312]: Epoch: 114, Loss:0.0929 Train: 0.9750, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:23,319]: Epoch: 115, Loss:0.0946 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:23,327]: Epoch: 116, Loss:0.0886 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:23,335]: Epoch: 117, Loss:0.1182 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,341]: Epoch: 118, Loss:0.1058 Train: 0.9750, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:23,349]: Epoch: 119, Loss:0.1250 Train: 0.9833, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:23,356]: Epoch: 120, Loss:0.0911 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:23,363]: Epoch: 121, Loss:0.0906 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:23,369]: Epoch: 122, Loss:0.0844 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:23,376]: Epoch: 123, Loss:0.0874 Train: 0.9833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:23,384]: Epoch: 124, Loss:0.1072 Train: 0.9917, Val:0.5750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:23,392]: Epoch: 125, Loss:0.1019 Train: 0.9917, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:23,400]: Epoch: 126, Loss:0.1182 Train: 1.0000, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0088
[2025-04-01 02:43:23,408]: Epoch: 127, Loss:0.0849 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:23,415]: Epoch: 128, Loss:0.0715 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:23,422]: Epoch: 129, Loss:0.1114 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:23,430]: Epoch: 130, Loss:0.0936 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:23,437]: Epoch: 131, Loss:0.1443 Train: 0.9833, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:23,444]: Epoch: 132, Loss:0.1717 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:23,451]: Epoch: 133, Loss:0.1142 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:23,458]: Epoch: 134, Loss:0.0928 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:23,467]: Epoch: 135, Loss:0.1130 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:23,474]: Epoch: 136, Loss:0.1007 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:23,480]: Epoch: 137, Loss:0.0984 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:23,488]: Epoch: 138, Loss:0.0827 Train: 0.9583, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:23,496]: Epoch: 139, Loss:0.1206 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:23,503]: Epoch: 140, Loss:0.1132 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:23,511]: Epoch: 141, Loss:0.0913 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:23,517]: Epoch: 142, Loss:0.1124 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:23,526]: Epoch: 143, Loss:0.1225 Train: 1.0000, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:23,534]: Epoch: 144, Loss:0.0817 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:23,542]: Epoch: 145, Loss:0.0796 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:23,548]: Epoch: 146, Loss:0.1200 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:23,555]: Epoch: 147, Loss:0.0869 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:23,562]: Epoch: 148, Loss:0.0991 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:23,569]: Epoch: 149, Loss:0.1112 Train: 0.9917, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:23,576]: Epoch: 150, Loss:0.1101 Train: 0.9917, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:23,584]: Epoch: 151, Loss:0.0918 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:23,590]: Epoch: 152, Loss:0.0883 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:23,599]: Epoch: 153, Loss:0.1162 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:23,607]: Epoch: 154, Loss:0.0853 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,615]: Epoch: 155, Loss:0.1193 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,623]: Epoch: 156, Loss:0.1127 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:23,631]: Epoch: 157, Loss:0.0786 Train: 1.0000, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:23,639]: Epoch: 158, Loss:0.0768 Train: 1.0000, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:23,648]: Epoch: 159, Loss:0.1017 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:23,656]: Epoch: 160, Loss:0.0745 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:23,664]: Epoch: 161, Loss:0.0873 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:23,672]: Epoch: 162, Loss:0.0949 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:23,678]: Epoch: 163, Loss:0.0645 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:23,686]: Epoch: 164, Loss:0.0715 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:23,694]: Epoch: 165, Loss:0.0672 Train: 0.9917, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:23,702]: Epoch: 166, Loss:0.0892 Train: 0.9917, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:23,708]: Epoch: 167, Loss:0.0887 Train: 1.0000, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:23,715]: Epoch: 168, Loss:0.0730 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:23,723]: Epoch: 169, Loss:0.1016 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:23,729]: Epoch: 170, Loss:0.0699 Train: 0.9833, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:23,736]: Epoch: 171, Loss:0.0756 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:23,744]: Epoch: 172, Loss:0.0926 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:23,750]: Epoch: 173, Loss:0.0850 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:23,758]: Epoch: 174, Loss:0.0759 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:23,766]: Epoch: 175, Loss:0.1262 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:23,773]: Epoch: 176, Loss:0.0906 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:23,780]: Epoch: 177, Loss:0.1101 Train: 0.9750, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:23,788]: Epoch: 178, Loss:0.0901 Train: 1.0000, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:23,795]: Epoch: 179, Loss:0.1636 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:23,803]: Epoch: 180, Loss:0.0796 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,811]: Epoch: 181, Loss:0.1021 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:23,818]: Epoch: 182, Loss:0.0822 Train: 1.0000, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:23,827]: Epoch: 183, Loss:0.0891 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:23,833]: Epoch: 184, Loss:0.1130 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:23,841]: Epoch: 185, Loss:0.1131 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:23,850]: Epoch: 186, Loss:0.1352 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:23,858]: Epoch: 187, Loss:0.0943 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:23,865]: Epoch: 188, Loss:0.1429 Train: 1.0000, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:23,874]: Epoch: 189, Loss:0.0895 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:23,880]: Epoch: 190, Loss:0.1242 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:23,888]: Epoch: 191, Loss:0.1185 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:23,896]: Epoch: 192, Loss:0.1825 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:23,903]: Epoch: 193, Loss:0.1215 Train: 0.9833, Val:0.6125, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:23,911]: Epoch: 194, Loss:0.1828 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:23,919]: Epoch: 195, Loss:0.0985 Train: 0.9917, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:23,927]: Epoch: 196, Loss:0.0776 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:23,933]: Epoch: 197, Loss:0.1237 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:23,941]: Epoch: 198, Loss:0.0848 Train: 0.9750, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:23,949]: Epoch: 199, Loss:0.0877 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:23,955]: Epoch: 200, Loss:0.1020 Train: 0.9833, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:23,955]: [Run-2 score] {'train': 0.7833333333333333, 'val': 0.6875, 'test': 0.6862745098039216}
[2025-04-01 02:43:23,955]: repeat 3/3
[2025-04-01 02:43:23,955]: Manual random seed:0
[2025-04-01 02:43:23,956]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:23,960]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:23,969]: Epoch: 001, Loss:1.7193 Train: 0.7583, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:43:23,976]: Epoch: 002, Loss:1.1067 Train: 0.7750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:23,984]: Epoch: 003, Loss:0.8815 Train: 0.8500, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:23,992]: Epoch: 004, Loss:0.5914 Train: 0.8750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:23,998]: Epoch: 005, Loss:0.4617 Train: 0.8583, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:24,005]: Epoch: 006, Loss:0.3989 Train: 0.8750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:24,012]: Epoch: 007, Loss:0.3525 Train: 0.9000, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:24,021]: Epoch: 008, Loss:0.3315 Train: 0.8917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:24,029]: Epoch: 009, Loss:0.3402 Train: 0.9333, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:24,037]: Epoch: 010, Loss:0.2766 Train: 0.9583, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:24,046]: Epoch: 011, Loss:0.2721 Train: 0.9500, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0088
[2025-04-01 02:43:24,052]: Epoch: 012, Loss:0.2901 Train: 0.9583, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:24,059]: Epoch: 013, Loss:0.2378 Train: 0.9500, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:24,066]: Epoch: 014, Loss:0.3048 Train: 0.9500, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:24,074]: Epoch: 015, Loss:0.2440 Train: 0.9417, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:24,082]: Epoch: 016, Loss:0.2803 Train: 0.9417, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:24,090]: Epoch: 017, Loss:0.2558 Train: 0.9417, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:24,096]: Epoch: 018, Loss:0.2652 Train: 0.9500, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:24,104]: Epoch: 019, Loss:0.2427 Train: 0.9583, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:24,110]: Epoch: 020, Loss:0.2205 Train: 0.9667, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0061
[2025-04-01 02:43:24,117]: Epoch: 021, Loss:0.2158 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:24,124]: Epoch: 022, Loss:0.2021 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:24,131]: Epoch: 023, Loss:0.1809 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:43:24,139]: Epoch: 024, Loss:0.2345 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:24,148]: Epoch: 025, Loss:0.1678 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:24,156]: Epoch: 026, Loss:0.1721 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:24,165]: Epoch: 027, Loss:0.1486 Train: 0.9583, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0090
[2025-04-01 02:43:24,171]: Epoch: 028, Loss:0.1857 Train: 0.9583, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:24,181]: Epoch: 029, Loss:0.1235 Train: 0.9583, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0096
[2025-04-01 02:43:24,191]: Epoch: 030, Loss:0.1419 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0100
[2025-04-01 02:43:24,199]: Epoch: 031, Loss:0.1329 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:24,208]: Epoch: 032, Loss:0.1308 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:24,215]: Epoch: 033, Loss:0.1397 Train: 0.9667, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:24,224]: Epoch: 034, Loss:0.1730 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:24,230]: Epoch: 035, Loss:0.1846 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:24,237]: Epoch: 036, Loss:0.1366 Train: 0.9833, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:24,247]: Epoch: 037, Loss:0.1526 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0093
[2025-04-01 02:43:24,255]: Epoch: 038, Loss:0.1525 Train: 0.9750, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:24,264]: Epoch: 039, Loss:0.1737 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:24,272]: Epoch: 040, Loss:0.1398 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:24,279]: Epoch: 041, Loss:0.1079 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:24,288]: Epoch: 042, Loss:0.1357 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:24,297]: Epoch: 043, Loss:0.1190 Train: 0.9667, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0093
[2025-04-01 02:43:24,305]: Epoch: 044, Loss:0.1317 Train: 0.9750, Val:0.5500, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:24,314]: Epoch: 045, Loss:0.1504 Train: 0.9667, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0089
[2025-04-01 02:43:24,323]: Epoch: 046, Loss:0.1548 Train: 0.9667, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0093
[2025-04-01 02:43:24,336]: Epoch: 047, Loss:0.1741 Train: 0.9667, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0124
[2025-04-01 02:43:24,344]: Epoch: 048, Loss:0.1445 Train: 0.9833, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:24,354]: Epoch: 049, Loss:0.1250 Train: 0.9917, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0096
[2025-04-01 02:43:24,363]: Epoch: 050, Loss:0.0963 Train: 1.0000, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0090
[2025-04-01 02:43:24,370]: Epoch: 051, Loss:0.1343 Train: 0.9917, Val:0.5625, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:24,379]: Epoch: 052, Loss:0.1276 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0087
[2025-04-01 02:43:24,387]: Epoch: 053, Loss:0.1322 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:24,396]: Epoch: 054, Loss:0.1254 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:24,404]: Epoch: 055, Loss:0.1394 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:24,411]: Epoch: 056, Loss:0.1043 Train: 0.9833, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:24,418]: Epoch: 057, Loss:0.1336 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:24,425]: Epoch: 058, Loss:0.1037 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:24,433]: Epoch: 059, Loss:0.1292 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:24,439]: Epoch: 060, Loss:0.1120 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:24,447]: Epoch: 061, Loss:0.0980 Train: 0.9667, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:24,454]: Epoch: 062, Loss:0.1567 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:24,462]: Epoch: 063, Loss:0.1087 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:24,468]: Epoch: 064, Loss:0.1475 Train: 0.9833, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:24,476]: Epoch: 065, Loss:0.1237 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:24,483]: Epoch: 066, Loss:0.1326 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:24,490]: Epoch: 067, Loss:0.0946 Train: 0.9750, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:24,498]: Epoch: 068, Loss:0.1357 Train: 0.9667, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:24,506]: Epoch: 069, Loss:0.1328 Train: 0.9667, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:24,513]: Epoch: 070, Loss:0.1310 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:24,520]: Epoch: 071, Loss:0.1220 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:24,528]: Epoch: 072, Loss:0.1424 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:24,534]: Epoch: 073, Loss:0.1118 Train: 0.9750, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:24,541]: Epoch: 074, Loss:0.1013 Train: 0.9667, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:24,549]: Epoch: 075, Loss:0.1642 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:24,556]: Epoch: 076, Loss:0.1386 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:24,564]: Epoch: 077, Loss:0.1416 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:24,571]: Epoch: 078, Loss:0.1297 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:24,578]: Epoch: 079, Loss:0.1357 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:24,585]: Epoch: 080, Loss:0.0920 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:24,594]: Epoch: 081, Loss:0.1206 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:24,601]: Epoch: 082, Loss:0.1087 Train: 0.9750, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:24,608]: Epoch: 083, Loss:0.1297 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:24,615]: Epoch: 084, Loss:0.1377 Train: 0.9750, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:24,622]: Epoch: 085, Loss:0.1524 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:24,630]: Epoch: 086, Loss:0.1473 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:24,638]: Epoch: 087, Loss:0.1080 Train: 0.9750, Val:0.5250, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:24,645]: Epoch: 088, Loss:0.1172 Train: 0.9750, Val:0.5375, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:24,653]: Epoch: 089, Loss:0.1672 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:24,661]: Epoch: 090, Loss:0.1239 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:24,669]: Epoch: 091, Loss:0.0992 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:24,676]: Epoch: 092, Loss:0.1415 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:24,683]: Epoch: 093, Loss:0.1148 Train: 0.9750, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:24,690]: Epoch: 094, Loss:0.0864 Train: 0.9667, Val:0.5375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:24,699]: Epoch: 095, Loss:0.1213 Train: 0.9667, Val:0.5375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:24,707]: Epoch: 096, Loss:0.1378 Train: 0.9750, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:24,714]: Epoch: 097, Loss:0.0810 Train: 0.9917, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:24,721]: Epoch: 098, Loss:0.0884 Train: 0.9917, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:24,729]: Epoch: 099, Loss:0.0959 Train: 1.0000, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:24,737]: Epoch: 100, Loss:0.1471 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:24,745]: Epoch: 101, Loss:0.0922 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:24,753]: Epoch: 102, Loss:0.0931 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:24,759]: Epoch: 103, Loss:0.1279 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:43:24,767]: Epoch: 104, Loss:0.0688 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:24,775]: Epoch: 105, Loss:0.1575 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:24,781]: Epoch: 106, Loss:0.1057 Train: 0.9917, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:24,789]: Epoch: 107, Loss:0.0770 Train: 0.9833, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:24,796]: Epoch: 108, Loss:0.0954 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:24,803]: Epoch: 109, Loss:0.1136 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:24,810]: Epoch: 110, Loss:0.1059 Train: 0.9917, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:24,816]: Epoch: 111, Loss:0.1801 Train: 0.9667, Val:0.5375, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:24,825]: Epoch: 112, Loss:0.1210 Train: 0.9833, Val:0.5500, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:24,833]: Epoch: 113, Loss:0.1611 Train: 0.9833, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:24,839]: Epoch: 114, Loss:0.1387 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:24,846]: Epoch: 115, Loss:0.1417 Train: 0.9750, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:24,852]: Epoch: 116, Loss:0.1326 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:24,860]: Epoch: 117, Loss:0.0980 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:24,868]: Epoch: 118, Loss:0.1424 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:24,874]: Epoch: 119, Loss:0.1062 Train: 0.9667, Val:0.5500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:24,882]: Epoch: 120, Loss:0.1970 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:24,889]: Epoch: 121, Loss:0.1304 Train: 0.9917, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:24,896]: Epoch: 122, Loss:0.1887 Train: 0.9917, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:24,904]: Epoch: 123, Loss:0.1407 Train: 0.9917, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:24,910]: Epoch: 124, Loss:0.1003 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:24,919]: Epoch: 125, Loss:0.0794 Train: 0.9917, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:24,928]: Epoch: 126, Loss:0.1053 Train: 1.0000, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:24,936]: Epoch: 127, Loss:0.1250 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:24,942]: Epoch: 128, Loss:0.1507 Train: 0.9917, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:24,951]: Epoch: 129, Loss:0.0893 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:24,959]: Epoch: 130, Loss:0.1700 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0087
[2025-04-01 02:43:24,967]: Epoch: 131, Loss:0.1517 Train: 1.0000, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:24,976]: Epoch: 132, Loss:0.0843 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:24,984]: Epoch: 133, Loss:0.1087 Train: 0.9583, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:24,992]: Epoch: 134, Loss:0.1751 Train: 0.9667, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:25,000]: Epoch: 135, Loss:0.1712 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:25,008]: Epoch: 136, Loss:0.1741 Train: 0.9833, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:25,016]: Epoch: 137, Loss:0.1188 Train: 0.9833, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:25,024]: Epoch: 138, Loss:0.1025 Train: 0.9917, Val:0.5500, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:25,032]: Epoch: 139, Loss:0.1083 Train: 0.9833, Val:0.5500, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:25,038]: Epoch: 140, Loss:0.1668 Train: 0.9833, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:25,046]: Epoch: 141, Loss:0.0872 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:25,053]: Epoch: 142, Loss:0.1098 Train: 0.9750, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:25,061]: Epoch: 143, Loss:0.1059 Train: 0.9667, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:25,069]: Epoch: 144, Loss:0.1142 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:25,075]: Epoch: 145, Loss:0.1580 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:25,082]: Epoch: 146, Loss:0.1797 Train: 1.0000, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:25,090]: Epoch: 147, Loss:0.1151 Train: 0.9917, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:25,098]: Epoch: 148, Loss:0.1346 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:25,107]: Epoch: 149, Loss:0.0910 Train: 0.9917, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:25,115]: Epoch: 150, Loss:0.0936 Train: 1.0000, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:25,123]: Epoch: 151, Loss:0.1014 Train: 0.9917, Val:0.5750, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:25,131]: Epoch: 152, Loss:0.1156 Train: 0.9917, Val:0.5750, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:25,138]: Epoch: 153, Loss:0.0947 Train: 1.0000, Val:0.6000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:25,147]: Epoch: 154, Loss:0.0810 Train: 1.0000, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:25,155]: Epoch: 155, Loss:0.1089 Train: 0.9833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:25,164]: Epoch: 156, Loss:0.1172 Train: 0.9750, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:25,171]: Epoch: 157, Loss:0.1190 Train: 0.9833, Val:0.5625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:25,178]: Epoch: 158, Loss:0.1279 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:25,189]: Epoch: 159, Loss:0.0983 Train: 0.9833, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0108
[2025-04-01 02:43:25,198]: Epoch: 160, Loss:0.0930 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:25,207]: Epoch: 161, Loss:0.0839 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:25,214]: Epoch: 162, Loss:0.1305 Train: 1.0000, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:25,222]: Epoch: 163, Loss:0.0890 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:25,230]: Epoch: 164, Loss:0.1532 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:25,236]: Epoch: 165, Loss:0.1183 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:25,243]: Epoch: 166, Loss:0.1286 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:25,250]: Epoch: 167, Loss:0.1314 Train: 0.9667, Val:0.5875, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:25,258]: Epoch: 168, Loss:0.1163 Train: 0.9667, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:25,266]: Epoch: 169, Loss:0.1324 Train: 0.9667, Val:0.5750, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:25,275]: Epoch: 170, Loss:0.1492 Train: 0.9833, Val:0.5750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:25,282]: Epoch: 171, Loss:0.1256 Train: 0.9833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:25,289]: Epoch: 172, Loss:0.1414 Train: 0.9833, Val:0.5875, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:25,297]: Epoch: 173, Loss:0.1338 Train: 0.9833, Val:0.5875, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:25,306]: Epoch: 174, Loss:0.1003 Train: 0.9750, Val:0.5625, Test: 0.6078, Time(s/epoch):0.0088
[2025-04-01 02:43:25,314]: Epoch: 175, Loss:0.0890 Train: 0.9750, Val:0.5625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:25,322]: Epoch: 176, Loss:0.1630 Train: 0.9750, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:25,331]: Epoch: 177, Loss:0.1082 Train: 0.9833, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:25,339]: Epoch: 178, Loss:0.1065 Train: 0.9833, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:25,346]: Epoch: 179, Loss:0.0861 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:25,353]: Epoch: 180, Loss:0.1368 Train: 0.9750, Val:0.6125, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:25,360]: Epoch: 181, Loss:0.1275 Train: 0.9667, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:25,368]: Epoch: 182, Loss:0.0932 Train: 1.0000, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:25,376]: Epoch: 183, Loss:0.0901 Train: 0.9833, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:25,383]: Epoch: 184, Loss:0.1138 Train: 0.9750, Val:0.5625, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:25,391]: Epoch: 185, Loss:0.1269 Train: 0.9750, Val:0.5500, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:25,397]: Epoch: 186, Loss:0.1640 Train: 0.9833, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:25,404]: Epoch: 187, Loss:0.1039 Train: 0.9833, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:25,410]: Epoch: 188, Loss:0.1631 Train: 0.9750, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:25,419]: Epoch: 189, Loss:0.1100 Train: 0.9667, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:25,427]: Epoch: 190, Loss:0.1002 Train: 0.9750, Val:0.5750, Test: 0.7255, Time(s/epoch):0.0082
[2025-04-01 02:43:25,433]: Epoch: 191, Loss:0.0790 Train: 0.9833, Val:0.5875, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:25,440]: Epoch: 192, Loss:0.1328 Train: 0.9917, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:25,447]: Epoch: 193, Loss:0.0660 Train: 0.9833, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:25,455]: Epoch: 194, Loss:0.0926 Train: 0.9750, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:25,463]: Epoch: 195, Loss:0.1297 Train: 0.9917, Val:0.5875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:25,470]: Epoch: 196, Loss:0.1133 Train: 0.9917, Val:0.5750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:25,477]: Epoch: 197, Loss:0.0963 Train: 1.0000, Val:0.5625, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:25,484]: Epoch: 198, Loss:0.0834 Train: 0.9917, Val:0.5750, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:25,492]: Epoch: 199, Loss:0.0932 Train: 0.9833, Val:0.5625, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:25,499]: Epoch: 200, Loss:0.1051 Train: 0.9833, Val:0.5625, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:25,499]: [Run-3 score] {'train': 0.7583333333333333, 'val': 0.65, 'test': 0.5294117647058824}
[2025-04-01 02:43:25,499]: repeat 1/3
[2025-04-01 02:43:25,499]: Manual random seed:0
[2025-04-01 02:43:25,499]: auto fixed data split seed to 0, model init seed to 0

Begin finetuning...
Epoch: 001, Supervised Loss:2.2578 Time(s/epoch):0.0076
Epoch: 002, Supervised Loss:2.1414 Time(s/epoch):0.0069
Epoch: 003, Supervised Loss:2.0235 Time(s/epoch):0.0072
Epoch: 004, Supervised Loss:1.9517 Time(s/epoch):0.0057
Epoch: 005, Supervised Loss:1.9335 Time(s/epoch):0.0053
Epoch: 006, Supervised Loss:1.9587 Time(s/epoch):0.0052
Epoch: 007, Supervised Loss:1.9823 Time(s/epoch):0.0049
Epoch: 008, Supervised Loss:1.9773 Time(s/epoch):0.0051
Epoch: 009, Supervised Loss:1.9546 Time(s/epoch):0.0050
Epoch: 010, Supervised Loss:1.9348 Time(s/epoch):0.0057
Epoch: 011, Supervised Loss:1.9248 Time(s/epoch):0.0050
Epoch: 012, Supervised Loss:1.9249 Time(s/epoch):0.0057
Epoch: 013, Supervised Loss:1.9348 Time(s/epoch):0.0078
Epoch: 014, Supervised Loss:1.9447 Time(s/epoch):0.0065
Epoch: 015, Supervised Loss:1.9456 Time(s/epoch):0.0075
Epoch: 016, Supervised Loss:1.9363 Time(s/epoch):0.0078
Epoch: 017, Supervised Loss:1.9209 Time(s/epoch):0.0076
Epoch: 018, Supervised Loss:1.9040 Time(s/epoch):0.0081
Epoch: 019, Supervised Loss:1.8884 Time(s/epoch):0.0073
Epoch: 020, Supervised Loss:1.8739 Time(s/epoch):0.0070
Epoch: 021, Supervised Loss:1.8598 Time(s/epoch):0.0071
Epoch: 022, Supervised Loss:1.8482 Time(s/epoch):0.0068
Epoch: 023, Supervised Loss:1.8445 Time(s/epoch):0.0071
Epoch: 024, Supervised Loss:1.8547 Time(s/epoch):0.0065
Epoch: 025, Supervised Loss:1.8763 Time(s/epoch):0.0068
Epoch: 026, Supervised Loss:1.8958 Time(s/epoch):0.0070
Epoch: 027, Supervised Loss:1.9015 Time(s/epoch):0.0066
Epoch: 028, Supervised Loss:1.8930 Time(s/epoch):0.0066
Epoch: 029, Supervised Loss:1.8766 Time(s/epoch):0.0064
Epoch: 030, Supervised Loss:1.8582 Time(s/epoch):0.0064
0.8299915 515
Add 417 edges.
Prune 355 edges from torch.Size([2, 515]) to torch.Size([2, 160])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.720
Data(x=[251, 1703], edge_index=[2, 571], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.3944 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.720
Data(x=[251, 1703], edge_index=[2, 571], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5070 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.720
Data(x=[251, 1703], edge_index=[2, 571], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5405 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(20)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0158
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0103
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0116
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0073
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0090
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0075
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0075
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0096
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0096
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0122
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0120
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0098
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0091
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0071
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0073
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0094
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0096
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0098
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0095
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0095
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0074
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0078
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0099
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0084
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0074
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0090
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0096
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0096
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0095
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0095
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0099
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0095
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0095
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0096
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0095
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0095
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0092
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0074
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0068
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0073
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0072
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0072
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0071
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0087
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0098
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0095
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0091
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0085
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0087
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0075
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0086
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0081
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0076
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0075
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0076
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0072
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0075
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0073
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0083
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0071
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0074
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0071
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0073
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0096
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0097
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0099
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0099
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0098
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0098
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0095
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0096
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0098
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0087
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0072
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0070
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0075
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0102
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0105
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0101
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0102
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0098
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0102
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0098
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0095
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0105
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0114
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0104
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0104
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0095
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0076
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0081
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0103
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0101
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0097
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0096
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0100
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0100
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0103
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0097
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0084
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0093
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0095
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0108
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0096
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0082
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0099
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0107
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0080
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0082
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0075
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0084
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0097
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0097
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0093
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0075
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0075
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0075
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0078
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0099
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0094
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0074
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0084
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0098
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0096
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0098
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0097
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0097
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0097
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0096
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0097
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0075
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0075
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0074
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0074
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0074
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0076
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0095
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0097
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0095
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0083
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0069
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0083
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0101
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0071
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0070
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0073
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0072
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0091
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0096
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0072
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0070
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0066
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0068
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0078
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0075
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0074
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0077
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0078
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0075
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0072
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0071
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0070
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0073
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0078
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0096
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0085
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0075
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0082
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0092
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0076
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0071
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0086
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0094
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0083
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0077
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0081
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0081
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0101
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0104
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0100
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0099
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0098
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0095
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0083
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0073
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0075
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0075
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0092
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0096
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0098
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0109
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0097
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0100
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0094
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0077
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0083
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0077
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0075
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0072
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0087
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.8514 Time(s/epoch):0.0064
Epoch: 002, Supervised Loss:1.7698 Time(s/epoch):0.0066
Epoch: 003, Supervised Loss:1.6982 Time(s/epoch):0.0059
Epoch: 004, Supervised Loss:1.6685 Time(s/epoch):0.0048
Epoch: 005, Supervised Loss:1.6721 Time(s/epoch):0.0059
Epoch: 006, Supervised Loss:1.6832 Time(s/epoch):0.0070
Epoch: 007, Supervised Loss:1.6779 Time(s/epoch):0.0065
Epoch: 008, Supervised Loss:1.6541 Time(s/epoch):0.0066
Epoch: 009, Supervised Loss:1.6279 Time(s/epoch):0.0064
Epoch: 010, Supervised Loss:1.6159 Time(s/epoch):0.0068
Epoch: 011, Supervised Loss:1.6240 Time(s/epoch):0.0065
Epoch: 012, Supervised Loss:1.6462 Time(s/epoch):0.0066
Epoch: 013, Supervised Loss:1.6717 Time(s/epoch):0.0072
Epoch: 014, Supervised Loss:1.6923 Time(s/epoch):0.0072
Epoch: 015, Supervised Loss:1.7046 Time(s/epoch):0.0072
Epoch: 016, Supervised Loss:1.7086 Time(s/epoch):0.0068
Epoch: 017, Supervised Loss:1.7061 Time(s/epoch):0.0066
Epoch: 018, Supervised Loss:1.6990 Time(s/epoch):0.0062
Epoch: 019, Supervised Loss:1.6890 Time(s/epoch):0.0064
Epoch: 020, Supervised Loss:1.6778 Time(s/epoch):0.0065
Epoch: 021, Supervised Loss:1.6672 Time(s/epoch):0.0064[2025-04-01 02:43:27,473]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:27,486]: Epoch: 001, Loss:1.7275 Train: 0.5500, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0099
[2025-04-01 02:43:27,494]: Epoch: 002, Loss:1.1849 Train: 0.8000, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:27,500]: Epoch: 003, Loss:0.7981 Train: 0.8333, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0055
[2025-04-01 02:43:27,507]: Epoch: 004, Loss:0.5834 Train: 0.8667, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:27,514]: Epoch: 005, Loss:0.5137 Train: 0.8750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:27,523]: Epoch: 006, Loss:0.4204 Train: 0.9000, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:27,531]: Epoch: 007, Loss:0.3539 Train: 0.8833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:27,538]: Epoch: 008, Loss:0.3639 Train: 0.8667, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:27,545]: Epoch: 009, Loss:0.3522 Train: 0.8917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:27,552]: Epoch: 010, Loss:0.3079 Train: 0.9083, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:27,559]: Epoch: 011, Loss:0.2493 Train: 0.9250, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:27,567]: Epoch: 012, Loss:0.2937 Train: 0.9333, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:27,575]: Epoch: 013, Loss:0.2471 Train: 0.9250, Val:0.7250, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:27,581]: Epoch: 014, Loss:0.2593 Train: 0.9417, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:27,589]: Epoch: 015, Loss:0.2167 Train: 0.9333, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:27,596]: Epoch: 016, Loss:0.2025 Train: 0.9417, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:27,602]: Epoch: 017, Loss:0.2052 Train: 0.9500, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:27,609]: Epoch: 018, Loss:0.1870 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:27,617]: Epoch: 019, Loss:0.1756 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:27,625]: Epoch: 020, Loss:0.1849 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:27,633]: Epoch: 021, Loss:0.1803 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:27,639]: Epoch: 022, Loss:0.1881 Train: 0.9667, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0057
[2025-04-01 02:43:27,647]: Epoch: 023, Loss:0.1591 Train: 0.9667, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:27,655]: Epoch: 024, Loss:0.1707 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:27,664]: Epoch: 025, Loss:0.1358 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:27,672]: Epoch: 026, Loss:0.1471 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:27,680]: Epoch: 027, Loss:0.1667 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:27,687]: Epoch: 028, Loss:0.1426 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:27,693]: Epoch: 029, Loss:0.1217 Train: 0.9917, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:27,699]: Epoch: 030, Loss:0.1280 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:27,708]: Epoch: 031, Loss:0.1513 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0088
[2025-04-01 02:43:27,717]: Epoch: 032, Loss:0.1341 Train: 0.9833, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:27,725]: Epoch: 033, Loss:0.1477 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:27,731]: Epoch: 034, Loss:0.1183 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:27,739]: Epoch: 035, Loss:0.1329 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:27,746]: Epoch: 036, Loss:0.1020 Train: 0.9750, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:27,754]: Epoch: 037, Loss:0.1386 Train: 0.9750, Val:0.7500, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:27,761]: Epoch: 038, Loss:0.1574 Train: 0.9917, Val:0.7500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:27,770]: Epoch: 039, Loss:0.1385 Train: 0.9750, Val:0.7375, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:27,777]: Epoch: 040, Loss:0.1530 Train: 0.9833, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:27,785]: Epoch: 041, Loss:0.1132 Train: 0.9750, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:27,792]: Epoch: 042, Loss:0.1282 Train: 0.9833, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:27,800]: Epoch: 043, Loss:0.1398 Train: 0.9667, Val:0.6375, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:27,808]: Epoch: 044, Loss:0.1230 Train: 0.9667, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:27,816]: Epoch: 045, Loss:0.1414 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:27,821]: Epoch: 046, Loss:0.1079 Train: 0.9917, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0058
[2025-04-01 02:43:27,830]: Epoch: 047, Loss:0.1139 Train: 0.9667, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:27,838]: Epoch: 048, Loss:0.1430 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:27,846]: Epoch: 049, Loss:0.1255 Train: 0.9833, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:27,852]: Epoch: 050, Loss:0.1383 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:27,859]: Epoch: 051, Loss:0.1144 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:27,867]: Epoch: 052, Loss:0.1001 Train: 0.9833, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:27,874]: Epoch: 053, Loss:0.1134 Train: 0.9833, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:27,881]: Epoch: 054, Loss:0.1247 Train: 0.9917, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:27,889]: Epoch: 055, Loss:0.1326 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:27,897]: Epoch: 056, Loss:0.1177 Train: 0.9917, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:27,903]: Epoch: 057, Loss:0.1143 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:27,909]: Epoch: 058, Loss:0.1191 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:27,916]: Epoch: 059, Loss:0.1030 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:27,924]: Epoch: 060, Loss:0.1401 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:27,932]: Epoch: 061, Loss:0.1255 Train: 0.9833, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:27,939]: Epoch: 062, Loss:0.1355 Train: 0.9917, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:27,947]: Epoch: 063, Loss:0.0935 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:27,955]: Epoch: 064, Loss:0.1031 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:27,964]: Epoch: 065, Loss:0.1004 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:27,972]: Epoch: 066, Loss:0.0940 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0084
[2025-04-01 02:43:27,978]: Epoch: 067, Loss:0.0919 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:27,986]: Epoch: 068, Loss:0.1174 Train: 0.9833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:27,995]: Epoch: 069, Loss:0.1018 Train: 0.9750, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:28,003]: Epoch: 070, Loss:0.1762 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:28,010]: Epoch: 071, Loss:0.1252 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:28,018]: Epoch: 072, Loss:0.1022 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:28,026]: Epoch: 073, Loss:0.1162 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:28,032]: Epoch: 074, Loss:0.1147 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:28,039]: Epoch: 075, Loss:0.0868 Train: 0.9750, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:28,047]: Epoch: 076, Loss:0.1016 Train: 0.9917, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:28,055]: Epoch: 077, Loss:0.1067 Train: 0.9917, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:28,063]: Epoch: 078, Loss:0.1120 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:28,069]: Epoch: 079, Loss:0.1063 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0060
[2025-04-01 02:43:28,077]: Epoch: 080, Loss:0.1323 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:28,083]: Epoch: 081, Loss:0.1210 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:28,092]: Epoch: 082, Loss:0.1341 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,099]: Epoch: 083, Loss:0.1130 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:28,106]: Epoch: 084, Loss:0.1031 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:28,113]: Epoch: 085, Loss:0.1021 Train: 0.9833, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:28,121]: Epoch: 086, Loss:0.1482 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:28,129]: Epoch: 087, Loss:0.1046 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:28,136]: Epoch: 088, Loss:0.0718 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:28,143]: Epoch: 089, Loss:0.1190 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:28,151]: Epoch: 090, Loss:0.1222 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:28,159]: Epoch: 091, Loss:0.1242 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:28,166]: Epoch: 092, Loss:0.0874 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:28,174]: Epoch: 093, Loss:0.1051 Train: 0.9833, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:28,185]: Epoch: 094, Loss:0.0913 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0100
[2025-04-01 02:43:28,193]: Epoch: 095, Loss:0.1044 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:28,199]: Epoch: 096, Loss:0.0909 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0056
[2025-04-01 02:43:28,207]: Epoch: 097, Loss:0.0883 Train: 0.9917, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:28,215]: Epoch: 098, Loss:0.0923 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:28,222]: Epoch: 099, Loss:0.1011 Train: 0.9917, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:28,228]: Epoch: 100, Loss:0.0879 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:28,237]: Epoch: 101, Loss:0.0894 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:28,244]: Epoch: 102, Loss:0.1107 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:28,251]: Epoch: 103, Loss:0.0963 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:28,259]: Epoch: 104, Loss:0.0849 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:28,267]: Epoch: 105, Loss:0.1048 Train: 0.9917, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:28,274]: Epoch: 106, Loss:0.0868 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:28,280]: Epoch: 107, Loss:0.1330 Train: 0.9750, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:28,288]: Epoch: 108, Loss:0.1566 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:28,295]: Epoch: 109, Loss:0.1125 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:28,304]: Epoch: 110, Loss:0.0943 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0086
[2025-04-01 02:43:28,310]: Epoch: 111, Loss:0.1130 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:28,318]: Epoch: 112, Loss:0.1132 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:28,327]: Epoch: 113, Loss:0.0990 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:28,334]: Epoch: 114, Loss:0.1164 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:28,343]: Epoch: 115, Loss:0.1263 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:28,350]: Epoch: 116, Loss:0.0998 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:28,357]: Epoch: 117, Loss:0.1410 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:28,365]: Epoch: 118, Loss:0.1518 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,376]: Epoch: 119, Loss:0.0909 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0107
[2025-04-01 02:43:28,397]: Epoch: 120, Loss:0.1009 Train: 0.9750, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0206
[2025-04-01 02:43:28,412]: Epoch: 121, Loss:0.0967 Train: 0.9833, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0152
[2025-04-01 02:43:28,422]: Epoch: 122, Loss:0.0916 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0090
[2025-04-01 02:43:28,429]: Epoch: 123, Loss:0.0997 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:28,439]: Epoch: 124, Loss:0.0951 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0095
[2025-04-01 02:43:28,447]: Epoch: 125, Loss:0.1159 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:28,455]: Epoch: 126, Loss:0.1116 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:28,462]: Epoch: 127, Loss:0.1145 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:28,470]: Epoch: 128, Loss:0.1164 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:28,479]: Epoch: 129, Loss:0.0914 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:28,488]: Epoch: 130, Loss:0.1738 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0090
[2025-04-01 02:43:28,496]: Epoch: 131, Loss:0.0870 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:28,505]: Epoch: 132, Loss:0.1291 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:43:28,514]: Epoch: 133, Loss:0.1283 Train: 0.9750, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0088
[2025-04-01 02:43:28,524]: Epoch: 134, Loss:0.1217 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0089
[2025-04-01 02:43:28,532]: Epoch: 135, Loss:0.1322 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:28,540]: Epoch: 136, Loss:0.1138 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:28,548]: Epoch: 137, Loss:0.0927 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:28,556]: Epoch: 138, Loss:0.0976 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,565]: Epoch: 139, Loss:0.0813 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0090
[2025-04-01 02:43:28,573]: Epoch: 140, Loss:0.1016 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:28,579]: Epoch: 141, Loss:0.0981 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:28,587]: Epoch: 142, Loss:0.0778 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:28,595]: Epoch: 143, Loss:0.1040 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:28,601]: Epoch: 144, Loss:0.1037 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0058
[2025-04-01 02:43:28,608]: Epoch: 145, Loss:0.0995 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:28,616]: Epoch: 146, Loss:0.0878 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:28,622]: Epoch: 147, Loss:0.0893 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:28,630]: Epoch: 148, Loss:0.0947 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:28,638]: Epoch: 149, Loss:0.1177 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:28,645]: Epoch: 150, Loss:0.0934 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:28,652]: Epoch: 151, Loss:0.0999 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:28,658]: Epoch: 152, Loss:0.0980 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:28,666]: Epoch: 153, Loss:0.1202 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:28,673]: Epoch: 154, Loss:0.0929 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:28,680]: Epoch: 155, Loss:0.1195 Train: 0.9833, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:28,688]: Epoch: 156, Loss:0.0974 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:28,697]: Epoch: 157, Loss:0.1268 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:28,705]: Epoch: 158, Loss:0.1091 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:28,712]: Epoch: 159, Loss:0.1114 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:28,719]: Epoch: 160, Loss:0.0818 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:28,726]: Epoch: 161, Loss:0.0880 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:28,734]: Epoch: 162, Loss:0.0848 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:28,740]: Epoch: 163, Loss:0.0875 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:28,748]: Epoch: 164, Loss:0.1135 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:28,755]: Epoch: 165, Loss:0.0820 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:28,762]: Epoch: 166, Loss:0.0866 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:28,770]: Epoch: 167, Loss:0.1346 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:28,776]: Epoch: 168, Loss:0.0785 Train: 0.9833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:28,784]: Epoch: 169, Loss:0.1542 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,791]: Epoch: 170, Loss:0.0784 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:28,798]: Epoch: 171, Loss:0.1484 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:28,805]: Epoch: 172, Loss:0.0945 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:28,815]: Epoch: 173, Loss:0.1075 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0092
[2025-04-01 02:43:28,821]: Epoch: 174, Loss:0.1287 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:28,830]: Epoch: 175, Loss:0.1158 Train: 0.9833, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:28,838]: Epoch: 176, Loss:0.0947 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:28,846]: Epoch: 177, Loss:0.1245 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,852]: Epoch: 178, Loss:0.0912 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:28,859]: Epoch: 179, Loss:0.1424 Train: 0.9667, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:28,867]: Epoch: 180, Loss:0.1237 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:28,874]: Epoch: 181, Loss:0.1128 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:28,881]: Epoch: 182, Loss:0.0975 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:28,889]: Epoch: 183, Loss:0.1333 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,897]: Epoch: 184, Loss:0.1208 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:28,903]: Epoch: 185, Loss:0.0873 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:28,910]: Epoch: 186, Loss:0.0822 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:28,918]: Epoch: 187, Loss:0.1125 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:28,926]: Epoch: 188, Loss:0.0863 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:28,934]: Epoch: 189, Loss:0.0895 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:28,941]: Epoch: 190, Loss:0.0843 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:28,949]: Epoch: 191, Loss:0.1083 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:28,958]: Epoch: 192, Loss:0.0645 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0088
[2025-04-01 02:43:28,965]: Epoch: 193, Loss:0.0855 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:28,972]: Epoch: 194, Loss:0.0960 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:28,979]: Epoch: 195, Loss:0.0852 Train: 0.9917, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:28,988]: Epoch: 196, Loss:0.0861 Train: 0.9917, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0089
[2025-04-01 02:43:28,996]: Epoch: 197, Loss:0.1028 Train: 0.9917, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:29,003]: Epoch: 198, Loss:0.0784 Train: 0.9917, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:29,011]: Epoch: 199, Loss:0.0820 Train: 0.9917, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:29,019]: Epoch: 200, Loss:0.1115 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:29,019]: [Run-1 score] {'train': 0.975, 'val': 0.75, 'test': 0.6470588235294118}
[2025-04-01 02:43:29,019]: repeat 2/3
[2025-04-01 02:43:29,019]: Manual random seed:0
[2025-04-01 02:43:29,020]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:29,023]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:29,033]: Epoch: 001, Loss:1.5695 Train: 0.7833, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:29,039]: Epoch: 002, Loss:0.9199 Train: 0.8500, Val:0.7250, Test: 0.7451, Time(s/epoch):0.0054
[2025-04-01 02:43:29,045]: Epoch: 003, Loss:0.6315 Train: 0.8500, Val:0.7500, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:29,053]: Epoch: 004, Loss:0.5307 Train: 0.8750, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:29,061]: Epoch: 005, Loss:0.4292 Train: 0.8583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:29,070]: Epoch: 006, Loss:0.3743 Train: 0.8917, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:29,076]: Epoch: 007, Loss:0.3798 Train: 0.9250, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:29,083]: Epoch: 008, Loss:0.2676 Train: 0.9250, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:29,090]: Epoch: 009, Loss:0.2989 Train: 0.9333, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:29,099]: Epoch: 010, Loss:0.2665 Train: 0.9250, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:29,107]: Epoch: 011, Loss:0.2370 Train: 0.9333, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:29,115]: Epoch: 012, Loss:0.2476 Train: 0.9500, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:29,124]: Epoch: 013, Loss:0.2008 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:29,130]: Epoch: 014, Loss:0.1487 Train: 0.9417, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:29,141]: Epoch: 015, Loss:0.2292 Train: 0.9333, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0100
[2025-04-01 02:43:29,149]: Epoch: 016, Loss:0.1985 Train: 0.9667, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:29,158]: Epoch: 017, Loss:0.2150 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:43:29,167]: Epoch: 018, Loss:0.1435 Train: 0.9667, Val:0.7375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:29,176]: Epoch: 019, Loss:0.1594 Train: 0.9583, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0094
[2025-04-01 02:43:29,185]: Epoch: 020, Loss:0.1514 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:29,193]: Epoch: 021, Loss:0.1422 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:29,200]: Epoch: 022, Loss:0.1741 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:29,207]: Epoch: 023, Loss:0.1270 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:29,214]: Epoch: 024, Loss:0.1313 Train: 0.9917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:29,223]: Epoch: 025, Loss:0.1135 Train: 0.9917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:29,228]: Epoch: 026, Loss:0.1378 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0056
[2025-04-01 02:43:29,236]: Epoch: 027, Loss:0.1251 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:29,244]: Epoch: 028, Loss:0.1197 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:29,249]: Epoch: 029, Loss:0.1146 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0057
[2025-04-01 02:43:29,259]: Epoch: 030, Loss:0.1305 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:29,267]: Epoch: 031, Loss:0.1251 Train: 0.9750, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:29,275]: Epoch: 032, Loss:0.0995 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:29,282]: Epoch: 033, Loss:0.1242 Train: 0.9917, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:29,290]: Epoch: 034, Loss:0.1338 Train: 0.9917, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:29,299]: Epoch: 035, Loss:0.1211 Train: 0.9917, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:29,305]: Epoch: 036, Loss:0.1270 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:29,312]: Epoch: 037, Loss:0.1392 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:29,319]: Epoch: 038, Loss:0.1097 Train: 0.9917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:29,326]: Epoch: 039, Loss:0.1251 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:29,334]: Epoch: 040, Loss:0.0997 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:29,340]: Epoch: 041, Loss:0.1013 Train: 0.9667, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:29,347]: Epoch: 042, Loss:0.1253 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:29,354]: Epoch: 043, Loss:0.0910 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:29,361]: Epoch: 044, Loss:0.1307 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:29,369]: Epoch: 045, Loss:0.1082 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:29,377]: Epoch: 046, Loss:0.0903 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:29,384]: Epoch: 047, Loss:0.0774 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:29,390]: Epoch: 048, Loss:0.1160 Train: 0.9833, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:29,398]: Epoch: 049, Loss:0.1301 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:29,406]: Epoch: 050, Loss:0.1104 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:29,412]: Epoch: 051, Loss:0.1034 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:29,419]: Epoch: 052, Loss:0.1448 Train: 0.9667, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:29,427]: Epoch: 053, Loss:0.1004 Train: 0.9833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:29,434]: Epoch: 054, Loss:0.1150 Train: 0.9917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:29,440]: Epoch: 055, Loss:0.0937 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:29,449]: Epoch: 056, Loss:0.1058 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0086
[2025-04-01 02:43:29,457]: Epoch: 057, Loss:0.1113 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:29,465]: Epoch: 058, Loss:0.0813 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:29,473]: Epoch: 059, Loss:0.1050 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:29,481]: Epoch: 060, Loss:0.0861 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:29,488]: Epoch: 061, Loss:0.1056 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:29,497]: Epoch: 062, Loss:0.1056 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:29,504]: Epoch: 063, Loss:0.0990 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:29,510]: Epoch: 064, Loss:0.1106 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0055
[2025-04-01 02:43:29,517]: Epoch: 065, Loss:0.1140 Train: 0.9833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:29,525]: Epoch: 066, Loss:0.1373 Train: 0.9833, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:29,533]: Epoch: 067, Loss:0.1848 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:29,541]: Epoch: 068, Loss:0.0993 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:29,547]: Epoch: 069, Loss:0.1129 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:29,554]: Epoch: 070, Loss:0.0756 Train: 0.9750, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:29,560]: Epoch: 071, Loss:0.1100 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0056
[2025-04-01 02:43:29,567]: Epoch: 072, Loss:0.1152 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:29,576]: Epoch: 073, Loss:0.1006 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:29,584]: Epoch: 074, Loss:0.0971 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:29,589]: Epoch: 075, Loss:0.0911 Train: 0.9917, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0054
[2025-04-01 02:43:29,596]: Epoch: 076, Loss:0.1071 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:29,603]: Epoch: 077, Loss:0.0932 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:29,609]: Epoch: 078, Loss:0.1272 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:29,618]: Epoch: 079, Loss:0.0945 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:29,624]: Epoch: 080, Loss:0.1246 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:29,630]: Epoch: 081, Loss:0.1331 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:29,637]: Epoch: 082, Loss:0.1082 Train: 0.9833, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:29,645]: Epoch: 083, Loss:0.0989 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:29,652]: Epoch: 084, Loss:0.0946 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:29,660]: Epoch: 085, Loss:0.0964 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:29,668]: Epoch: 086, Loss:0.0951 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:29,676]: Epoch: 087, Loss:0.1080 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:29,697]: Epoch: 088, Loss:0.1266 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0209
[2025-04-01 02:43:29,707]: Epoch: 089, Loss:0.1303 Train: 0.9750, Val:0.7250, Test: 0.5882, Time(s/epoch):0.0099
[2025-04-01 02:43:29,715]: Epoch: 090, Loss:0.1065 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:29,724]: Epoch: 091, Loss:0.0778 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:43:29,731]: Epoch: 092, Loss:0.1108 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:29,739]: Epoch: 093, Loss:0.0841 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:29,746]: Epoch: 094, Loss:0.1008 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:29,755]: Epoch: 095, Loss:0.1088 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0087
[2025-04-01 02:43:29,763]: Epoch: 096, Loss:0.1249 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:29,771]: Epoch: 097, Loss:0.1334 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:29,779]: Epoch: 098, Loss:0.1224 Train: 0.9917, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:29,789]: Epoch: 099, Loss:0.1001 Train: 0.9833, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0091
[2025-04-01 02:43:29,796]: Epoch: 100, Loss:0.1123 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:29,803]: Epoch: 101, Loss:0.1105 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:29,809]: Epoch: 102, Loss:0.0861 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:29,817]: Epoch: 103, Loss:0.1144 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:29,825]: Epoch: 104, Loss:0.1073 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:29,833]: Epoch: 105, Loss:0.1466 Train: 0.9917, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:29,839]: Epoch: 106, Loss:0.0948 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0056
[2025-04-01 02:43:29,846]: Epoch: 107, Loss:0.1184 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:29,853]: Epoch: 108, Loss:0.1011 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:29,861]: Epoch: 109, Loss:0.0879 Train: 0.9833, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:29,869]: Epoch: 110, Loss:0.1087 Train: 0.9833, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:29,876]: Epoch: 111, Loss:0.0862 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:29,884]: Epoch: 112, Loss:0.0994 Train: 0.9917, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:29,892]: Epoch: 113, Loss:0.1181 Train: 0.9917, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:29,900]: Epoch: 114, Loss:0.0819 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:29,908]: Epoch: 115, Loss:0.1694 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:29,916]: Epoch: 116, Loss:0.0949 Train: 0.9917, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:29,924]: Epoch: 117, Loss:0.1121 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:29,933]: Epoch: 118, Loss:0.1474 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:29,941]: Epoch: 119, Loss:0.0928 Train: 0.9750, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:43:29,949]: Epoch: 120, Loss:0.1140 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:29,956]: Epoch: 121, Loss:0.1562 Train: 0.9917, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:29,965]: Epoch: 122, Loss:0.1154 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0087
[2025-04-01 02:43:29,971]: Epoch: 123, Loss:0.1270 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0056
[2025-04-01 02:43:29,979]: Epoch: 124, Loss:0.1036 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:29,986]: Epoch: 125, Loss:0.1114 Train: 0.9917, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:29,994]: Epoch: 126, Loss:0.0838 Train: 0.9917, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:30,003]: Epoch: 127, Loss:0.1027 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0089
[2025-04-01 02:43:30,012]: Epoch: 128, Loss:0.1131 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:30,018]: Epoch: 129, Loss:0.1111 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0060
[2025-04-01 02:43:30,025]: Epoch: 130, Loss:0.1075 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:30,032]: Epoch: 131, Loss:0.0811 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:30,038]: Epoch: 132, Loss:0.1372 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:30,045]: Epoch: 133, Loss:0.0825 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:30,053]: Epoch: 134, Loss:0.1119 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:30,059]: Epoch: 135, Loss:0.0953 Train: 0.9833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:30,067]: Epoch: 136, Loss:0.1626 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:30,074]: Epoch: 137, Loss:0.0911 Train: 0.9833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:30,082]: Epoch: 138, Loss:0.0925 Train: 0.9833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:30,090]: Epoch: 139, Loss:0.0815 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:30,098]: Epoch: 140, Loss:0.0860 Train: 0.9750, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:30,104]: Epoch: 141, Loss:0.0927 Train: 0.9833, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:30,110]: Epoch: 142, Loss:0.0693 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0059
[2025-04-01 02:43:30,118]: Epoch: 143, Loss:0.0783 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:30,125]: Epoch: 144, Loss:0.0870 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:30,133]: Epoch: 145, Loss:0.1085 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:30,140]: Epoch: 146, Loss:0.0916 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:30,148]: Epoch: 147, Loss:0.0793 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:30,157]: Epoch: 148, Loss:0.0818 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:30,165]: Epoch: 149, Loss:0.0669 Train: 0.9917, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:30,172]: Epoch: 150, Loss:0.1206 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:30,179]: Epoch: 151, Loss:0.0964 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0059
[2025-04-01 02:43:30,188]: Epoch: 152, Loss:0.0846 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0098
[2025-04-01 02:43:30,197]: Epoch: 153, Loss:0.0743 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0088
[2025-04-01 02:43:30,205]: Epoch: 154, Loss:0.0818 Train: 0.9667, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:30,212]: Epoch: 155, Loss:0.1108 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:30,220]: Epoch: 156, Loss:0.0865 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:30,228]: Epoch: 157, Loss:0.0782 Train: 0.9917, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:30,234]: Epoch: 158, Loss:0.0984 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:30,241]: Epoch: 159, Loss:0.0654 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:30,249]: Epoch: 160, Loss:0.0818 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:30,256]: Epoch: 161, Loss:0.0869 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:30,264]: Epoch: 162, Loss:0.0944 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:30,271]: Epoch: 163, Loss:0.0912 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:30,279]: Epoch: 164, Loss:0.0809 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:30,287]: Epoch: 165, Loss:0.0792 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:30,295]: Epoch: 166, Loss:0.0616 Train: 0.9833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:30,303]: Epoch: 167, Loss:0.0809 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:43:30,311]: Epoch: 168, Loss:0.0899 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:30,319]: Epoch: 169, Loss:0.0783 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:30,327]: Epoch: 170, Loss:0.1269 Train: 0.9917, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:30,334]: Epoch: 171, Loss:0.1242 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:30,342]: Epoch: 172, Loss:0.1009 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:30,349]: Epoch: 173, Loss:0.0982 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:30,356]: Epoch: 174, Loss:0.0846 Train: 0.9833, Val:0.7125, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:30,363]: Epoch: 175, Loss:0.0891 Train: 0.9750, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:30,371]: Epoch: 176, Loss:0.1072 Train: 0.9917, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:30,379]: Epoch: 177, Loss:0.0960 Train: 0.9917, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:30,387]: Epoch: 178, Loss:0.0967 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:30,395]: Epoch: 179, Loss:0.1038 Train: 0.9667, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:30,401]: Epoch: 180, Loss:0.1075 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:30,409]: Epoch: 181, Loss:0.0963 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:30,416]: Epoch: 182, Loss:0.0958 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:30,424]: Epoch: 183, Loss:0.0893 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:30,430]: Epoch: 184, Loss:0.0758 Train: 0.9917, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0059
[2025-04-01 02:43:30,438]: Epoch: 185, Loss:0.0868 Train: 0.9917, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:30,444]: Epoch: 186, Loss:0.0934 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:30,451]: Epoch: 187, Loss:0.0807 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:30,458]: Epoch: 188, Loss:0.0853 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:30,465]: Epoch: 189, Loss:0.0795 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:30,471]: Epoch: 190, Loss:0.0864 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:30,480]: Epoch: 191, Loss:0.0821 Train: 0.9750, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:30,488]: Epoch: 192, Loss:0.1117 Train: 0.9833, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:30,495]: Epoch: 193, Loss:0.0614 Train: 0.9750, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:30,501]: Epoch: 194, Loss:0.0808 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:30,510]: Epoch: 195, Loss:0.0715 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0084
[2025-04-01 02:43:30,517]: Epoch: 196, Loss:0.0802 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:30,524]: Epoch: 197, Loss:0.1158 Train: 0.9917, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:30,530]: Epoch: 198, Loss:0.0549 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:30,540]: Epoch: 199, Loss:0.1102 Train: 0.9833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0098
[2025-04-01 02:43:30,549]: Epoch: 200, Loss:0.1007 Train: 0.9917, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0083
[2025-04-01 02:43:30,549]: [Run-2 score] {'train': 0.85, 'val': 0.75, 'test': 0.7058823529411765}
[2025-04-01 02:43:30,549]: repeat 3/3
[2025-04-01 02:43:30,549]: Manual random seed:0
[2025-04-01 02:43:30,549]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:30,553]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:30,564]: Epoch: 001, Loss:1.7249 Train: 0.7500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:30,571]: Epoch: 002, Loss:1.0602 Train: 0.7917, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:30,577]: Epoch: 003, Loss:0.7321 Train: 0.8417, Val:0.7500, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:30,586]: Epoch: 004, Loss:0.5106 Train: 0.8667, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:30,596]: Epoch: 005, Loss:0.4875 Train: 0.8583, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0099
[2025-04-01 02:43:30,605]: Epoch: 006, Loss:0.3840 Train: 0.9000, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0093
[2025-04-01 02:43:30,615]: Epoch: 007, Loss:0.3642 Train: 0.8917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0094
[2025-04-01 02:43:30,622]: Epoch: 008, Loss:0.3099 Train: 0.9083, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:30,629]: Epoch: 009, Loss:0.3047 Train: 0.9250, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:30,636]: Epoch: 010, Loss:0.2733 Train: 0.9250, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:30,645]: Epoch: 011, Loss:0.2799 Train: 0.9333, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:30,653]: Epoch: 012, Loss:0.2902 Train: 0.9667, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:30,660]: Epoch: 013, Loss:0.2493 Train: 0.9500, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0059
[2025-04-01 02:43:30,668]: Epoch: 014, Loss:0.2460 Train: 0.9333, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:30,676]: Epoch: 015, Loss:0.2137 Train: 0.9417, Val:0.7250, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:30,683]: Epoch: 016, Loss:0.2232 Train: 0.9417, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:30,689]: Epoch: 017, Loss:0.2081 Train: 0.9667, Val:0.7625, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:30,697]: Epoch: 018, Loss:0.1967 Train: 0.9667, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:30,703]: Epoch: 019, Loss:0.1766 Train: 0.9750, Val:0.7375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:30,710]: Epoch: 020, Loss:0.1644 Train: 0.9833, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:30,718]: Epoch: 021, Loss:0.1272 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:30,725]: Epoch: 022, Loss:0.1490 Train: 0.9667, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:30,733]: Epoch: 023, Loss:0.1777 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:30,739]: Epoch: 024, Loss:0.1835 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:30,747]: Epoch: 025, Loss:0.1730 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:30,754]: Epoch: 026, Loss:0.1395 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:30,762]: Epoch: 027, Loss:0.1484 Train: 0.9750, Val:0.7375, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:30,771]: Epoch: 028, Loss:0.1342 Train: 0.9750, Val:0.7375, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:30,777]: Epoch: 029, Loss:0.1301 Train: 0.9917, Val:0.7375, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:30,785]: Epoch: 030, Loss:0.1126 Train: 0.9833, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:30,793]: Epoch: 031, Loss:0.1362 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:30,801]: Epoch: 032, Loss:0.1444 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:30,808]: Epoch: 033, Loss:0.1851 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:30,816]: Epoch: 034, Loss:0.1478 Train: 0.9833, Val:0.7500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:30,824]: Epoch: 035, Loss:0.1378 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:30,831]: Epoch: 036, Loss:0.1104 Train: 0.9750, Val:0.7500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:30,838]: Epoch: 037, Loss:0.1882 Train: 0.9917, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:30,844]: Epoch: 038, Loss:0.1243 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:30,852]: Epoch: 039, Loss:0.1248 Train: 0.9667, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:30,860]: Epoch: 040, Loss:0.1299 Train: 0.9667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:30,868]: Epoch: 041, Loss:0.1563 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:30,875]: Epoch: 042, Loss:0.1523 Train: 0.9750, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0066
[2025-04-01 02:43:30,883]: Epoch: 043, Loss:0.1102 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:30,891]: Epoch: 044, Loss:0.1214 Train: 0.9750, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:30,899]: Epoch: 045, Loss:0.1028 Train: 0.9667, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:30,906]: Epoch: 046, Loss:0.1234 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:30,914]: Epoch: 047, Loss:0.1694 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:30,921]: Epoch: 048, Loss:0.1453 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:30,930]: Epoch: 049, Loss:0.1382 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0079
[2025-04-01 02:43:30,937]: Epoch: 050, Loss:0.1257 Train: 0.9833, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:30,946]: Epoch: 051, Loss:0.1011 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:30,953]: Epoch: 052, Loss:0.1007 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:30,961]: Epoch: 053, Loss:0.1208 Train: 0.9750, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:30,969]: Epoch: 054, Loss:0.1451 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:30,978]: Epoch: 055, Loss:0.1103 Train: 0.9833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:30,986]: Epoch: 056, Loss:0.1461 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:30,992]: Epoch: 057, Loss:0.1353 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:30,999]: Epoch: 058, Loss:0.1229 Train: 0.9917, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0062
[2025-04-01 02:43:31,006]: Epoch: 059, Loss:0.1325 Train: 0.9917, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:31,013]: Epoch: 060, Loss:0.1145 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:31,019]: Epoch: 061, Loss:0.1096 Train: 0.9917, Val:0.7250, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:31,026]: Epoch: 062, Loss:0.1198 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:31,033]: Epoch: 063, Loss:0.1319 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:31,040]: Epoch: 064, Loss:0.1060 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:31,047]: Epoch: 065, Loss:0.0737 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:31,054]: Epoch: 066, Loss:0.1377 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,062]: Epoch: 067, Loss:0.1005 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0077
[2025-04-01 02:43:31,069]: Epoch: 068, Loss:0.1001 Train: 0.9750, Val:0.7250, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:31,075]: Epoch: 069, Loss:0.1179 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:31,082]: Epoch: 070, Loss:0.0888 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:31,090]: Epoch: 071, Loss:0.0935 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:31,098]: Epoch: 072, Loss:0.1081 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0081
[2025-04-01 02:43:31,106]: Epoch: 073, Loss:0.0872 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:31,114]: Epoch: 074, Loss:0.1342 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:31,122]: Epoch: 075, Loss:0.0969 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:31,130]: Epoch: 076, Loss:0.0872 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,137]: Epoch: 077, Loss:0.1097 Train: 0.9917, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,144]: Epoch: 078, Loss:0.1026 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:31,152]: Epoch: 079, Loss:0.0966 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:31,159]: Epoch: 080, Loss:0.1128 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:31,167]: Epoch: 081, Loss:0.1011 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:31,175]: Epoch: 082, Loss:0.1046 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:31,186]: Epoch: 083, Loss:0.0989 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0108
[2025-04-01 02:43:31,196]: Epoch: 084, Loss:0.0949 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0094
[2025-04-01 02:43:31,204]: Epoch: 085, Loss:0.0888 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:31,213]: Epoch: 086, Loss:0.1131 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:43:31,222]: Epoch: 087, Loss:0.0915 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:31,229]: Epoch: 088, Loss:0.1166 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,237]: Epoch: 089, Loss:0.1187 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:31,246]: Epoch: 090, Loss:0.1004 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:31,254]: Epoch: 091, Loss:0.1026 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:31,260]: Epoch: 092, Loss:0.1181 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:31,269]: Epoch: 093, Loss:0.1051 Train: 0.9750, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0086
[2025-04-01 02:43:31,277]: Epoch: 094, Loss:0.1179 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:31,285]: Epoch: 095, Loss:0.0922 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:31,293]: Epoch: 096, Loss:0.1043 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:31,299]: Epoch: 097, Loss:0.0735 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:31,306]: Epoch: 098, Loss:0.0990 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:31,314]: Epoch: 099, Loss:0.0757 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:31,320]: Epoch: 100, Loss:0.1340 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0063
[2025-04-01 02:43:31,329]: Epoch: 101, Loss:0.0945 Train: 0.9917, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:43:31,336]: Epoch: 102, Loss:0.1034 Train: 0.9750, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:31,342]: Epoch: 103, Loss:0.1235 Train: 0.9750, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0060
[2025-04-01 02:43:31,349]: Epoch: 104, Loss:0.1007 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:31,357]: Epoch: 105, Loss:0.0902 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:31,364]: Epoch: 106, Loss:0.1149 Train: 0.9750, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:31,371]: Epoch: 107, Loss:0.1413 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:31,378]: Epoch: 108, Loss:0.0955 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:31,386]: Epoch: 109, Loss:0.0920 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:31,394]: Epoch: 110, Loss:0.0919 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:31,400]: Epoch: 111, Loss:0.1053 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0057
[2025-04-01 02:43:31,407]: Epoch: 112, Loss:0.1081 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:31,414]: Epoch: 113, Loss:0.0989 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:31,420]: Epoch: 114, Loss:0.1251 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0059
[2025-04-01 02:43:31,428]: Epoch: 115, Loss:0.1128 Train: 0.9750, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:31,435]: Epoch: 116, Loss:0.1082 Train: 0.9667, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:31,443]: Epoch: 117, Loss:0.1053 Train: 0.9917, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:31,451]: Epoch: 118, Loss:0.1081 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:31,458]: Epoch: 119, Loss:0.1448 Train: 0.9750, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:31,464]: Epoch: 120, Loss:0.1183 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:31,472]: Epoch: 121, Loss:0.1160 Train: 0.9833, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:31,478]: Epoch: 122, Loss:0.1493 Train: 0.9667, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:31,485]: Epoch: 123, Loss:0.1176 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:31,492]: Epoch: 124, Loss:0.1043 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:31,500]: Epoch: 125, Loss:0.1084 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:31,507]: Epoch: 126, Loss:0.1490 Train: 0.9833, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:43:31,515]: Epoch: 127, Loss:0.0994 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:31,523]: Epoch: 128, Loss:0.1068 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:31,529]: Epoch: 129, Loss:0.1105 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0057
[2025-04-01 02:43:31,536]: Epoch: 130, Loss:0.0963 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:31,544]: Epoch: 131, Loss:0.0958 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:31,551]: Epoch: 132, Loss:0.1184 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0065
[2025-04-01 02:43:31,558]: Epoch: 133, Loss:0.0765 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:31,566]: Epoch: 134, Loss:0.1047 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:31,575]: Epoch: 135, Loss:0.0972 Train: 0.9750, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:31,583]: Epoch: 136, Loss:0.0969 Train: 0.9750, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:31,590]: Epoch: 137, Loss:0.1062 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:31,597]: Epoch: 138, Loss:0.1051 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0061
[2025-04-01 02:43:31,603]: Epoch: 139, Loss:0.0659 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:31,609]: Epoch: 140, Loss:0.1437 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0060
[2025-04-01 02:43:31,617]: Epoch: 141, Loss:0.1188 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:31,625]: Epoch: 142, Loss:0.1064 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,631]: Epoch: 143, Loss:0.1068 Train: 0.9833, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0058
[2025-04-01 02:43:31,638]: Epoch: 144, Loss:0.0802 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:31,645]: Epoch: 145, Loss:0.0886 Train: 0.9833, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:31,653]: Epoch: 146, Loss:0.0826 Train: 0.9917, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:31,661]: Epoch: 147, Loss:0.1178 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,669]: Epoch: 148, Loss:0.0932 Train: 0.9750, Val:0.7125, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:31,676]: Epoch: 149, Loss:0.0921 Train: 0.9667, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:43:31,684]: Epoch: 150, Loss:0.1079 Train: 0.9917, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:43:31,692]: Epoch: 151, Loss:0.1198 Train: 0.9750, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:31,698]: Epoch: 152, Loss:0.1637 Train: 0.9750, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:31,706]: Epoch: 153, Loss:0.0927 Train: 0.9750, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:31,713]: Epoch: 154, Loss:0.1264 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:31,719]: Epoch: 155, Loss:0.0904 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0057
[2025-04-01 02:43:31,728]: Epoch: 156, Loss:0.0794 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:31,735]: Epoch: 157, Loss:0.0894 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:43:31,741]: Epoch: 158, Loss:0.1088 Train: 0.9917, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:31,749]: Epoch: 159, Loss:0.0898 Train: 0.9833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:31,756]: Epoch: 160, Loss:0.1031 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:31,764]: Epoch: 161, Loss:0.0913 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:31,771]: Epoch: 162, Loss:0.0850 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:31,779]: Epoch: 163, Loss:0.1197 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:31,787]: Epoch: 164, Loss:0.0981 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:31,795]: Epoch: 165, Loss:0.0824 Train: 0.9833, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:31,801]: Epoch: 166, Loss:0.1358 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:31,809]: Epoch: 167, Loss:0.0946 Train: 0.9917, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0075
[2025-04-01 02:43:31,817]: Epoch: 168, Loss:0.0876 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:31,823]: Epoch: 169, Loss:0.0723 Train: 0.9917, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:31,829]: Epoch: 170, Loss:0.0856 Train: 0.9917, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0060
[2025-04-01 02:43:31,836]: Epoch: 171, Loss:0.0900 Train: 0.9833, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:31,843]: Epoch: 172, Loss:0.0734 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:43:31,850]: Epoch: 173, Loss:0.0922 Train: 0.9833, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:31,857]: Epoch: 174, Loss:0.0819 Train: 0.9833, Val:0.7000, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:31,864]: Epoch: 175, Loss:0.1709 Train: 0.9917, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:31,872]: Epoch: 176, Loss:0.0788 Train: 0.9750, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:31,878]: Epoch: 177, Loss:0.1121 Train: 0.9750, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:31,885]: Epoch: 178, Loss:0.1516 Train: 0.9833, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:31,894]: Epoch: 179, Loss:0.1334 Train: 0.9750, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:43:31,900]: Epoch: 180, Loss:0.0841 Train: 0.9833, Val:0.7125, Test: 0.6471, Time(s/epoch):0.0057
[2025-04-01 02:43:31,906]: Epoch: 181, Loss:0.1239 Train: 0.9833, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:31,915]: Epoch: 182, Loss:0.0978 Train: 0.9917, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:31,923]: Epoch: 183, Loss:0.1004 Train: 0.9833, Val:0.7125, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:31,929]: Epoch: 184, Loss:0.1103 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0056
[2025-04-01 02:43:31,936]: Epoch: 185, Loss:0.0895 Train: 0.9750, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:31,944]: Epoch: 186, Loss:0.1116 Train: 0.9833, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:31,949]: Epoch: 187, Loss:0.0739 Train: 0.9917, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0055
[2025-04-01 02:43:31,957]: Epoch: 188, Loss:0.1014 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:31,965]: Epoch: 189, Loss:0.1129 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:31,973]: Epoch: 190, Loss:0.0897 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:31,982]: Epoch: 191, Loss:0.0757 Train: 0.9833, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0084
[2025-04-01 02:43:31,989]: Epoch: 192, Loss:0.1058 Train: 0.9917, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:31,998]: Epoch: 193, Loss:0.0998 Train: 0.9917, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0090
[2025-04-01 02:43:32,007]: Epoch: 194, Loss:0.0948 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:43:32,017]: Epoch: 195, Loss:0.0774 Train: 0.9833, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0101
[2025-04-01 02:43:32,027]: Epoch: 196, Loss:0.1578 Train: 0.9917, Val:0.7000, Test: 0.6078, Time(s/epoch):0.0093
[2025-04-01 02:43:32,035]: Epoch: 197, Loss:0.0829 Train: 0.9833, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:32,045]: Epoch: 198, Loss:0.1042 Train: 0.9667, Val:0.6375, Test: 0.5686, Time(s/epoch):0.0094
[2025-04-01 02:43:32,053]: Epoch: 199, Loss:0.1387 Train: 0.9833, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:32,060]: Epoch: 200, Loss:0.1244 Train: 0.9833, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:32,061]: [Run-3 score] {'train': 0.9416666666666667, 'val': 0.7625, 'test': 0.6666666666666666}
[2025-04-01 02:43:32,061]: repeat 1/3
[2025-04-01 02:43:32,061]: Manual random seed:0
[2025-04-01 02:43:32,061]: auto fixed data split seed to 0, model init seed to 0

Epoch: 022, Supervised Loss:1.6583 Time(s/epoch):0.0065
Epoch: 023, Supervised Loss:1.6517 Time(s/epoch):0.0073
Epoch: 024, Supervised Loss:1.6480 Time(s/epoch):0.0075
Epoch: 025, Supervised Loss:1.6466 Time(s/epoch):0.0067
Epoch: 026, Supervised Loss:1.6468 Time(s/epoch):0.0070
Epoch: 027, Supervised Loss:1.6478 Time(s/epoch):0.0066
Epoch: 028, Supervised Loss:1.6490 Time(s/epoch):0.0064
Epoch: 029, Supervised Loss:1.6506 Time(s/epoch):0.0068
Epoch: 030, Supervised Loss:1.6519 Time(s/epoch):0.0065
0.9318529 515
Add 367 edges.
Prune 355 edges from torch.Size([2, 515]) to torch.Size([2, 160])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.701
Data(x=[251, 1703], edge_index=[2, 521], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.5168 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.701
Data(x=[251, 1703], edge_index=[2, 521], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5268 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.701
Data(x=[251, 1703], edge_index=[2, 521], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5087 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(29)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0175
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0100
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0118
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0096
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0115
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0083
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0071
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0093
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0075
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0096
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0109
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0107
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0080
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0074
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0082
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0075
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0075
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0076
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0074
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0075
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0078
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0075
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0090
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0096
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0101
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0097
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0096
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0083
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0077
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0071
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0072
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0071
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0072
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0087
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0096
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0084
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0076
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0096
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0095
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0095
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0096
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0093
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0080
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0074
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0075
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0074
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0093
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0096
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0094
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0096
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0095
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0073
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0071
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0072
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0077
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0075
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0071
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0074
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0071
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0081
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0075
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0075
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0073
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0077
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0081
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0078
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0078
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0083
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0079
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0090
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0088
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0104
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0102
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0100
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0086
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0079
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0097
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0101
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0097
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0098
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0093
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0096
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0095
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0098
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0083
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0072
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0076
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0071
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0074
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0071
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0075
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0099
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0099
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0096
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0095
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0096
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0096
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0095
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0092
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0072
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0070
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0079
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0077
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0082[2025-04-01 02:43:33,983]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:33,992]: Epoch: 001, Loss:1.7905 Train: 0.5583, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:43:34,001]: Epoch: 002, Loss:1.1903 Train: 0.7833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:34,009]: Epoch: 003, Loss:0.8058 Train: 0.8250, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0082
[2025-04-01 02:43:34,017]: Epoch: 004, Loss:0.5748 Train: 0.8750, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0080
[2025-04-01 02:43:34,024]: Epoch: 005, Loss:0.5908 Train: 0.8917, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:34,031]: Epoch: 006, Loss:0.4396 Train: 0.9000, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:34,038]: Epoch: 007, Loss:0.5354 Train: 0.9250, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:34,046]: Epoch: 008, Loss:0.3833 Train: 0.9417, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:34,055]: Epoch: 009, Loss:0.3557 Train: 0.9417, Val:0.7250, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:34,062]: Epoch: 010, Loss:0.3022 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:34,069]: Epoch: 011, Loss:0.2738 Train: 0.9417, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:34,077]: Epoch: 012, Loss:0.2391 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:34,086]: Epoch: 013, Loss:0.2474 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:34,092]: Epoch: 014, Loss:0.2726 Train: 0.9500, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:34,100]: Epoch: 015, Loss:0.2697 Train: 0.9417, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:34,107]: Epoch: 016, Loss:0.2125 Train: 0.9417, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0065
[2025-04-01 02:43:34,114]: Epoch: 017, Loss:0.2351 Train: 0.9500, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:34,122]: Epoch: 018, Loss:0.2499 Train: 0.9417, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:43:34,129]: Epoch: 019, Loss:0.2131 Train: 0.9583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:34,136]: Epoch: 020, Loss:0.2087 Train: 0.9500, Val:0.6875, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:43:34,142]: Epoch: 021, Loss:0.1782 Train: 0.9417, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:34,149]: Epoch: 022, Loss:0.2194 Train: 0.9167, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:34,157]: Epoch: 023, Loss:0.1876 Train: 0.9167, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:34,165]: Epoch: 024, Loss:0.1638 Train: 0.9167, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:34,173]: Epoch: 025, Loss:0.1704 Train: 0.9250, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:34,182]: Epoch: 026, Loss:0.1623 Train: 0.9500, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0094
[2025-04-01 02:43:34,191]: Epoch: 027, Loss:0.1675 Train: 0.9750, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0082
[2025-04-01 02:43:34,198]: Epoch: 028, Loss:0.1571 Train: 0.9667, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:34,207]: Epoch: 029, Loss:0.1673 Train: 0.9417, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:34,215]: Epoch: 030, Loss:0.1966 Train: 0.9500, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:34,222]: Epoch: 031, Loss:0.1864 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:34,230]: Epoch: 032, Loss:0.1412 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:34,238]: Epoch: 033, Loss:0.1409 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:34,246]: Epoch: 034, Loss:0.1330 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:34,255]: Epoch: 035, Loss:0.2018 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:34,263]: Epoch: 036, Loss:0.1736 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:34,270]: Epoch: 037, Loss:0.1676 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:34,278]: Epoch: 038, Loss:0.1431 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:34,286]: Epoch: 039, Loss:0.1381 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:34,293]: Epoch: 040, Loss:0.1488 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:34,299]: Epoch: 041, Loss:0.1577 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:34,306]: Epoch: 042, Loss:0.1403 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:34,314]: Epoch: 043, Loss:0.1323 Train: 0.9333, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:34,322]: Epoch: 044, Loss:0.2023 Train: 0.9500, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:34,328]: Epoch: 045, Loss:0.1364 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:34,338]: Epoch: 046, Loss:0.1720 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0092
[2025-04-01 02:43:34,346]: Epoch: 047, Loss:0.1669 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:34,354]: Epoch: 048, Loss:0.1483 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:34,361]: Epoch: 049, Loss:0.1626 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:34,368]: Epoch: 050, Loss:0.1492 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:34,377]: Epoch: 051, Loss:0.1420 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:34,384]: Epoch: 052, Loss:0.1142 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:34,392]: Epoch: 053, Loss:0.1567 Train: 0.9500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:34,399]: Epoch: 054, Loss:0.1628 Train: 0.9583, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:34,406]: Epoch: 055, Loss:0.1383 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:34,413]: Epoch: 056, Loss:0.1459 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:34,419]: Epoch: 057, Loss:0.1621 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:34,426]: Epoch: 058, Loss:0.1678 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:34,432]: Epoch: 059, Loss:0.1393 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:34,438]: Epoch: 060, Loss:0.1395 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:34,446]: Epoch: 061, Loss:0.1268 Train: 0.9583, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:34,454]: Epoch: 062, Loss:0.2075 Train: 0.9750, Val:0.6875, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:34,460]: Epoch: 063, Loss:0.1700 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:34,468]: Epoch: 064, Loss:0.1760 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:34,475]: Epoch: 065, Loss:0.1718 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:34,484]: Epoch: 066, Loss:0.1759 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:34,490]: Epoch: 067, Loss:0.1611 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:34,497]: Epoch: 068, Loss:0.1321 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:34,504]: Epoch: 069, Loss:0.1371 Train: 0.9333, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:34,511]: Epoch: 070, Loss:0.1578 Train: 0.9500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:34,518]: Epoch: 071, Loss:0.1517 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:34,525]: Epoch: 072, Loss:0.1531 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:34,533]: Epoch: 073, Loss:0.1262 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:34,539]: Epoch: 074, Loss:0.1169 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:43:34,546]: Epoch: 075, Loss:0.1234 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:34,554]: Epoch: 076, Loss:0.1542 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:34,562]: Epoch: 077, Loss:0.1389 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:34,569]: Epoch: 078, Loss:0.1181 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:34,577]: Epoch: 079, Loss:0.2373 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:34,584]: Epoch: 080, Loss:0.1560 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:34,591]: Epoch: 081, Loss:0.1386 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:34,599]: Epoch: 082, Loss:0.1311 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:34,606]: Epoch: 083, Loss:0.1785 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:34,613]: Epoch: 084, Loss:0.1182 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:34,621]: Epoch: 085, Loss:0.1171 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:34,630]: Epoch: 086, Loss:0.1948 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:34,638]: Epoch: 087, Loss:0.1337 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:34,646]: Epoch: 088, Loss:0.1580 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:34,655]: Epoch: 089, Loss:0.1145 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:34,663]: Epoch: 090, Loss:0.1325 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:34,672]: Epoch: 091, Loss:0.1343 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:34,680]: Epoch: 092, Loss:0.1355 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:34,687]: Epoch: 093, Loss:0.1303 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:34,696]: Epoch: 094, Loss:0.1043 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:34,705]: Epoch: 095, Loss:0.1273 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0092
[2025-04-01 02:43:34,715]: Epoch: 096, Loss:0.1235 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0097
[2025-04-01 02:43:34,724]: Epoch: 097, Loss:0.1291 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:34,733]: Epoch: 098, Loss:0.1247 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:34,741]: Epoch: 099, Loss:0.1405 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:34,750]: Epoch: 100, Loss:0.1290 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:34,760]: Epoch: 101, Loss:0.1352 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0099
[2025-04-01 02:43:34,768]: Epoch: 102, Loss:0.1170 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:34,776]: Epoch: 103, Loss:0.1328 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:34,785]: Epoch: 104, Loss:0.0947 Train: 0.9750, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:34,793]: Epoch: 105, Loss:0.1210 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:34,801]: Epoch: 106, Loss:0.1428 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:34,809]: Epoch: 107, Loss:0.1108 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:34,818]: Epoch: 108, Loss:0.1239 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:34,826]: Epoch: 109, Loss:0.1134 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:34,832]: Epoch: 110, Loss:0.1289 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:34,840]: Epoch: 111, Loss:0.1336 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:34,849]: Epoch: 112, Loss:0.1113 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:34,855]: Epoch: 113, Loss:0.1256 Train: 0.9667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:43:34,863]: Epoch: 114, Loss:0.1309 Train: 0.9667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:34,869]: Epoch: 115, Loss:0.1112 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:34,876]: Epoch: 116, Loss:0.1304 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:34,884]: Epoch: 117, Loss:0.1059 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:34,889]: Epoch: 118, Loss:0.1167 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0054
[2025-04-01 02:43:34,898]: Epoch: 119, Loss:0.1276 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:34,906]: Epoch: 120, Loss:0.1417 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:34,913]: Epoch: 121, Loss:0.1849 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:34,921]: Epoch: 122, Loss:0.1190 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:34,930]: Epoch: 123, Loss:0.1482 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:34,938]: Epoch: 124, Loss:0.2158 Train: 0.9500, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:34,944]: Epoch: 125, Loss:0.1628 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:34,951]: Epoch: 126, Loss:0.1791 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:34,959]: Epoch: 127, Loss:0.1305 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:34,967]: Epoch: 128, Loss:0.1458 Train: 0.9750, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:34,975]: Epoch: 129, Loss:0.1271 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:34,981]: Epoch: 130, Loss:0.1130 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:43:34,989]: Epoch: 131, Loss:0.1272 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:34,997]: Epoch: 132, Loss:0.1494 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:35,004]: Epoch: 133, Loss:0.1435 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:35,012]: Epoch: 134, Loss:0.1831 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:35,019]: Epoch: 135, Loss:0.1152 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:35,027]: Epoch: 136, Loss:0.1030 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:35,034]: Epoch: 137, Loss:0.0943 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:35,040]: Epoch: 138, Loss:0.0974 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0056
[2025-04-01 02:43:35,048]: Epoch: 139, Loss:0.1281 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:35,054]: Epoch: 140, Loss:0.1514 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:35,060]: Epoch: 141, Loss:0.1457 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0059
[2025-04-01 02:43:35,068]: Epoch: 142, Loss:0.1568 Train: 0.9583, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:35,075]: Epoch: 143, Loss:0.0948 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:35,082]: Epoch: 144, Loss:0.1312 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:35,088]: Epoch: 145, Loss:0.1662 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:35,097]: Epoch: 146, Loss:0.2048 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:35,103]: Epoch: 147, Loss:0.1565 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:35,110]: Epoch: 148, Loss:0.1160 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:35,117]: Epoch: 149, Loss:0.1401 Train: 0.9667, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:35,124]: Epoch: 150, Loss:0.1531 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:35,130]: Epoch: 151, Loss:0.1168 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:35,137]: Epoch: 152, Loss:0.1123 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:35,144]: Epoch: 153, Loss:0.1196 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:35,151]: Epoch: 154, Loss:0.1444 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:35,158]: Epoch: 155, Loss:0.1697 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:35,165]: Epoch: 156, Loss:0.1365 Train: 0.9500, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:35,172]: Epoch: 157, Loss:0.1315 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:35,179]: Epoch: 158, Loss:0.1376 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:35,189]: Epoch: 159, Loss:0.1528 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0095
[2025-04-01 02:43:35,197]: Epoch: 160, Loss:0.1438 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:35,205]: Epoch: 161, Loss:0.1696 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:35,213]: Epoch: 162, Loss:0.1269 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:35,221]: Epoch: 163, Loss:0.1137 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:35,228]: Epoch: 164, Loss:0.1210 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:35,235]: Epoch: 165, Loss:0.1179 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:35,242]: Epoch: 166, Loss:0.1084 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:35,250]: Epoch: 167, Loss:0.1074 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:35,257]: Epoch: 168, Loss:0.1217 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:35,264]: Epoch: 169, Loss:0.1118 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:35,272]: Epoch: 170, Loss:0.1448 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:35,278]: Epoch: 171, Loss:0.1157 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:35,287]: Epoch: 172, Loss:0.1313 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:35,295]: Epoch: 173, Loss:0.1403 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:35,303]: Epoch: 174, Loss:0.1300 Train: 0.9417, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:35,309]: Epoch: 175, Loss:0.1505 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0055
[2025-04-01 02:43:35,316]: Epoch: 176, Loss:0.1052 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:35,323]: Epoch: 177, Loss:0.1538 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:35,333]: Epoch: 178, Loss:0.1423 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0097
[2025-04-01 02:43:35,342]: Epoch: 179, Loss:0.1052 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0092
[2025-04-01 02:43:35,351]: Epoch: 180, Loss:0.1182 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:35,360]: Epoch: 181, Loss:0.2121 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0093
[2025-04-01 02:43:35,369]: Epoch: 182, Loss:0.1509 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:35,378]: Epoch: 183, Loss:0.1214 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0093
[2025-04-01 02:43:35,386]: Epoch: 184, Loss:0.1306 Train: 0.9500, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:35,395]: Epoch: 185, Loss:0.1380 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:35,403]: Epoch: 186, Loss:0.1224 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:35,410]: Epoch: 187, Loss:0.1393 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:35,417]: Epoch: 188, Loss:0.1278 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:35,425]: Epoch: 189, Loss:0.1116 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0071
[2025-04-01 02:43:35,431]: Epoch: 190, Loss:0.1307 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:35,439]: Epoch: 191, Loss:0.1008 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:35,447]: Epoch: 192, Loss:0.1313 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:35,455]: Epoch: 193, Loss:0.1764 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:35,461]: Epoch: 194, Loss:0.1038 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0058
[2025-04-01 02:43:35,468]: Epoch: 195, Loss:0.1197 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:35,475]: Epoch: 196, Loss:0.1262 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:35,482]: Epoch: 197, Loss:0.1623 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:35,490]: Epoch: 198, Loss:0.1845 Train: 0.9500, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:35,497]: Epoch: 199, Loss:0.1576 Train: 0.9500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:35,505]: Epoch: 200, Loss:0.1687 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:35,505]: [Run-1 score] {'train': 0.9416666666666667, 'val': 0.725, 'test': 0.6862745098039216}
[2025-04-01 02:43:35,505]: repeat 2/3
[2025-04-01 02:43:35,505]: Manual random seed:0
[2025-04-01 02:43:35,506]: auto fixed data split seed to 0, model init seed to 1

Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0078
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0107
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0099
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0078
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0079
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0075
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0075
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0075
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0080
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0099
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0097
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0093
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0092
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0100
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0096
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0096
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0094
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0094
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0085
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0071
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0073
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0072
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0072
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0103
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0103
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0082
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0094
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0095
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0080
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0074
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0097
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0085
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0074
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0090
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0096
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0083
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0080
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0086
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0081
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0094
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0081
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0078
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0076
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0072
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0079
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0095
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0096
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0079
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0078
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0073
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0074
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0077
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0093
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0097
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0097
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0090
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0077
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0080
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0070
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0071
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0073
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0075
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0101
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0098
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0098
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0099
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0098
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0068
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0073
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0099
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0100
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0094
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0080
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0072
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0072
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0069
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0074
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0074
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0075
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0074
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0084
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0100
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0098
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0098
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0099
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0099
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0095
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0098
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0098
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0099
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0098
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0096
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0079
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0069
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0075
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0071
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:2.1796 Time(s/epoch):0.0062
Epoch: 002, Supervised Loss:2.0827 Time(s/epoch):0.0073
Epoch: 003, Supervised Loss:1.9856 Time(s/epoch):0.0068
Epoch: 004, Supervised Loss:1.9268 Time(s/epoch):0.0068
Epoch: 005, Supervised Loss:1.9133 Time(s/epoch):0.0068
Epoch: 006, Supervised Loss:1.9293 Time(s/epoch):0.0059
Epoch: 007, Supervised Loss:1.9415 Time(s/epoch):0.0050
Epoch: 008, Supervised Loss:1.9394 Time(s/epoch):0.0048
Epoch: 009, Supervised Loss:1.9338 Time(s/epoch):0.0050
Epoch: 010, Supervised Loss:1.9315 Time(s/epoch):0.0050
Epoch: 011, Supervised Loss:1.9292 Time(s/epoch):0.0051
Epoch: 012, Supervised Loss:1.9224 Time(s/epoch):0.0056
Epoch: 013, Supervised Loss:1.9127 Time(s/epoch):0.0064
Epoch: 014, Supervised Loss:1.9051 Time(s/epoch):0.0062
Epoch: 015, Supervised Loss:1.9020 Time(s/epoch):0.0050
Epoch: 016, Supervised Loss:1.9024 Time(s/epoch):0.0049
Epoch: 017, Supervised Loss:1.9039 Time(s/epoch):0.0047
Epoch: 018, Supervised Loss:1.9045 Time(s/epoch):0.0063
Epoch: 019, Supervised Loss:1.9037 Time(s/epoch):0.0065
Epoch: 020, Supervised Loss:1.9022 Time(s/epoch):0.0066
Epoch: 021, Supervised Loss:1.9016 Time(s/epoch):0.0066
Epoch: 022, Supervised Loss:1.9033 Time(s/epoch):0.0063
Epoch: 023, Supervised Loss:1.9075 Time(s/epoch):0.0065
Epoch: 024, Supervised Loss:1.9129 Time(s/epoch):0.0063
Epoch: 025, Supervised Loss:1.9176 Time(s/epoch):0.0066
Epoch: 026, Supervised Loss:1.9204 Time(s/epoch):0.0067
Epoch: 027, Supervised Loss:1.9213 Time(s/epoch):0.0067
Epoch: 028, Supervised Loss:1.9209 Time(s/epoch):0.0067
Epoch: 029, Supervised Loss:1.9195 Time(s/epoch):0.0065
Epoch: 030, Supervised Loss:1.9173 Time(s/epoch):0.0065
0.9296738 515
Add 395 edges.
Prune 370 edges from torch.Size([2, 515]) to torch.Size([2, 145])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.736
Data(x=[251, 1703], edge_index=[2, 535], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.4416 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.736
Data(x=[251, 1703], edge_index=[2, 535], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])[2025-04-01 02:43:35,509]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:35,521]: Epoch: 001, Loss:1.6230 Train: 0.7167, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:35,528]: Epoch: 002, Loss:0.9864 Train: 0.8083, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:35,535]: Epoch: 003, Loss:0.6171 Train: 0.8917, Val:0.7000, Test: 0.7451, Time(s/epoch):0.0064
[2025-04-01 02:43:35,541]: Epoch: 004, Loss:0.5291 Train: 0.8750, Val:0.6750, Test: 0.7451, Time(s/epoch):0.0058
[2025-04-01 02:43:35,547]: Epoch: 005, Loss:0.4533 Train: 0.8917, Val:0.7250, Test: 0.7451, Time(s/epoch):0.0065
[2025-04-01 02:43:35,554]: Epoch: 006, Loss:0.4755 Train: 0.9167, Val:0.7125, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:35,562]: Epoch: 007, Loss:0.4396 Train: 0.9167, Val:0.7125, Test: 0.7255, Time(s/epoch):0.0075
[2025-04-01 02:43:35,568]: Epoch: 008, Loss:0.3037 Train: 0.9000, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0061
[2025-04-01 02:43:35,576]: Epoch: 009, Loss:0.3282 Train: 0.9000, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:35,583]: Epoch: 010, Loss:0.2937 Train: 0.9250, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:35,591]: Epoch: 011, Loss:0.2609 Train: 0.9333, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:35,598]: Epoch: 012, Loss:0.2393 Train: 0.9250, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:35,606]: Epoch: 013, Loss:0.2473 Train: 0.9417, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:35,614]: Epoch: 014, Loss:0.2296 Train: 0.9250, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:35,622]: Epoch: 015, Loss:0.2118 Train: 0.9250, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:35,628]: Epoch: 016, Loss:0.2246 Train: 0.9333, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:35,635]: Epoch: 017, Loss:0.2378 Train: 0.9333, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:35,643]: Epoch: 018, Loss:0.1894 Train: 0.9500, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:35,649]: Epoch: 019, Loss:0.2048 Train: 0.9250, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:35,657]: Epoch: 020, Loss:0.1734 Train: 0.9333, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:35,663]: Epoch: 021, Loss:0.2104 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:35,670]: Epoch: 022, Loss:0.2246 Train: 0.9333, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:35,678]: Epoch: 023, Loss:0.1959 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:35,686]: Epoch: 024, Loss:0.1566 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:35,694]: Epoch: 025, Loss:0.1574 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:35,700]: Epoch: 026, Loss:0.1548 Train: 0.9583, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:35,707]: Epoch: 027, Loss:0.1690 Train: 0.9583, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:35,715]: Epoch: 028, Loss:0.1516 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:35,721]: Epoch: 029, Loss:0.1576 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:35,728]: Epoch: 030, Loss:0.1338 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:35,737]: Epoch: 031, Loss:0.1334 Train: 0.9417, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:35,746]: Epoch: 032, Loss:0.1435 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:35,755]: Epoch: 033, Loss:0.1852 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:35,763]: Epoch: 034, Loss:0.1528 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:35,770]: Epoch: 035, Loss:0.1889 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:35,777]: Epoch: 036, Loss:0.1526 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:35,785]: Epoch: 037, Loss:0.1420 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:35,794]: Epoch: 038, Loss:0.1291 Train: 0.9500, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:35,801]: Epoch: 039, Loss:0.1704 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:35,809]: Epoch: 040, Loss:0.1418 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:35,816]: Epoch: 041, Loss:0.1237 Train: 0.9583, Val:0.6750, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:35,825]: Epoch: 042, Loss:0.1430 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:35,833]: Epoch: 043, Loss:0.1584 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:35,840]: Epoch: 044, Loss:0.1628 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:35,847]: Epoch: 045, Loss:0.1528 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:35,854]: Epoch: 046, Loss:0.1578 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:35,862]: Epoch: 047, Loss:0.1243 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:35,870]: Epoch: 048, Loss:0.1754 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:35,878]: Epoch: 049, Loss:0.1441 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:35,885]: Epoch: 050, Loss:0.1584 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:35,893]: Epoch: 051, Loss:0.1607 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:35,899]: Epoch: 052, Loss:0.1387 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:35,906]: Epoch: 053, Loss:0.1295 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:35,914]: Epoch: 054, Loss:0.1427 Train: 0.9417, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:35,921]: Epoch: 055, Loss:0.1478 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:35,928]: Epoch: 056, Loss:0.1377 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:35,936]: Epoch: 057, Loss:0.1612 Train: 0.9333, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:35,945]: Epoch: 058, Loss:0.1717 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:35,952]: Epoch: 059, Loss:0.1735 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:35,959]: Epoch: 060, Loss:0.1271 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:35,965]: Epoch: 061, Loss:0.1493 Train: 0.9583, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:35,972]: Epoch: 062, Loss:0.1549 Train: 0.9500, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:35,979]: Epoch: 063, Loss:0.1575 Train: 0.9583, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:35,986]: Epoch: 064, Loss:0.1382 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:35,992]: Epoch: 065, Loss:0.1314 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:36,001]: Epoch: 066, Loss:0.1438 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:36,009]: Epoch: 067, Loss:0.1395 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:36,017]: Epoch: 068, Loss:0.1394 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:36,024]: Epoch: 069, Loss:0.1178 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:36,031]: Epoch: 070, Loss:0.1255 Train: 0.9583, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:36,038]: Epoch: 071, Loss:0.1408 Train: 0.9583, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:36,045]: Epoch: 072, Loss:0.1345 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:36,052]: Epoch: 073, Loss:0.1488 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:36,060]: Epoch: 074, Loss:0.1227 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:36,067]: Epoch: 075, Loss:0.1144 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:36,075]: Epoch: 076, Loss:0.1217 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:36,083]: Epoch: 077, Loss:0.1441 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:36,090]: Epoch: 078, Loss:0.1423 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:36,098]: Epoch: 079, Loss:0.1354 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:36,105]: Epoch: 080, Loss:0.1195 Train: 0.9500, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:36,113]: Epoch: 081, Loss:0.1403 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:36,119]: Epoch: 082, Loss:0.1358 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:43:36,127]: Epoch: 083, Loss:0.1331 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:36,135]: Epoch: 084, Loss:0.1576 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:36,141]: Epoch: 085, Loss:0.1480 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:36,149]: Epoch: 086, Loss:0.1124 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:36,157]: Epoch: 087, Loss:0.1142 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:36,165]: Epoch: 088, Loss:0.1256 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:36,171]: Epoch: 089, Loss:0.1491 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0058
[2025-04-01 02:43:36,178]: Epoch: 090, Loss:0.1377 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:36,188]: Epoch: 091, Loss:0.1160 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0092
[2025-04-01 02:43:36,196]: Epoch: 092, Loss:0.1303 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:36,203]: Epoch: 093, Loss:0.1446 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:36,211]: Epoch: 094, Loss:0.1113 Train: 0.9833, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:36,219]: Epoch: 095, Loss:0.1148 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:36,226]: Epoch: 096, Loss:0.1266 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:36,233]: Epoch: 097, Loss:0.1157 Train: 0.9500, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:36,240]: Epoch: 098, Loss:0.1335 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:36,249]: Epoch: 099, Loss:0.1610 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:36,257]: Epoch: 100, Loss:0.1761 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:36,263]: Epoch: 101, Loss:0.1531 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:36,271]: Epoch: 102, Loss:0.1107 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:36,280]: Epoch: 103, Loss:0.1330 Train: 0.9500, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:36,286]: Epoch: 104, Loss:0.1460 Train: 0.9417, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:36,293]: Epoch: 105, Loss:0.1353 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:36,300]: Epoch: 106, Loss:0.1360 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:36,308]: Epoch: 107, Loss:0.1439 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:36,316]: Epoch: 108, Loss:0.1209 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:36,324]: Epoch: 109, Loss:0.1309 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:36,331]: Epoch: 110, Loss:0.1336 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:36,340]: Epoch: 111, Loss:0.2302 Train: 0.9500, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:36,347]: Epoch: 112, Loss:0.1537 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:36,353]: Epoch: 113, Loss:0.1382 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:36,360]: Epoch: 114, Loss:0.1417 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:36,369]: Epoch: 115, Loss:0.1464 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:36,377]: Epoch: 116, Loss:0.1970 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:36,384]: Epoch: 117, Loss:0.2021 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:36,390]: Epoch: 118, Loss:0.1442 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:36,397]: Epoch: 119, Loss:0.1702 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:36,404]: Epoch: 120, Loss:0.1391 Train: 0.9333, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:36,411]: Epoch: 121, Loss:0.1464 Train: 0.9333, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:36,419]: Epoch: 122, Loss:0.1912 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:36,426]: Epoch: 123, Loss:0.1746 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:36,433]: Epoch: 124, Loss:0.1495 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:36,440]: Epoch: 125, Loss:0.1582 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:36,448]: Epoch: 126, Loss:0.1371 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:36,455]: Epoch: 127, Loss:0.1288 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:36,461]: Epoch: 128, Loss:0.1126 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:36,468]: Epoch: 129, Loss:0.1101 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:36,476]: Epoch: 130, Loss:0.1251 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:36,484]: Epoch: 131, Loss:0.1068 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:36,489]: Epoch: 132, Loss:0.1668 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0055
[2025-04-01 02:43:36,497]: Epoch: 133, Loss:0.1226 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:36,505]: Epoch: 134, Loss:0.1336 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:36,511]: Epoch: 135, Loss:0.1187 Train: 0.9750, Val:0.7375, Test: 0.6471, Time(s/epoch):0.0058
[2025-04-01 02:43:36,518]: Epoch: 136, Loss:0.1310 Train: 0.9833, Val:0.7250, Test: 0.6471, Time(s/epoch):0.0072
[2025-04-01 02:43:36,527]: Epoch: 137, Loss:0.1213 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:36,535]: Epoch: 138, Loss:0.1022 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:36,543]: Epoch: 139, Loss:0.1270 Train: 0.9583, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:36,552]: Epoch: 140, Loss:0.1402 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:36,559]: Epoch: 141, Loss:0.1293 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:36,568]: Epoch: 142, Loss:0.1251 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:36,574]: Epoch: 143, Loss:0.1127 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:36,581]: Epoch: 144, Loss:0.1029 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:36,589]: Epoch: 145, Loss:0.1062 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:36,596]: Epoch: 146, Loss:0.1193 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:36,604]: Epoch: 147, Loss:0.1256 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:36,613]: Epoch: 148, Loss:0.1157 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:36,619]: Epoch: 149, Loss:0.1266 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0057
[2025-04-01 02:43:36,627]: Epoch: 150, Loss:0.1318 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:36,635]: Epoch: 151, Loss:0.1507 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:36,641]: Epoch: 152, Loss:0.1253 Train: 0.9667, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:36,649]: Epoch: 153, Loss:0.1343 Train: 0.9500, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:36,657]: Epoch: 154, Loss:0.1297 Train: 0.9833, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:36,664]: Epoch: 155, Loss:0.1080 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:36,670]: Epoch: 156, Loss:0.1291 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:43:36,679]: Epoch: 157, Loss:0.1696 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:36,686]: Epoch: 158, Loss:0.1330 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:36,693]: Epoch: 159, Loss:0.1288 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:36,701]: Epoch: 160, Loss:0.1322 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:36,709]: Epoch: 161, Loss:0.1387 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:36,715]: Epoch: 162, Loss:0.1608 Train: 0.9583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:36,722]: Epoch: 163, Loss:0.1747 Train: 0.9667, Val:0.7000, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:36,730]: Epoch: 164, Loss:0.1393 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:36,737]: Epoch: 165, Loss:0.1327 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:36,745]: Epoch: 166, Loss:0.1405 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:36,752]: Epoch: 167, Loss:0.1989 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:36,758]: Epoch: 168, Loss:0.1457 Train: 0.9750, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:43:36,765]: Epoch: 169, Loss:0.1190 Train: 0.9750, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:36,774]: Epoch: 170, Loss:0.1347 Train: 0.9667, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:36,781]: Epoch: 171, Loss:0.1510 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:36,790]: Epoch: 172, Loss:0.1577 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:36,798]: Epoch: 173, Loss:0.1378 Train: 0.9667, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:36,806]: Epoch: 174, Loss:0.1356 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:36,814]: Epoch: 175, Loss:0.1298 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:36,821]: Epoch: 176, Loss:0.1318 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:36,830]: Epoch: 177, Loss:0.1400 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0095
[2025-04-01 02:43:36,839]: Epoch: 178, Loss:0.1130 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:36,848]: Epoch: 179, Loss:0.1326 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:36,856]: Epoch: 180, Loss:0.1142 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:36,865]: Epoch: 181, Loss:0.1022 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:36,871]: Epoch: 182, Loss:0.1408 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:36,879]: Epoch: 183, Loss:0.1149 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:36,887]: Epoch: 184, Loss:0.1267 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:36,896]: Epoch: 185, Loss:0.1354 Train: 0.9500, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:36,905]: Epoch: 186, Loss:0.1694 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:36,911]: Epoch: 187, Loss:0.1504 Train: 0.9417, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:36,918]: Epoch: 188, Loss:0.1394 Train: 0.9583, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:36,925]: Epoch: 189, Loss:0.2018 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:36,933]: Epoch: 190, Loss:0.1095 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:36,939]: Epoch: 191, Loss:0.1743 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:36,946]: Epoch: 192, Loss:0.1928 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:36,953]: Epoch: 193, Loss:0.1195 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0064
[2025-04-01 02:43:36,961]: Epoch: 194, Loss:0.1056 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:36,967]: Epoch: 195, Loss:0.1534 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:36,975]: Epoch: 196, Loss:0.1358 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:36,982]: Epoch: 197, Loss:0.1423 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:36,990]: Epoch: 198, Loss:0.1514 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:36,998]: Epoch: 199, Loss:0.1163 Train: 0.9667, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:37,005]: Epoch: 200, Loss:0.1184 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:37,006]: [Run-2 score] {'train': 0.975, 'val': 0.7375, 'test': 0.6470588235294118}
[2025-04-01 02:43:37,006]: repeat 3/3
[2025-04-01 02:43:37,006]: Manual random seed:0
[2025-04-01 02:43:37,006]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:37,009]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:37,019]: Epoch: 001, Loss:1.6849 Train: 0.7500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:37,026]: Epoch: 002, Loss:1.0644 Train: 0.8083, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:37,033]: Epoch: 003, Loss:0.7995 Train: 0.8417, Val:0.6875, Test: 0.7451, Time(s/epoch):0.0072
[2025-04-01 02:43:37,039]: Epoch: 004, Loss:0.5214 Train: 0.8417, Val:0.7000, Test: 0.7647, Time(s/epoch):0.0061
[2025-04-01 02:43:37,047]: Epoch: 005, Loss:0.4823 Train: 0.8583, Val:0.6875, Test: 0.7451, Time(s/epoch):0.0077
[2025-04-01 02:43:37,054]: Epoch: 006, Loss:0.4046 Train: 0.8917, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:37,061]: Epoch: 007, Loss:0.4071 Train: 0.9000, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:37,068]: Epoch: 008, Loss:0.3495 Train: 0.9000, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:37,075]: Epoch: 009, Loss:0.3239 Train: 0.9333, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:37,083]: Epoch: 010, Loss:0.3035 Train: 0.9250, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:37,090]: Epoch: 011, Loss:0.2442 Train: 0.9167, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:37,098]: Epoch: 012, Loss:0.2947 Train: 0.9250, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:37,105]: Epoch: 013, Loss:0.2319 Train: 0.9167, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:37,112]: Epoch: 014, Loss:0.3162 Train: 0.9333, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:37,119]: Epoch: 015, Loss:0.2658 Train: 0.9333, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:37,128]: Epoch: 016, Loss:0.2818 Train: 0.9250, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:37,135]: Epoch: 017, Loss:0.2378 Train: 0.9333, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:37,142]: Epoch: 018, Loss:0.2527 Train: 0.9333, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:37,151]: Epoch: 019, Loss:0.2182 Train: 0.9250, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:37,158]: Epoch: 020, Loss:0.2379 Train: 0.9417, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:37,165]: Epoch: 021, Loss:0.1928 Train: 0.9417, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:37,171]: Epoch: 022, Loss:0.1810 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:37,180]: Epoch: 023, Loss:0.1767 Train: 0.9250, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:37,189]: Epoch: 024, Loss:0.2067 Train: 0.9333, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:37,198]: Epoch: 025, Loss:0.1980 Train: 0.9500, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:37,206]: Epoch: 026, Loss:0.2287 Train: 0.9500, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:37,214]: Epoch: 027, Loss:0.1928 Train: 0.9500, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:37,222]: Epoch: 028, Loss:0.1627 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:37,230]: Epoch: 029, Loss:0.1537 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:37,238]: Epoch: 030, Loss:0.1941 Train: 0.9250, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:37,244]: Epoch: 031, Loss:0.2495 Train: 0.9333, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:37,250]: Epoch: 032, Loss:0.1878 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:37,259]: Epoch: 033, Loss:0.1969 Train: 0.9500, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:37,268]: Epoch: 034, Loss:0.1918 Train: 0.9417, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:37,275]: Epoch: 035, Loss:0.2135 Train: 0.9500, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:37,284]: Epoch: 036, Loss:0.1449 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:37,291]: Epoch: 037, Loss:0.1676 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:37,298]: Epoch: 038, Loss:0.1575 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:37,306]: Epoch: 039, Loss:0.1824 Train: 0.9417, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:37,315]: Epoch: 040, Loss:0.1618 Train: 0.9417, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:37,321]: Epoch: 041, Loss:0.2096 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:37,329]: Epoch: 042, Loss:0.1234 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:37,337]: Epoch: 043, Loss:0.1714 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:37,345]: Epoch: 044, Loss:0.1699 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:37,352]: Epoch: 045, Loss:0.1836 Train: 0.9833, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:37,360]: Epoch: 046, Loss:0.1692 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:37,368]: Epoch: 047, Loss:0.1814 Train: 0.9500, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:37,375]: Epoch: 048, Loss:0.1577 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:37,383]: Epoch: 049, Loss:0.1588 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:37,391]: Epoch: 050, Loss:0.1766 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:37,399]: Epoch: 051, Loss:0.1642 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:37,405]: Epoch: 052, Loss:0.1531 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:37,414]: Epoch: 053, Loss:0.1429 Train: 0.9833, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:37,422]: Epoch: 054, Loss:0.1594 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:37,430]: Epoch: 055, Loss:0.1602 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:37,437]: Epoch: 056, Loss:0.1518 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:37,446]: Epoch: 057, Loss:0.1413 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:37,452]: Epoch: 058, Loss:0.1418 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:37,461]: Epoch: 059, Loss:0.1718 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:37,469]: Epoch: 060, Loss:0.1747 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:37,476]: Epoch: 061, Loss:0.1405 Train: 0.9583, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:37,485]: Epoch: 062, Loss:0.1561 Train: 0.9500, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0090
[2025-04-01 02:43:37,494]: Epoch: 063, Loss:0.1755 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:37,501]: Epoch: 064, Loss:0.1258 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:37,508]: Epoch: 065, Loss:0.1233 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:37,516]: Epoch: 066, Loss:0.1342 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:37,524]: Epoch: 067, Loss:0.1298 Train: 0.9917, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:37,529]: Epoch: 068, Loss:0.1780 Train: 0.9833, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0053
[2025-04-01 02:43:37,537]: Epoch: 069, Loss:0.1327 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:37,543]: Epoch: 070, Loss:0.1358 Train: 0.9583, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:37,551]: Epoch: 071, Loss:0.1789 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:37,559]: Epoch: 072, Loss:0.1330 Train: 0.9500, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:37,567]: Epoch: 073, Loss:0.1509 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:37,574]: Epoch: 074, Loss:0.1439 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:37,581]: Epoch: 075, Loss:0.1344 Train: 0.9667, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:37,589]: Epoch: 076, Loss:0.1255 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:37,597]: Epoch: 077, Loss:0.1275 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:37,603]: Epoch: 078, Loss:0.1330 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:37,611]: Epoch: 079, Loss:0.1510 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:37,618]: Epoch: 080, Loss:0.1445 Train: 0.9583, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:37,626]: Epoch: 081, Loss:0.1258 Train: 0.9583, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:37,635]: Epoch: 082, Loss:0.1720 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:37,643]: Epoch: 083, Loss:0.1326 Train: 0.9500, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:37,651]: Epoch: 084, Loss:0.1684 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:37,657]: Epoch: 085, Loss:0.1313 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:37,665]: Epoch: 086, Loss:0.1635 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:37,674]: Epoch: 087, Loss:0.1310 Train: 0.9583, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:37,681]: Epoch: 088, Loss:0.1925 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:37,688]: Epoch: 089, Loss:0.1439 Train: 0.9583, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:37,694]: Epoch: 090, Loss:0.1314 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:37,701]: Epoch: 091, Loss:0.1583 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:37,709]: Epoch: 092, Loss:0.1885 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:37,717]: Epoch: 093, Loss:0.1218 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:37,724]: Epoch: 094, Loss:0.1461 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:37,731]: Epoch: 095, Loss:0.1387 Train: 0.9667, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:37,739]: Epoch: 096, Loss:0.1613 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:37,748]: Epoch: 097, Loss:0.1218 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:37,756]: Epoch: 098, Loss:0.1358 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:37,764]: Epoch: 099, Loss:0.1229 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:37,771]: Epoch: 100, Loss:0.1328 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:37,779]: Epoch: 101, Loss:0.1151 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:37,788]: Epoch: 102, Loss:0.1335 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0088
[2025-04-01 02:43:37,796]: Epoch: 103, Loss:0.1183 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:37,805]: Epoch: 104, Loss:0.1535 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:37,813]: Epoch: 105, Loss:0.1693 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:37,821]: Epoch: 106, Loss:0.1299 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:37,827]: Epoch: 107, Loss:0.1405 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:37,835]: Epoch: 108, Loss:0.1456 Train: 0.9667, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:37,841]: Epoch: 109, Loss:0.1913 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:37,850]: Epoch: 110, Loss:0.1471 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:37,857]: Epoch: 111, Loss:0.1677 Train: 0.9833, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:37,864]: Epoch: 112, Loss:0.1200 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:37,871]: Epoch: 113, Loss:0.1619 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:37,879]: Epoch: 114, Loss:0.1202 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:37,885]: Epoch: 115, Loss:0.1782 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:43:37,892]: Epoch: 116, Loss:0.1488 Train: 0.9500, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:43:37,900]: Epoch: 117, Loss:0.1516 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:37,907]: Epoch: 118, Loss:0.1337 Train: 0.9750, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:37,915]: Epoch: 119, Loss:0.1345 Train: 0.9750, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:37,922]: Epoch: 120, Loss:0.1421 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:37,929]: Epoch: 121, Loss:0.1379 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:37,936]: Epoch: 122, Loss:0.1334 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:37,943]: Epoch: 123, Loss:0.1261 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:37,951]: Epoch: 124, Loss:0.1119 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:37,958]: Epoch: 125, Loss:0.1135 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:37,966]: Epoch: 126, Loss:0.1099 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:37,972]: Epoch: 127, Loss:0.1120 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:37,978]: Epoch: 128, Loss:0.1262 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:37,986]: Epoch: 129, Loss:0.1262 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:37,994]: Epoch: 130, Loss:0.1322 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:38,000]: Epoch: 131, Loss:0.1360 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:38,009]: Epoch: 132, Loss:0.1179 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:38,017]: Epoch: 133, Loss:0.1317 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:38,024]: Epoch: 134, Loss:0.1689 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:38,032]: Epoch: 135, Loss:0.1070 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,039]: Epoch: 136, Loss:0.1024 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:38,046]: Epoch: 137, Loss:0.1895 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:38,054]: Epoch: 138, Loss:0.1781 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:38,060]: Epoch: 139, Loss:0.1061 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:38,069]: Epoch: 140, Loss:0.1422 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:38,077]: Epoch: 141, Loss:0.1545 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:38,084]: Epoch: 142, Loss:0.1273 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:38,091]: Epoch: 143, Loss:0.1182 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:38,099]: Epoch: 144, Loss:0.1262 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:38,107]: Epoch: 145, Loss:0.1032 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:38,114]: Epoch: 146, Loss:0.1110 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:38,122]: Epoch: 147, Loss:0.1194 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:38,128]: Epoch: 148, Loss:0.1155 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:38,135]: Epoch: 149, Loss:0.1493 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:38,143]: Epoch: 150, Loss:0.1213 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:38,149]: Epoch: 151, Loss:0.1065 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:43:38,157]: Epoch: 152, Loss:0.1196 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,164]: Epoch: 153, Loss:0.1341 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:38,172]: Epoch: 154, Loss:0.1371 Train: 0.9500, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:38,181]: Epoch: 155, Loss:0.1280 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,188]: Epoch: 156, Loss:0.1410 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,196]: Epoch: 157, Loss:0.1137 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:38,206]: Epoch: 158, Loss:0.1100 Train: 0.9667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0095
[2025-04-01 02:43:38,215]: Epoch: 159, Loss:0.1580 Train: 0.9500, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:38,222]: Epoch: 160, Loss:0.1305 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:38,230]: Epoch: 161, Loss:0.1572 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:38,238]: Epoch: 162, Loss:0.1216 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:38,246]: Epoch: 163, Loss:0.1877 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,254]: Epoch: 164, Loss:0.1098 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:38,261]: Epoch: 165, Loss:0.1209 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:38,268]: Epoch: 166, Loss:0.1258 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:38,277]: Epoch: 167, Loss:0.1025 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:38,285]: Epoch: 168, Loss:0.1379 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:38,292]: Epoch: 169, Loss:0.1213 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:38,299]: Epoch: 170, Loss:0.1213 Train: 0.9667, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:38,307]: Epoch: 171, Loss:0.1591 Train: 0.9750, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:38,316]: Epoch: 172, Loss:0.1203 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:38,324]: Epoch: 173, Loss:0.1252 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:38,331]: Epoch: 174, Loss:0.0999 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:38,340]: Epoch: 175, Loss:0.1164 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:38,347]: Epoch: 176, Loss:0.1080 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:38,354]: Epoch: 177, Loss:0.1106 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:38,361]: Epoch: 178, Loss:0.1318 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:38,370]: Epoch: 179, Loss:0.0904 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:38,378]: Epoch: 180, Loss:0.1406 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:38,386]: Epoch: 181, Loss:0.1286 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:38,394]: Epoch: 182, Loss:0.1191 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:38,400]: Epoch: 183, Loss:0.1213 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:38,409]: Epoch: 184, Loss:0.1140 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:38,417]: Epoch: 185, Loss:0.1373 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:38,426]: Epoch: 186, Loss:0.1037 Train: 0.9583, Val:0.7125, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:38,434]: Epoch: 187, Loss:0.1157 Train: 0.9583, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:38,442]: Epoch: 188, Loss:0.1768 Train: 0.9667, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:38,450]: Epoch: 189, Loss:0.1419 Train: 0.9667, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:38,459]: Epoch: 190, Loss:0.1304 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0088
[2025-04-01 02:43:38,468]: Epoch: 191, Loss:0.1186 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:38,475]: Epoch: 192, Loss:0.1155 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:38,481]: Epoch: 193, Loss:0.1373 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:38,488]: Epoch: 194, Loss:0.1648 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:38,497]: Epoch: 195, Loss:0.1246 Train: 0.9667, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:38,505]: Epoch: 196, Loss:0.1521 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:38,513]: Epoch: 197, Loss:0.1251 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:38,519]: Epoch: 198, Loss:0.0988 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:38,526]: Epoch: 199, Loss:0.1145 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:38,533]: Epoch: 200, Loss:0.1261 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:38,534]: [Run-3 score] {'train': 0.9333333333333333, 'val': 0.7125, 'test': 0.6862745098039216}
[2025-04-01 02:43:38,534]: repeat 1/3
[2025-04-01 02:43:38,534]: Manual random seed:0
[2025-04-01 02:43:38,534]: auto fixed data split seed to 0, model init seed to 0

GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4974 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.736
Data(x=[251, 1703], edge_index=[2, 535], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5255 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(21)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0142
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0082
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0102
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0078
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0095
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0087
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0074
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0078
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0074
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0107
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0120
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0097
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0079
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0071
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0074
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0072
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0075
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0075
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0074
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0069
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0074
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0095
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0096
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0094
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0079
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0072
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0074
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0072
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0080
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0073
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0085
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0097
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0096
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0095
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0070
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0075
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0099
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0093
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0078
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0080
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0077
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0081
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0081
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0082
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0085
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0096
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0101
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0102
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0098
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0085
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0081
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0075
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0076
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0074
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0091
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0092
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0074
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0074
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0072
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0080
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0103
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0117
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0107
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0073
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0083
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0097
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0082
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0072
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0080
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0094
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0080
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0073
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0075
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0074
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0092
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0098
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0100
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0101
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0098
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0077
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0082
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0073
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0072
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0098
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0096
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0097
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0096
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0074
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0077
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0073
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0072
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0070
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0089
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0100
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0075
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0078
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0084
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0083
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0095
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0104
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0098
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0096
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0097
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0098
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0080
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0072
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0075
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0073
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0072
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0069
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0085
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0094
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0097
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0090
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0071
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0076
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0074
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0075
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0080
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0082
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0076
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0073
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0101
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0095[2025-04-01 02:43:40,437]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:40,448]: Epoch: 001, Loss:1.7700 Train: 0.6333, Val:0.5625, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:40,456]: Epoch: 002, Loss:1.3092 Train: 0.7500, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:40,463]: Epoch: 003, Loss:0.8956 Train: 0.7750, Val:0.7625, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:43:40,469]: Epoch: 004, Loss:0.7600 Train: 0.8000, Val:0.7500, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:43:40,476]: Epoch: 005, Loss:0.7782 Train: 0.8500, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:40,484]: Epoch: 006, Loss:0.6709 Train: 0.8583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0071
[2025-04-01 02:43:40,490]: Epoch: 007, Loss:0.5527 Train: 0.8500, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:40,498]: Epoch: 008, Loss:0.5263 Train: 0.8500, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:40,506]: Epoch: 009, Loss:0.5393 Train: 0.8500, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:40,514]: Epoch: 010, Loss:0.4746 Train: 0.8833, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:40,522]: Epoch: 011, Loss:0.4436 Train: 0.8750, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:43:40,529]: Epoch: 012, Loss:0.4344 Train: 0.8833, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:40,535]: Epoch: 013, Loss:0.3928 Train: 0.8750, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:43:40,542]: Epoch: 014, Loss:0.4512 Train: 0.8750, Val:0.6375, Test: 0.5686, Time(s/epoch):0.0059
[2025-04-01 02:43:40,548]: Epoch: 015, Loss:0.4326 Train: 0.9083, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:40,555]: Epoch: 016, Loss:0.3494 Train: 0.8917, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:40,561]: Epoch: 017, Loss:0.4368 Train: 0.8917, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:43:40,569]: Epoch: 018, Loss:0.3606 Train: 0.9000, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:40,578]: Epoch: 019, Loss:0.3711 Train: 0.9000, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:43:40,584]: Epoch: 020, Loss:0.3059 Train: 0.9167, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:43:40,590]: Epoch: 021, Loss:0.3153 Train: 0.9083, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:43:40,598]: Epoch: 022, Loss:0.3309 Train: 0.9167, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:40,605]: Epoch: 023, Loss:0.3131 Train: 0.9083, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:40,614]: Epoch: 024, Loss:0.3330 Train: 0.9083, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:40,621]: Epoch: 025, Loss:0.3028 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0072
[2025-04-01 02:43:40,628]: Epoch: 026, Loss:0.2849 Train: 0.9167, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:40,636]: Epoch: 027, Loss:0.3200 Train: 0.9083, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:40,644]: Epoch: 028, Loss:0.3121 Train: 0.9250, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:40,651]: Epoch: 029, Loss:0.2910 Train: 0.9250, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:40,658]: Epoch: 030, Loss:0.3007 Train: 0.9167, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:43:40,666]: Epoch: 031, Loss:0.3206 Train: 0.9250, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:43:40,673]: Epoch: 032, Loss:0.2840 Train: 0.9333, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:43:40,680]: Epoch: 033, Loss:0.2811 Train: 0.9333, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:43:40,688]: Epoch: 034, Loss:0.2786 Train: 0.9167, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:40,695]: Epoch: 035, Loss:0.2844 Train: 0.9083, Val:0.6375, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:40,704]: Epoch: 036, Loss:0.2844 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0091
[2025-04-01 02:43:40,712]: Epoch: 037, Loss:0.2605 Train: 0.9250, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:43:40,719]: Epoch: 038, Loss:0.2519 Train: 0.9167, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:43:40,727]: Epoch: 039, Loss:0.2658 Train: 0.9250, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:43:40,735]: Epoch: 040, Loss:0.3199 Train: 0.9333, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:40,743]: Epoch: 041, Loss:0.2814 Train: 0.9250, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:40,750]: Epoch: 042, Loss:0.2513 Train: 0.9250, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:40,757]: Epoch: 043, Loss:0.2688 Train: 0.9167, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:40,766]: Epoch: 044, Loss:0.3240 Train: 0.9333, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0086
[2025-04-01 02:43:40,773]: Epoch: 045, Loss:0.2797 Train: 0.9500, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:40,779]: Epoch: 046, Loss:0.2561 Train: 0.9583, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:43:40,787]: Epoch: 047, Loss:0.2552 Train: 0.9333, Val:0.6625, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:43:40,795]: Epoch: 048, Loss:0.2578 Train: 0.9250, Val:0.6625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:43:40,801]: Epoch: 049, Loss:0.2843 Train: 0.9417, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:43:40,808]: Epoch: 050, Loss:0.2888 Train: 0.9333, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:40,816]: Epoch: 051, Loss:0.3019 Train: 0.9250, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:40,824]: Epoch: 052, Loss:0.2508 Train: 0.9000, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:40,830]: Epoch: 053, Loss:0.2199 Train: 0.9000, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:43:40,838]: Epoch: 054, Loss:0.2633 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:40,845]: Epoch: 055, Loss:0.2506 Train: 0.9333, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:43:40,853]: Epoch: 056, Loss:0.2530 Train: 0.9583, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:43:40,869]: Epoch: 057, Loss:0.2308 Train: 0.9500, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0153
[2025-04-01 02:43:40,886]: Epoch: 058, Loss:0.2493 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0169
[2025-04-01 02:43:40,898]: Epoch: 059, Loss:0.2473 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0123
[2025-04-01 02:43:40,906]: Epoch: 060, Loss:0.2599 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:43:40,918]: Epoch: 061, Loss:0.2106 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0117
[2025-04-01 02:43:40,927]: Epoch: 062, Loss:0.2667 Train: 0.9250, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:43:40,935]: Epoch: 063, Loss:0.2490 Train: 0.9167, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:40,943]: Epoch: 064, Loss:0.2505 Train: 0.9167, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0085
[2025-04-01 02:43:40,952]: Epoch: 065, Loss:0.2704 Train: 0.9250, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:43:40,961]: Epoch: 066, Loss:0.2197 Train: 0.9333, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0093
[2025-04-01 02:43:40,971]: Epoch: 067, Loss:0.2184 Train: 0.9333, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0095
[2025-04-01 02:43:40,979]: Epoch: 068, Loss:0.2823 Train: 0.9417, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:40,988]: Epoch: 069, Loss:0.2547 Train: 0.9500, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0093
[2025-04-01 02:43:40,998]: Epoch: 070, Loss:0.2239 Train: 0.9333, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0095
[2025-04-01 02:43:41,006]: Epoch: 071, Loss:0.2461 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:41,015]: Epoch: 072, Loss:0.2263 Train: 0.9167, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0086
[2025-04-01 02:43:41,024]: Epoch: 073, Loss:0.2407 Train: 0.9250, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0091
[2025-04-01 02:43:41,033]: Epoch: 074, Loss:0.2626 Train: 0.9167, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0091
[2025-04-01 02:43:41,042]: Epoch: 075, Loss:0.2382 Train: 0.9083, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:43:41,051]: Epoch: 076, Loss:0.2795 Train: 0.9083, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:41,059]: Epoch: 077, Loss:0.2412 Train: 0.9333, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:43:41,067]: Epoch: 078, Loss:0.2693 Train: 0.9333, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:41,074]: Epoch: 079, Loss:0.2838 Train: 0.9167, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:41,081]: Epoch: 080, Loss:0.2911 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:43:41,089]: Epoch: 081, Loss:0.2477 Train: 0.9333, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:41,097]: Epoch: 082, Loss:0.2256 Train: 0.9000, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:41,105]: Epoch: 083, Loss:0.2714 Train: 0.9250, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:41,113]: Epoch: 084, Loss:0.2328 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:41,119]: Epoch: 085, Loss:0.2446 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0056
[2025-04-01 02:43:41,126]: Epoch: 086, Loss:0.2560 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:43:41,134]: Epoch: 087, Loss:0.2511 Train: 0.9333, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:41,141]: Epoch: 088, Loss:0.2446 Train: 0.9333, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:41,150]: Epoch: 089, Loss:0.2751 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:41,157]: Epoch: 090, Loss:0.2538 Train: 0.9167, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:43:41,164]: Epoch: 091, Loss:0.2333 Train: 0.9250, Val:0.7000, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:43:41,172]: Epoch: 092, Loss:0.3155 Train: 0.9250, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:43:41,180]: Epoch: 093, Loss:0.2340 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:43:41,186]: Epoch: 094, Loss:0.2186 Train: 0.9333, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:41,194]: Epoch: 095, Loss:0.2571 Train: 0.9333, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:41,203]: Epoch: 096, Loss:0.1809 Train: 0.9500, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0091
[2025-04-01 02:43:41,213]: Epoch: 097, Loss:0.2074 Train: 0.9500, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:43:41,219]: Epoch: 098, Loss:0.1938 Train: 0.9500, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:43:41,227]: Epoch: 099, Loss:0.2052 Train: 0.9500, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:43:41,235]: Epoch: 100, Loss:0.2027 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:41,242]: Epoch: 101, Loss:0.2061 Train: 0.9250, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:41,249]: Epoch: 102, Loss:0.1985 Train: 0.9083, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:43:41,257]: Epoch: 103, Loss:0.2516 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:41,263]: Epoch: 104, Loss:0.2662 Train: 0.9417, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:41,270]: Epoch: 105, Loss:0.2408 Train: 0.9500, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:41,279]: Epoch: 106, Loss:0.2270 Train: 0.9167, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:43:41,288]: Epoch: 107, Loss:0.2710 Train: 0.9500, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:41,294]: Epoch: 108, Loss:0.2423 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:43:41,303]: Epoch: 109, Loss:0.1671 Train: 0.9500, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0090
[2025-04-01 02:43:41,310]: Epoch: 110, Loss:0.2192 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:43:41,318]: Epoch: 111, Loss:0.2249 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:41,325]: Epoch: 112, Loss:0.2561 Train: 0.9583, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:43:41,332]: Epoch: 113, Loss:0.2460 Train: 0.9500, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:43:41,340]: Epoch: 114, Loss:0.2413 Train: 0.9500, Val:0.6875, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:43:41,347]: Epoch: 115, Loss:0.2686 Train: 0.9333, Val:0.6125, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:41,355]: Epoch: 116, Loss:0.2384 Train: 0.9500, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:43:41,362]: Epoch: 117, Loss:0.2363 Train: 0.9417, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:41,369]: Epoch: 118, Loss:0.2027 Train: 0.9333, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:43:41,377]: Epoch: 119, Loss:0.1972 Train: 0.9333, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:43:41,385]: Epoch: 120, Loss:0.2419 Train: 0.9417, Val:0.6500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:43:41,393]: Epoch: 121, Loss:0.2327 Train: 0.9417, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:43:41,399]: Epoch: 122, Loss:0.2312 Train: 0.9250, Val:0.6500, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:43:41,407]: Epoch: 123, Loss:0.2354 Train: 0.9250, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:41,413]: Epoch: 124, Loss:0.2371 Train: 0.9333, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:43:41,419]: Epoch: 125, Loss:0.2076 Train: 0.9333, Val:0.6125, Test: 0.5686, Time(s/epoch):0.0054
[2025-04-01 02:43:41,426]: Epoch: 126, Loss:0.2505 Train: 0.9500, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:41,433]: Epoch: 127, Loss:0.2422 Train: 0.9500, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:43:41,440]: Epoch: 128, Loss:0.2279 Train: 0.9417, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:43:41,449]: Epoch: 129, Loss:0.2420 Train: 0.9417, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:43:41,457]: Epoch: 130, Loss:0.2509 Train: 0.9500, Val:0.6500, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:43:41,465]: Epoch: 131, Loss:0.1890 Train: 0.9667, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:43:41,473]: Epoch: 132, Loss:0.2142 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:41,479]: Epoch: 133, Loss:0.1948 Train: 0.9583, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0059
[2025-04-01 02:43:41,487]: Epoch: 134, Loss:0.2027 Train: 0.9583, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:41,495]: Epoch: 135, Loss:0.1958 Train: 0.9583, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:41,503]: Epoch: 136, Loss:0.1879 Train: 0.9583, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:43:41,511]: Epoch: 137, Loss:0.1771 Train: 0.9667, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:43:41,517]: Epoch: 138, Loss:0.2009 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:43:41,525]: Epoch: 139, Loss:0.1727 Train: 0.9417, Val:0.6500, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:43:41,533]: Epoch: 140, Loss:0.1661 Train: 0.9333, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:43:41,541]: Epoch: 141, Loss:0.2003 Train: 0.9417, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:43:41,548]: Epoch: 142, Loss:0.1651 Train: 0.9583, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:43:41,556]: Epoch: 143, Loss:0.1823 Train: 0.9500, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:41,564]: Epoch: 144, Loss:0.1882 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:41,572]: Epoch: 145, Loss:0.2067 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:43:41,580]: Epoch: 146, Loss:0.2006 Train: 0.9333, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:41,586]: Epoch: 147, Loss:0.2301 Train: 0.9500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:41,595]: Epoch: 148, Loss:0.2588 Train: 0.9583, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:43:41,603]: Epoch: 149, Loss:0.2403 Train: 0.9417, Val:0.6125, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:43:41,610]: Epoch: 150, Loss:0.3460 Train: 0.9333, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:43:41,618]: Epoch: 151, Loss:0.2186 Train: 0.9250, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:43:41,625]: Epoch: 152, Loss:0.3263 Train: 0.9250, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:41,633]: Epoch: 153, Loss:0.2525 Train: 0.9250, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:43:41,643]: Epoch: 154, Loss:0.2291 Train: 0.9417, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0094
[2025-04-01 02:43:41,650]: Epoch: 155, Loss:0.1841 Train: 0.9583, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:43:41,658]: Epoch: 156, Loss:0.2036 Train: 0.9500, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:41,666]: Epoch: 157, Loss:0.2414 Train: 0.9167, Val:0.6250, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:41,674]: Epoch: 158, Loss:0.2191 Train: 0.9333, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:41,682]: Epoch: 159, Loss:0.2521 Train: 0.9417, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:41,689]: Epoch: 160, Loss:0.2210 Train: 0.9417, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:43:41,697]: Epoch: 161, Loss:0.2517 Train: 0.9417, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:41,705]: Epoch: 162, Loss:0.2011 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:41,713]: Epoch: 163, Loss:0.2202 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:43:41,723]: Epoch: 164, Loss:0.2245 Train: 0.9500, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0089
[2025-04-01 02:43:41,728]: Epoch: 165, Loss:0.2365 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0056
[2025-04-01 02:43:41,736]: Epoch: 166, Loss:0.2133 Train: 0.9417, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:41,743]: Epoch: 167, Loss:0.2407 Train: 0.9500, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:41,749]: Epoch: 168, Loss:0.1779 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:43:41,758]: Epoch: 169, Loss:0.2131 Train: 0.9583, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:41,765]: Epoch: 170, Loss:0.2377 Train: 0.9250, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:43:41,772]: Epoch: 171, Loss:0.2129 Train: 0.9333, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:41,780]: Epoch: 172, Loss:0.2079 Train: 0.9500, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:41,789]: Epoch: 173, Loss:0.2105 Train: 0.9500, Val:0.6375, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:43:41,797]: Epoch: 174, Loss:0.2142 Train: 0.9583, Val:0.6375, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:43:41,806]: Epoch: 175, Loss:0.1745 Train: 0.9583, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:43:41,814]: Epoch: 176, Loss:0.2071 Train: 0.9500, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:43:41,821]: Epoch: 177, Loss:0.2689 Train: 0.9417, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:43:41,830]: Epoch: 178, Loss:0.1725 Train: 0.9417, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:43:41,837]: Epoch: 179, Loss:0.2053 Train: 0.9583, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:43:41,845]: Epoch: 180, Loss:0.1888 Train: 0.9500, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:41,853]: Epoch: 181, Loss:0.1934 Train: 0.9500, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:41,859]: Epoch: 182, Loss:0.2143 Train: 0.9500, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0058
[2025-04-01 02:43:41,867]: Epoch: 183, Loss:0.2859 Train: 0.9500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:41,875]: Epoch: 184, Loss:0.2204 Train: 0.9417, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:41,882]: Epoch: 185, Loss:0.2376 Train: 0.9333, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:41,889]: Epoch: 186, Loss:0.2008 Train: 0.9500, Val:0.6375, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:43:41,897]: Epoch: 187, Loss:0.2063 Train: 0.9500, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:43:41,904]: Epoch: 188, Loss:0.2350 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:43:41,910]: Epoch: 189, Loss:0.2346 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:43:41,918]: Epoch: 190, Loss:0.2274 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:41,925]: Epoch: 191, Loss:0.2232 Train: 0.9500, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:41,931]: Epoch: 192, Loss:0.2044 Train: 0.9417, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:43:41,938]: Epoch: 193, Loss:0.1735 Train: 0.9500, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:43:41,945]: Epoch: 194, Loss:0.2107 Train: 0.9583, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:43:41,953]: Epoch: 195, Loss:0.2017 Train: 0.9583, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:41,959]: Epoch: 196, Loss:0.1729 Train: 0.9583, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:43:41,968]: Epoch: 197, Loss:0.2032 Train: 0.9583, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:43:41,976]: Epoch: 198, Loss:0.1918 Train: 0.9500, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:41,982]: Epoch: 199, Loss:0.1807 Train: 0.9500, Val:0.6375, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:43:41,990]: Epoch: 200, Loss:0.1944 Train: 0.9500, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:43:41,990]: [Run-1 score] {'train': 0.775, 'val': 0.7625, 'test': 0.5294117647058824}
[2025-04-01 02:43:41,990]: repeat 2/3
[2025-04-01 02:43:41,990]: Manual random seed:0
[2025-04-01 02:43:41,991]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:41,995]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:42,005]: Epoch: 001, Loss:1.6498 Train: 0.6667, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:43:42,010]: Epoch: 002, Loss:1.2679 Train: 0.7667, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0055
[2025-04-01 02:43:42,018]: Epoch: 003, Loss:0.7600 Train: 0.8000, Val:0.7625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:43:42,026]: Epoch: 004, Loss:0.7315 Train: 0.7833, Val:0.7250, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:43:42,034]: Epoch: 005, Loss:0.6862 Train: 0.8333, Val:0.7250, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:42,040]: Epoch: 006, Loss:0.6784 Train: 0.8500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0056
[2025-04-01 02:43:42,048]: Epoch: 007, Loss:0.5710 Train: 0.8500, Val:0.6125, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:43:42,057]: Epoch: 008, Loss:0.5228 Train: 0.8500, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0083
[2025-04-01 02:43:42,063]: Epoch: 009, Loss:0.4703 Train: 0.8833, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:42,071]: Epoch: 010, Loss:0.5182 Train: 0.8833, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:42,080]: Epoch: 011, Loss:0.5011 Train: 0.8667, Val:0.7250, Test: 0.5882, Time(s/epoch):0.0082
[2025-04-01 02:43:42,088]: Epoch: 012, Loss:0.4299 Train: 0.8750, Val:0.7375, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:42,095]: Epoch: 013, Loss:0.4493 Train: 0.8750, Val:0.7250, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:43:42,101]: Epoch: 014, Loss:0.5092 Train: 0.8583, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:42,110]: Epoch: 015, Loss:0.4512 Train: 0.8750, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:42,116]: Epoch: 016, Loss:0.3655 Train: 0.8750, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,123]: Epoch: 017, Loss:0.3705 Train: 0.8833, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:42,130]: Epoch: 018, Loss:0.3358 Train: 0.8833, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,137]: Epoch: 019, Loss:0.4011 Train: 0.9083, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:42,146]: Epoch: 020, Loss:0.3403 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:43:42,152]: Epoch: 021, Loss:0.3262 Train: 0.8917, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:43:42,160]: Epoch: 022, Loss:0.3498 Train: 0.8917, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:42,168]: Epoch: 023, Loss:0.3410 Train: 0.8917, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:43:42,175]: Epoch: 024, Loss:0.2778 Train: 0.9000, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:43:42,183]: Epoch: 025, Loss:0.3269 Train: 0.8917, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:42,191]: Epoch: 026, Loss:0.2960 Train: 0.9083, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:42,199]: Epoch: 027, Loss:0.2881 Train: 0.9167, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:42,208]: Epoch: 028, Loss:0.2850 Train: 0.9000, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0083
[2025-04-01 02:43:42,217]: Epoch: 029, Loss:0.2683 Train: 0.9167, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0090
[2025-04-01 02:43:42,226]: Epoch: 030, Loss:0.2868 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0087
[2025-04-01 02:43:42,234]: Epoch: 031, Loss:0.3040 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:42,242]: Epoch: 032, Loss:0.2967 Train: 0.9250, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:42,251]: Epoch: 033, Loss:0.3082 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0083
[2025-04-01 02:43:42,258]: Epoch: 034, Loss:0.2921 Train: 0.9250, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:42,266]: Epoch: 035, Loss:0.3084 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:42,273]: Epoch: 036, Loss:0.2901 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:43:42,281]: Epoch: 037, Loss:0.2734 Train: 0.9167, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:43:42,288]: Epoch: 038, Loss:0.2495 Train: 0.9167, Val:0.6875, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:43:42,296]: Epoch: 039, Loss:0.3020 Train: 0.9000, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:42,304]: Epoch: 040, Loss:0.3237 Train: 0.9083, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:43:42,312]: Epoch: 041, Loss:0.2276 Train: 0.9167, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:42,318]: Epoch: 042, Loss:0.2871 Train: 0.9250, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0062
[2025-04-01 02:43:42,327]: Epoch: 043, Loss:0.2854 Train: 0.9167, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0087
[2025-04-01 02:43:42,336]: Epoch: 044, Loss:0.2712 Train: 0.9167, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:43:42,344]: Epoch: 045, Loss:0.2713 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:42,352]: Epoch: 046, Loss:0.3032 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:43:42,360]: Epoch: 047, Loss:0.2839 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:43:42,367]: Epoch: 048, Loss:0.2610 Train: 0.9250, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:42,376]: Epoch: 049, Loss:0.2431 Train: 0.9167, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:42,382]: Epoch: 050, Loss:0.2762 Train: 0.9417, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:43:42,390]: Epoch: 051, Loss:0.2852 Train: 0.9333, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:42,396]: Epoch: 052, Loss:0.2360 Train: 0.9250, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0063
[2025-04-01 02:43:42,404]: Epoch: 053, Loss:0.2655 Train: 0.9250, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:43:42,411]: Epoch: 054, Loss:0.2987 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:42,419]: Epoch: 055, Loss:0.3067 Train: 0.9583, Val:0.7000, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:43:42,427]: Epoch: 056, Loss:0.2598 Train: 0.9500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:42,435]: Epoch: 057, Loss:0.2317 Train: 0.9333, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:42,441]: Epoch: 058, Loss:0.3072 Train: 0.9417, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:42,450]: Epoch: 059, Loss:0.3035 Train: 0.9167, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:42,457]: Epoch: 060, Loss:0.3112 Train: 0.9417, Val:0.7250, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:43:42,464]: Epoch: 061, Loss:0.2514 Train: 0.9333, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:43:42,471]: Epoch: 062, Loss:0.2716 Train: 0.9417, Val:0.7000, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:43:42,479]: Epoch: 063, Loss:0.2675 Train: 0.9333, Val:0.7000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:43:42,488]: Epoch: 064, Loss:0.2470 Train: 0.9250, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:42,497]: Epoch: 065, Loss:0.2423 Train: 0.9167, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0093
[2025-04-01 02:43:42,505]: Epoch: 066, Loss:0.2546 Train: 0.9250, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:42,511]: Epoch: 067, Loss:0.3348 Train: 0.9000, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0057
[2025-04-01 02:43:42,518]: Epoch: 068, Loss:0.2582 Train: 0.9167, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:42,526]: Epoch: 069, Loss:0.3586 Train: 0.9500, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:42,534]: Epoch: 070, Loss:0.2518 Train: 0.9417, Val:0.6875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:43:42,542]: Epoch: 071, Loss:0.2408 Train: 0.9500, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:42,549]: Epoch: 072, Loss:0.2495 Train: 0.9583, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:43:42,557]: Epoch: 073, Loss:0.2909 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:42,564]: Epoch: 074, Loss:0.2140 Train: 0.9250, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:42,572]: Epoch: 075, Loss:0.2708 Train: 0.9250, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:42,578]: Epoch: 076, Loss:0.2575 Train: 0.9250, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0061
[2025-04-01 02:43:42,585]: Epoch: 077, Loss:0.3122 Train: 0.9167, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:43:42,593]: Epoch: 078, Loss:0.3040 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:42,599]: Epoch: 079, Loss:0.1977 Train: 0.9250, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:43:42,607]: Epoch: 080, Loss:0.2409 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:42,615]: Epoch: 081, Loss:0.2510 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:42,624]: Epoch: 082, Loss:0.2409 Train: 0.9333, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:42,631]: Epoch: 083, Loss:0.2209 Train: 0.9250, Val:0.7000, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:43:42,639]: Epoch: 084, Loss:0.2257 Train: 0.9167, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:42,647]: Epoch: 085, Loss:0.2414 Train: 0.9167, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:42,656]: Epoch: 086, Loss:0.2908 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:43:42,662]: Epoch: 087, Loss:0.2241 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,669]: Epoch: 088, Loss:0.2277 Train: 0.9583, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0059
[2025-04-01 02:43:42,676]: Epoch: 089, Loss:0.2610 Train: 0.9500, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:43:42,684]: Epoch: 090, Loss:0.2280 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:42,691]: Epoch: 091, Loss:0.2126 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:43:42,699]: Epoch: 092, Loss:0.1926 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:42,707]: Epoch: 093, Loss:0.2182 Train: 0.9250, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:42,715]: Epoch: 094, Loss:0.2242 Train: 0.9417, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:42,721]: Epoch: 095, Loss:0.2250 Train: 0.9500, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:43:42,729]: Epoch: 096, Loss:0.2211 Train: 0.9417, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:43:42,736]: Epoch: 097, Loss:0.2566 Train: 0.9250, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,744]: Epoch: 098, Loss:0.1762 Train: 0.9250, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:42,750]: Epoch: 099, Loss:0.2747 Train: 0.9333, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,759]: Epoch: 100, Loss:0.2404 Train: 0.9333, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:43:42,767]: Epoch: 101, Loss:0.2107 Train: 0.9167, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:43:42,775]: Epoch: 102, Loss:0.2230 Train: 0.9250, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:43:42,784]: Epoch: 103, Loss:0.2154 Train: 0.9500, Val:0.6625, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:43:42,791]: Epoch: 104, Loss:0.2311 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:43:42,798]: Epoch: 105, Loss:0.2287 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:42,806]: Epoch: 106, Loss:0.2651 Train: 0.9333, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:42,814]: Epoch: 107, Loss:0.2162 Train: 0.9333, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:42,821]: Epoch: 108, Loss:0.2373 Train: 0.9250, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:43:42,830]: Epoch: 109, Loss:0.2559 Train: 0.9417, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:42,837]: Epoch: 110, Loss:0.2144 Train: 0.9583, Val:0.6500, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:43:42,845]: Epoch: 111, Loss:0.2441 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:42,851]: Epoch: 112, Loss:0.2137 Train: 0.9417, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:43:42,859]: Epoch: 113, Loss:0.2557 Train: 0.9583, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:42,867]: Epoch: 114, Loss:0.2815 Train: 0.9583, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:42,875]: Epoch: 115, Loss:0.1700 Train: 0.9417, Val:0.6250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:43:42,882]: Epoch: 116, Loss:0.2683 Train: 0.9417, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:43:42,889]: Epoch: 117, Loss:0.2075 Train: 0.9583, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:42,897]: Epoch: 118, Loss:0.2012 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:42,905]: Epoch: 119, Loss:0.1996 Train: 0.9167, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:42,912]: Epoch: 120, Loss:0.2086 Train: 0.9333, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:42,919]: Epoch: 121, Loss:0.2239 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0060
[2025-04-01 02:43:42,926]: Epoch: 122, Loss:0.2656 Train: 0.9583, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:43:42,932]: Epoch: 123, Loss:0.1958 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:43:42,939]: Epoch: 124, Loss:0.2217 Train: 0.9583, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:43:42,946]: Epoch: 125, Loss:0.2247 Train: 0.9333, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:43:42,954]: Epoch: 126, Loss:0.2342 Train: 0.9333, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:42,962]: Epoch: 127, Loss:0.2180 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:42,969]: Epoch: 128, Loss:0.2318 Train: 0.9500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:43:42,978]: Epoch: 129, Loss:0.1862 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:43:42,985]: Epoch: 130, Loss:0.2106 Train: 0.9583, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:43:42,994]: Epoch: 131, Loss:0.2265 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:43:43,000]: Epoch: 132, Loss:0.2749 Train: 0.9333, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:43:43,008]: Epoch: 133, Loss:0.2064 Train: 0.9167, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:43:43,017]: Epoch: 134, Loss:0.2225 Train: 0.9333, Val:0.6500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:43:43,025]: Epoch: 135, Loss:0.2216 Train: 0.9500, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:43:43,033]: Epoch: 136, Loss:0.1837 Train: 0.9583, Val:0.6250, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:43:43,040]: Epoch: 137, Loss:0.2542 Train: 0.9583, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:43,049]: Epoch: 138, Loss:0.2000 Train: 0.9583, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0088
[2025-04-01 02:43:43,057]: Epoch: 139, Loss:0.1954 Train: 0.9500, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:43:43,066]: Epoch: 140, Loss:0.2949 Train: 0.9500, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0091
[2025-04-01 02:43:43,075]: Epoch: 141, Loss:0.2247 Train: 0.9167, Val:0.6250, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:43:43,084]: Epoch: 142, Loss:0.2924 Train: 0.9333, Val:0.6250, Test: 0.4510, Time(s/epoch):0.0098
[2025-04-01 02:43:43,091]: Epoch: 143, Loss:0.2454 Train: 0.9583, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:43:43,100]: Epoch: 144, Loss:0.2024 Train: 0.9250, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:43,109]: Epoch: 145, Loss:0.1818 Train: 0.9167, Val:0.6250, Test: 0.5490, Time(s/epoch):0.0087
[2025-04-01 02:43:43,117]: Epoch: 146, Loss:0.2899 Train: 0.9167, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:43,125]: Epoch: 147, Loss:0.2360 Train: 0.9417, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:43,134]: Epoch: 148, Loss:0.1796 Train: 0.9333, Val:0.6375, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:43:43,141]: Epoch: 149, Loss:0.2555 Train: 0.9417, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:43:43,147]: Epoch: 150, Loss:0.2039 Train: 0.9500, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:43:43,155]: Epoch: 151, Loss:0.2536 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:43,161]: Epoch: 152, Loss:0.2183 Train: 0.9333, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0061
[2025-04-01 02:43:43,168]: Epoch: 153, Loss:0.2147 Train: 0.9417, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:43,176]: Epoch: 154, Loss:0.2194 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:43:43,183]: Epoch: 155, Loss:0.2120 Train: 0.9583, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:43:43,189]: Epoch: 156, Loss:0.2397 Train: 0.9500, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:43:43,197]: Epoch: 157, Loss:0.2225 Train: 0.9333, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:43,204]: Epoch: 158, Loss:0.1888 Train: 0.9333, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:43:43,211]: Epoch: 159, Loss:0.2569 Train: 0.9417, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:43:43,220]: Epoch: 160, Loss:0.2188 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:43:43,229]: Epoch: 161, Loss:0.2106 Train: 0.9417, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:43:43,237]: Epoch: 162, Loss:0.2046 Train: 0.9500, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:43:43,245]: Epoch: 163, Loss:0.2332 Train: 0.9417, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:43:43,254]: Epoch: 164, Loss:0.2439 Train: 0.9500, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:43:43,261]: Epoch: 165, Loss:0.2185 Train: 0.9583, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:43:43,269]: Epoch: 166, Loss:0.1879 Train: 0.9417, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:43:43,277]: Epoch: 167, Loss:0.2046 Train: 0.9417, Val:0.6375, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:43:43,285]: Epoch: 168, Loss:0.1901 Train: 0.9333, Val:0.6250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:43:43,294]: Epoch: 169, Loss:0.2159 Train: 0.9500, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:43,301]: Epoch: 170, Loss:0.2157 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:43,310]: Epoch: 171, Loss:0.1798 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:43,318]: Epoch: 172, Loss:0.2591 Train: 0.9500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:43,325]: Epoch: 173, Loss:0.1938 Train: 0.9250, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:43:43,333]: Epoch: 174, Loss:0.2105 Train: 0.9583, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:43:43,340]: Epoch: 175, Loss:0.2719 Train: 0.9500, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:43,349]: Epoch: 176, Loss:0.2206 Train: 0.9333, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:43,357]: Epoch: 177, Loss:0.1949 Train: 0.9750, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:43:43,364]: Epoch: 178, Loss:0.2349 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:43,371]: Epoch: 179, Loss:0.1921 Train: 0.9250, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0061
[2025-04-01 02:43:43,378]: Epoch: 180, Loss:0.2190 Train: 0.9250, Val:0.6250, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:43,385]: Epoch: 181, Loss:0.2438 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:43:43,393]: Epoch: 182, Loss:0.2002 Train: 0.9333, Val:0.6375, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:43:43,401]: Epoch: 183, Loss:0.2080 Train: 0.9417, Val:0.6375, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:43,408]: Epoch: 184, Loss:0.2048 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:43:43,415]: Epoch: 185, Loss:0.2360 Train: 0.9333, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:43,422]: Epoch: 186, Loss:0.2217 Train: 0.9333, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:43:43,428]: Epoch: 187, Loss:0.3133 Train: 0.9583, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:43,436]: Epoch: 188, Loss:0.2171 Train: 0.9583, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:43:43,443]: Epoch: 189, Loss:0.1671 Train: 0.9500, Val:0.6750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:43:43,450]: Epoch: 190, Loss:0.1946 Train: 0.9500, Val:0.6500, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:43:43,458]: Epoch: 191, Loss:0.2029 Train: 0.9500, Val:0.6250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:43:43,464]: Epoch: 192, Loss:0.1942 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:43:43,470]: Epoch: 193, Loss:0.1885 Train: 0.9667, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:43:43,477]: Epoch: 194, Loss:0.2213 Train: 0.9667, Val:0.6500, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:43:43,485]: Epoch: 195, Loss:0.2125 Train: 0.9583, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:43,492]: Epoch: 196, Loss:0.1873 Train: 0.9500, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:43:43,501]: Epoch: 197, Loss:0.2271 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0089
[2025-04-01 02:43:43,510]: Epoch: 198, Loss:0.1734 Train: 0.9500, Val:0.7000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:43:43,518]: Epoch: 199, Loss:0.2265 Train: 0.9583, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:43:43,526]: Epoch: 200, Loss:0.2573 Train: 0.9583, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0072
[2025-04-01 02:43:43,526]: [Run-2 score] {'train': 0.8, 'val': 0.7625, 'test': 0.49019607843137253}
[2025-04-01 02:43:43,526]: repeat 3/3
[2025-04-01 02:43:43,526]: Manual random seed:0
[2025-04-01 02:43:43,526]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:43,531]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:43,541]: Epoch: 001, Loss:1.7341 Train: 0.6833, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:43,548]: Epoch: 002, Loss:1.1852 Train: 0.7250, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0072
[2025-04-01 02:43:43,555]: Epoch: 003, Loss:0.9226 Train: 0.8083, Val:0.6875, Test: 0.5882, Time(s/epoch):0.0067
[2025-04-01 02:43:43,563]: Epoch: 004, Loss:0.7288 Train: 0.8167, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:43:43,570]: Epoch: 005, Loss:0.6744 Train: 0.8250, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:43,577]: Epoch: 006, Loss:0.5884 Train: 0.8333, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:43:43,584]: Epoch: 007, Loss:0.5176 Train: 0.8500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:43,592]: Epoch: 008, Loss:0.5639 Train: 0.8333, Val:0.7250, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:43,600]: Epoch: 009, Loss:0.5008 Train: 0.8167, Val:0.7250, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:43:43,609]: Epoch: 010, Loss:0.5780 Train: 0.8417, Val:0.7125, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:43:43,617]: Epoch: 011, Loss:0.5299 Train: 0.8583, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:43:43,624]: Epoch: 012, Loss:0.5620 Train: 0.8750, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:43,631]: Epoch: 013, Loss:0.4561 Train: 0.8833, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:43:43,639]: Epoch: 014, Loss:0.5063 Train: 0.9083, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:43,647]: Epoch: 015, Loss:0.4373 Train: 0.8750, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0083
[2025-04-01 02:43:43,656]: Epoch: 016, Loss:0.4148 Train: 0.8750, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0087
[2025-04-01 02:43:43,664]: Epoch: 017, Loss:0.3772 Train: 0.8750, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:43,673]: Epoch: 018, Loss:0.4224 Train: 0.8750, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:43:43,679]: Epoch: 019, Loss:0.3724 Train: 0.8917, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:43:43,688]: Epoch: 020, Loss:0.3727 Train: 0.9000, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:43:43,697]: Epoch: 021, Loss:0.3861 Train: 0.9083, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:43:43,705]: Epoch: 022, Loss:0.3486 Train: 0.9083, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:43,712]: Epoch: 023, Loss:0.3271 Train: 0.8917, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:43:43,719]: Epoch: 024, Loss:0.3718 Train: 0.9000, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:43:43,727]: Epoch: 025, Loss:0.3772 Train: 0.9000, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:43,734]: Epoch: 026, Loss:0.3324 Train: 0.8917, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:43:43,743]: Epoch: 027, Loss:0.3170 Train: 0.8750, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:43:43,749]: Epoch: 028, Loss:0.3691 Train: 0.9167, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:43:43,758]: Epoch: 029, Loss:0.3322 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:43:43,766]: Epoch: 030, Loss:0.3229 Train: 0.9250, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:43:43,773]: Epoch: 031, Loss:0.3639 Train: 0.9167, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0065
[2025-04-01 02:43:43,780]: Epoch: 032, Loss:0.3216 Train: 0.9167, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:43:43,789]: Epoch: 033, Loss:0.3621 Train: 0.9250, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:43:43,796]: Epoch: 034, Loss:0.3015 Train: 0.9167, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:43,805]: Epoch: 035, Loss:0.3528 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:43,813]: Epoch: 036, Loss:0.3030 Train: 0.9250, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:43:43,820]: Epoch: 037, Loss:0.3408 Train: 0.9250, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:43,828]: Epoch: 038, Loss:0.3235 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:43,836]: Epoch: 039, Loss:0.3145 Train: 0.9000, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:43:43,844]: Epoch: 040, Loss:0.2972 Train: 0.9000, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:43,850]: Epoch: 041, Loss:0.3098 Train: 0.9167, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0059
[2025-04-01 02:43:43,857]: Epoch: 042, Loss:0.2918 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:43,864]: Epoch: 043, Loss:0.2613 Train: 0.9333, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:43,873]: Epoch: 044, Loss:0.2926 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0088
[2025-04-01 02:43:43,880]: Epoch: 045, Loss:0.2807 Train: 0.9083, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:43,887]: Epoch: 046, Loss:0.2786 Train: 0.9083, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:43:43,895]: Epoch: 047, Loss:0.2883 Train: 0.9167, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:43,901]: Epoch: 048, Loss:0.2560 Train: 0.9083, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:43:43,909]: Epoch: 049, Loss:0.2738 Train: 0.9167, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:43,916]: Epoch: 050, Loss:0.2666 Train: 0.9250, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:43,924]: Epoch: 051, Loss:0.2637 Train: 0.9250, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:43,932]: Epoch: 052, Loss:0.2626 Train: 0.9333, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:43,938]: Epoch: 053, Loss:0.2770 Train: 0.9167, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:43,945]: Epoch: 054, Loss:0.2473 Train: 0.9250, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:43:43,953]: Epoch: 055, Loss:0.2773 Train: 0.9333, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:43:43,959]: Epoch: 056, Loss:0.2827 Train: 0.9417, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:43:43,966]: Epoch: 057, Loss:0.2450 Train: 0.9500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:43,973]: Epoch: 058, Loss:0.2575 Train: 0.9417, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:43,979]: Epoch: 059, Loss:0.2963 Train: 0.9417, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:43:43,987]: Epoch: 060, Loss:0.3223 Train: 0.9333, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:43:43,993]: Epoch: 061, Loss:0.2496 Train: 0.9500, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:43:44,000]: Epoch: 062, Loss:0.2601 Train: 0.9583, Val:0.7125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:43:44,007]: Epoch: 063, Loss:0.3024 Train: 0.9417, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:43:44,015]: Epoch: 064, Loss:0.2857 Train: 0.9250, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:43:44,024]: Epoch: 065, Loss:0.2433 Train: 0.9250, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:43:44,031]: Epoch: 066, Loss:0.2662 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:43:44,039]: Epoch: 067, Loss:0.2524 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:44,046]: Epoch: 068, Loss:0.2419 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:44,054]: Epoch: 069, Loss:0.2288 Train: 0.9417, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:44,063]: Epoch: 070, Loss:0.2977 Train: 0.9417, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:43:44,071]: Epoch: 071, Loss:0.2671 Train: 0.9500, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:43:44,080]: Epoch: 072, Loss:0.2480 Train: 0.9417, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:43:44,087]: Epoch: 073, Loss:0.2676 Train: 0.9417, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:43:44,095]: Epoch: 074, Loss:0.2686 Train: 0.9333, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:44,104]: Epoch: 075, Loss:0.3349 Train: 0.9417, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:43:44,111]: Epoch: 076, Loss:0.2460 Train: 0.9500, Val:0.6625, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:43:44,119]: Epoch: 077, Loss:0.2364 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:44,127]: Epoch: 078, Loss:0.1952 Train: 0.9250, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:43:44,135]: Epoch: 079, Loss:0.2697 Train: 0.9333, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:43:44,143]: Epoch: 080, Loss:0.2207 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:44,150]: Epoch: 081, Loss:0.2077 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0072
[2025-04-01 02:43:44,158]: Epoch: 082, Loss:0.2529 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:43:44,166]: Epoch: 083, Loss:0.2484 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:43:44,175]: Epoch: 084, Loss:0.2506 Train: 0.9417, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:43:44,183]: Epoch: 085, Loss:0.2348 Train: 0.9417, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:44,191]: Epoch: 086, Loss:0.2830 Train: 0.9250, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:43:44,199]: Epoch: 087, Loss:0.2732 Train: 0.9417, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:44,208]: Epoch: 088, Loss:0.3230 Train: 0.9333, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:43:44,218]: Epoch: 089, Loss:0.3069 Train: 0.9583, Val:0.6625, Test: 0.4902, Time(s/epoch):0.0096
[2025-04-01 02:43:44,226]: Epoch: 090, Loss:0.2914 Train: 0.9500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:43:44,234]: Epoch: 091, Loss:0.2633 Train: 0.9250, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:44,240]: Epoch: 092, Loss:0.2528 Train: 0.9083, Val:0.7000, Test: 0.5882, Time(s/epoch):0.0059
[2025-04-01 02:43:44,249]: Epoch: 093, Loss:0.3357 Train: 0.9250, Val:0.7250, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:43:44,256]: Epoch: 094, Loss:0.2724 Train: 0.9333, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:43:44,264]: Epoch: 095, Loss:0.2485 Train: 0.9417, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:43:44,273]: Epoch: 096, Loss:0.2805 Train: 0.9500, Val:0.6250, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:43:44,283]: Epoch: 097, Loss:0.2417 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0103
[2025-04-01 02:43:44,290]: Epoch: 098, Loss:0.2860 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:43:44,299]: Epoch: 099, Loss:0.2614 Train: 0.9250, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0093
[2025-04-01 02:43:44,307]: Epoch: 100, Loss:0.2112 Train: 0.9417, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:44,315]: Epoch: 101, Loss:0.2047 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:44,324]: Epoch: 102, Loss:0.2362 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:43:44,332]: Epoch: 103, Loss:0.2524 Train: 0.9417, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:43:44,339]: Epoch: 104, Loss:0.2138 Train: 0.9250, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:44,346]: Epoch: 105, Loss:0.2836 Train: 0.9500, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:44,354]: Epoch: 106, Loss:0.2357 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:43:44,364]: Epoch: 107, Loss:0.3018 Train: 0.9333, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0094
[2025-04-01 02:43:44,372]: Epoch: 108, Loss:0.2630 Train: 0.9333, Val:0.6750, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:43:44,378]: Epoch: 109, Loss:0.2591 Train: 0.9333, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0061
[2025-04-01 02:43:44,386]: Epoch: 110, Loss:0.2078 Train: 0.9333, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:43:44,393]: Epoch: 111, Loss:0.2246 Train: 0.9333, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:44,399]: Epoch: 112, Loss:0.2281 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:43:44,407]: Epoch: 113, Loss:0.2327 Train: 0.9083, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:43:44,415]: Epoch: 114, Loss:0.2674 Train: 0.9167, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:43:44,422]: Epoch: 115, Loss:0.2339 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:43:44,431]: Epoch: 116, Loss:0.2244 Train: 0.9250, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0087
[2025-04-01 02:43:44,440]: Epoch: 117, Loss:0.2416 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:43:44,449]: Epoch: 118, Loss:0.2540 Train: 0.9500, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0092
[2025-04-01 02:43:44,457]: Epoch: 119, Loss:0.2423 Train: 0.9667, Val:0.6250, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:43:44,464]: Epoch: 120, Loss:0.2346 Train: 0.9417, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:44,474]: Epoch: 121, Loss:0.2140 Train: 0.9500, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0092
[2025-04-01 02:43:44,481]: Epoch: 122, Loss:0.2371 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:43:44,488]: Epoch: 123, Loss:0.2217 Train: 0.9583, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:43:44,497]: Epoch: 124, Loss:0.2124 Train: 0.9333, Val:0.6375, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:43:44,505]: Epoch: 125, Loss:0.2218 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:43:44,512]: Epoch: 126, Loss:0.1957 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:43:44,519]: Epoch: 127, Loss:0.2162 Train: 0.9417, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:43:44,528]: Epoch: 128, Loss:0.2213 Train: 0.9333, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:43:44,535]: Epoch: 129, Loss:0.2052 Train: 0.9250, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:43:44,541]: Epoch: 130, Loss:0.2201 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:43:44,549]: Epoch: 131, Loss:0.2579 Train: 0.9417, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:43:44,558]: Epoch: 132, Loss:0.2102 Train: 0.9667, Val:0.6875, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:43:44,565]: Epoch: 133, Loss:0.2597 Train: 0.9583, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:43:44,574]: Epoch: 134, Loss:0.2207 Train: 0.9417, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:43:44,583]: Epoch: 135, Loss:0.2240 Train: 0.9500, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:43:44,593]: Epoch: 136, Loss:0.1952 Train: 0.9583, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0100
[2025-04-01 02:43:44,599]: Epoch: 137, Loss:0.2714 Train: 0.9167, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0055
[2025-04-01 02:43:44,607]: Epoch: 138, Loss:0.2466 Train: 0.9250, Val:0.6875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:43:44,614]: Epoch: 139, Loss:0.2411 Train: 0.9500, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:43:44,620]: Epoch: 140, Loss:0.2317 Train: 0.9417, Val:0.6500, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:43:44,628]: Epoch: 141, Loss:0.2141 Train: 0.9417, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:44,636]: Epoch: 142, Loss:0.2353 Train: 0.9417, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:43:44,644]: Epoch: 143, Loss:0.2414 Train: 0.9417, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:43:44,652]: Epoch: 144, Loss:0.2349 Train: 0.9500, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:43:44,661]: Epoch: 145, Loss:0.1983 Train: 0.9500, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:43:44,669]: Epoch: 146, Loss:0.1997 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:43:44,677]: Epoch: 147, Loss:0.2208 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:43:44,686]: Epoch: 148, Loss:0.2468 Train: 0.9250, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:43:44,693]: Epoch: 149, Loss:0.2063 Train: 0.9417, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:44,701]: Epoch: 150, Loss:0.2184 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:43:44,707]: Epoch: 151, Loss:0.2440 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:43:44,714]: Epoch: 152, Loss:0.2020 Train: 0.9250, Val:0.6125, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:43:44,723]: Epoch: 153, Loss:0.2277 Train: 0.9417, Val:0.6625, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:43:44,730]: Epoch: 154, Loss:0.2265 Train: 0.9250, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:44,738]: Epoch: 155, Loss:0.2248 Train: 0.9167, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:43:44,745]: Epoch: 156, Loss:0.2303 Train: 0.9167, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0064
[2025-04-01 02:43:44,753]: Epoch: 157, Loss:0.2279 Train: 0.9167, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:43:44,759]: Epoch: 158, Loss:0.2342 Train: 0.9417, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:43:44,766]: Epoch: 159, Loss:0.2762 Train: 0.9333, Val:0.6500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:43:44,773]: Epoch: 160, Loss:0.3362 Train: 0.9333, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:43:44,780]: Epoch: 161, Loss:0.2121 Train: 0.9333, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:43:44,789]: Epoch: 162, Loss:0.2126 Train: 0.9250, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0092
[2025-04-01 02:43:44,797]: Epoch: 163, Loss:0.2037 Train: 0.9333, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:43:44,806]: Epoch: 164, Loss:0.2513 Train: 0.9250, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:43:44,812]: Epoch: 165, Loss:0.2137 Train: 0.9250, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:43:44,820]: Epoch: 166, Loss:0.2827 Train: 0.9333, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:44,828]: Epoch: 167, Loss:0.2808 Train: 0.9417, Val:0.6625, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:43:44,836]: Epoch: 168, Loss:0.2196 Train: 0.9583, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:44,844]: Epoch: 169, Loss:0.1919 Train: 0.9333, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:43:44,849]: Epoch: 170, Loss:0.2264 Train: 0.9167, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0057
[2025-04-01 02:43:44,857]: Epoch: 171, Loss:0.2532 Train: 0.9250, Val:0.6875, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:43:44,865]: Epoch: 172, Loss:0.2950 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:43:44,874]: Epoch: 173, Loss:0.2161 Train: 0.9417, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:43:44,880]: Epoch: 174, Loss:0.2567 Train: 0.9250, Val:0.7000, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:43:44,888]: Epoch: 175, Loss:0.2292 Train: 0.9333, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:43:44,895]: Epoch: 176, Loss:0.2350 Train: 0.9500, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:43:44,901]: Epoch: 177, Loss:0.1915 Train: 0.9417, Val:0.6625, Test: 0.5490, Time(s/epoch):0.0060
[2025-04-01 02:43:44,908]: Epoch: 178, Loss:0.2325 Train: 0.9417, Val:0.6375, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:43:44,916]: Epoch: 179, Loss:0.2149 Train: 0.9417, Val:0.6750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:43:44,923]: Epoch: 180, Loss:0.2205 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:43:44,932]: Epoch: 181, Loss:0.1800 Train: 0.9583, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:43:44,939]: Epoch: 182, Loss:0.2134 Train: 0.9500, Val:0.7000, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:44,946]: Epoch: 183, Loss:0.2237 Train: 0.9500, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:43:44,954]: Epoch: 184, Loss:0.1973 Train: 0.9500, Val:0.6500, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:43:44,963]: Epoch: 185, Loss:0.1946 Train: 0.9333, Val:0.6750, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:43:44,969]: Epoch: 186, Loss:0.2235 Train: 0.9417, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:43:44,976]: Epoch: 187, Loss:0.2688 Train: 0.9333, Val:0.6750, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:43:44,983]: Epoch: 188, Loss:0.2433 Train: 0.9417, Val:0.6500, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:43:44,990]: Epoch: 189, Loss:0.2202 Train: 0.9417, Val:0.6750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:44,997]: Epoch: 190, Loss:0.2385 Train: 0.9417, Val:0.6875, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:43:45,004]: Epoch: 191, Loss:0.1717 Train: 0.9667, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:43:45,013]: Epoch: 192, Loss:0.2095 Train: 0.9500, Val:0.6750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:43:45,020]: Epoch: 193, Loss:0.2178 Train: 0.9500, Val:0.6875, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:43:45,029]: Epoch: 194, Loss:0.2017 Train: 0.9417, Val:0.7000, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:43:45,037]: Epoch: 195, Loss:0.1894 Train: 0.9500, Val:0.7000, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:43:45,044]: Epoch: 196, Loss:0.2202 Train: 0.9583, Val:0.6500, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:43:45,051]: Epoch: 197, Loss:0.1835 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:43:45,060]: Epoch: 198, Loss:0.1826 Train: 0.9583, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0094
[2025-04-01 02:43:45,067]: Epoch: 199, Loss:0.2104 Train: 0.9500, Val:0.6625, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:43:45,075]: Epoch: 200, Loss:0.1785 Train: 0.9500, Val:0.6750, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:43:45,075]: [Run-3 score] {'train': 0.8333333333333334, 'val': 0.725, 'test': 0.5098039215686274}
[2025-04-01 02:43:45,075]: repeat 1/3
[2025-04-01 02:43:45,075]: Manual random seed:0
[2025-04-01 02:43:45,076]: auto fixed data split seed to 0, model init seed to 0

Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0109
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0108
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0098
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0077
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0091
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0072
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0077
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0076
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0086
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0097
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0098
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0091
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0073
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0079
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0077
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0076
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0073
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0081
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0095
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0096
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0094
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0102
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0097
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0096
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0095
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0095
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0095
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0094
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0094
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0094
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0095
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0093
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0101
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0097
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0097
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0107
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0086
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0082
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0080
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0073
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0074
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0073
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0071
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0082
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0094
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0080
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0072
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0088
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0097
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0078
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0071
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0077
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0069
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0074
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0074
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0074
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0069
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0084
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0093
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0093
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0092
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0094
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0084
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0071
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0080
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0071
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0078
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0097
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0102
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0107
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0083
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0092
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0110
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0099
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0095
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0095
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.6467 Time(s/epoch):0.0065
Epoch: 002, Supervised Loss:1.5446 Time(s/epoch):0.0063
Epoch: 003, Supervised Loss:1.4415 Time(s/epoch):0.0068
Epoch: 004, Supervised Loss:1.3767 Time(s/epoch):0.0072
Epoch: 005, Supervised Loss:1.3478 Time(s/epoch):0.0065
Epoch: 006, Supervised Loss:1.3399 Time(s/epoch):0.0063
Epoch: 007, Supervised Loss:1.3479 Time(s/epoch):0.0063
Epoch: 008, Supervised Loss:1.3583 Time(s/epoch):0.0050
Epoch: 009, Supervised Loss:1.3542 Time(s/epoch):0.0065
Epoch: 010, Supervised Loss:1.3409 Time(s/epoch):0.0063
Epoch: 011, Supervised Loss:1.3344 Time(s/epoch):0.0066
Epoch: 012, Supervised Loss:1.3383 Time(s/epoch):0.0066
Epoch: 013, Supervised Loss:1.3446 Time(s/epoch):0.0065
Epoch: 014, Supervised Loss:1.3478 Time(s/epoch):0.0064
Epoch: 015, Supervised Loss:1.3482 Time(s/epoch):0.0064
Epoch: 016, Supervised Loss:1.3483 Time(s/epoch):0.0052
Epoch: 017, Supervised Loss:1.3507 Time(s/epoch):0.0049
Epoch: 018, Supervised Loss:1.3577 Time(s/epoch):0.0046
Epoch: 019, Supervised Loss:1.3679 Time(s/epoch):0.0049
Epoch: 020, Supervised Loss:1.3766 Time(s/epoch):0.0047
Epoch: 021, Supervised Loss:1.3792 Time(s/epoch):0.0047
Epoch: 022, Supervised Loss:1.3740 Time(s/epoch):0.0046
Epoch: 023, Supervised Loss:1.3623 Time(s/epoch):0.0049
Epoch: 024, Supervised Loss:1.3482 Time(s/epoch):0.0048
Epoch: 025, Supervised Loss:1.3367 Time(s/epoch):0.0048
Epoch: 026, Supervised Loss:1.3319 Time(s/epoch):0.0053
Epoch: 027, Supervised Loss:1.3347 Time(s/epoch):0.0049
Epoch: 028, Supervised Loss:1.3442 Time(s/epoch):0.0047
Epoch: 029, Supervised Loss:1.3583 Time(s/epoch):0.0049
Epoch: 030, Supervised Loss:1.3745 Time(s/epoch):0.0048
0.88843113 515
Add 423 edges.
Prune 328 edges from torch.Size([2, 515]) to torch.Size([2, 187])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.652
Data(x=[251, 1703], edge_index=[2, 600], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.4539 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.652
Data(x=[251, 1703], edge_index=[2, 600], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5322 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.652
Data(x=[251, 1703], edge_index=[2, 600], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5456 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(14)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0148
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0094
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0105
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0084
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0097
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0101
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0103
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0102
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0107
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0118
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0134
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0113
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0096
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0076
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0078
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0081
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0097
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0085
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0070
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0085
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0096
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0085
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0076
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0100
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0100
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0102
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0088
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0072
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0075
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0075
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0082
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0072
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0070
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0071
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0083
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0095
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0075
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0069
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0082
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0070
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0076
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0075
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0082
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0078
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0079
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0073
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0088
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0101
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0093
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0082
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0083
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0096
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0077
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0083
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0083
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0084
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0074
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0077
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0071
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0071
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0069
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0081
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0078
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0077
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0075
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0078
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0074
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0081
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0099
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0089
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0080
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0097
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0100
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0101
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0105
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0100
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0091
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0090
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0102
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0096
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0109
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0106
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0094
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0081
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0075
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0084
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0099
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0088
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0090
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0101
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0110
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0104
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0105
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0099
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0079
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0082
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0106
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0096
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0086
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0079
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0081
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0093
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0090
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0101
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0089
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0090
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0102
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0103
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0104
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0087
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0089
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0102
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0088
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0085
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0103
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0104
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0106
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0110
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0103
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0101
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0101
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0101
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0103
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0102
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0112
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0095
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0096
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0101
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0084
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0090
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0106
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0106
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0112
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0112
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0111
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0094
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0105
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0096
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0107
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0106
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0100
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0096
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0081
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0083[2025-04-01 02:43:47,084]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:47,093]: Epoch: 001, Loss:1.7814 Train: 0.5333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:43:47,101]: Epoch: 002, Loss:1.4027 Train: 0.7083, Val:0.5375, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:43:47,108]: Epoch: 003, Loss:0.7934 Train: 0.8083, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0069
[2025-04-01 02:43:47,115]: Epoch: 004, Loss:0.6623 Train: 0.8750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:47,121]: Epoch: 005, Loss:0.5892 Train: 0.8833, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0060
[2025-04-01 02:43:47,129]: Epoch: 006, Loss:0.5271 Train: 0.8917, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:43:47,137]: Epoch: 007, Loss:0.4200 Train: 0.8917, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:47,144]: Epoch: 008, Loss:0.3683 Train: 0.9000, Val:0.6375, Test: 0.5882, Time(s/epoch):0.0063
[2025-04-01 02:43:47,151]: Epoch: 009, Loss:0.3668 Train: 0.9000, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:47,159]: Epoch: 010, Loss:0.3260 Train: 0.9167, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:47,167]: Epoch: 011, Loss:0.3090 Train: 0.9250, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:47,176]: Epoch: 012, Loss:0.2465 Train: 0.9333, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:47,182]: Epoch: 013, Loss:0.2138 Train: 0.9250, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:47,190]: Epoch: 014, Loss:0.3156 Train: 0.9250, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0071
[2025-04-01 02:43:47,198]: Epoch: 015, Loss:0.2913 Train: 0.9500, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:47,207]: Epoch: 016, Loss:0.1840 Train: 0.9500, Val:0.6375, Test: 0.6275, Time(s/epoch):0.0084
[2025-04-01 02:43:47,215]: Epoch: 017, Loss:0.3093 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:47,224]: Epoch: 018, Loss:0.1817 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0095
[2025-04-01 02:43:47,237]: Epoch: 019, Loss:0.1964 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0122
[2025-04-01 02:43:47,248]: Epoch: 020, Loss:0.2379 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0106
[2025-04-01 02:43:47,256]: Epoch: 021, Loss:0.1540 Train: 0.9667, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:47,265]: Epoch: 022, Loss:0.2008 Train: 0.9667, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:47,273]: Epoch: 023, Loss:0.1496 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:47,282]: Epoch: 024, Loss:0.1399 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:47,290]: Epoch: 025, Loss:0.1366 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:47,298]: Epoch: 026, Loss:0.1224 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:47,306]: Epoch: 027, Loss:0.1401 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:47,314]: Epoch: 028, Loss:0.1607 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:47,320]: Epoch: 029, Loss:0.1249 Train: 0.9917, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:47,329]: Epoch: 030, Loss:0.1391 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0090
[2025-04-01 02:43:47,338]: Epoch: 031, Loss:0.1097 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:47,347]: Epoch: 032, Loss:0.1568 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0090
[2025-04-01 02:43:47,356]: Epoch: 033, Loss:0.1411 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0090
[2025-04-01 02:43:47,365]: Epoch: 034, Loss:0.1322 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:47,372]: Epoch: 035, Loss:0.0934 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:47,380]: Epoch: 036, Loss:0.1027 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:47,389]: Epoch: 037, Loss:0.1175 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:47,398]: Epoch: 038, Loss:0.1375 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:47,406]: Epoch: 039, Loss:0.1485 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:47,415]: Epoch: 040, Loss:0.1249 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:47,435]: Epoch: 041, Loss:0.1294 Train: 0.9667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0196
[2025-04-01 02:43:47,445]: Epoch: 042, Loss:0.1532 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0102
[2025-04-01 02:43:47,454]: Epoch: 043, Loss:0.1254 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0088
[2025-04-01 02:43:47,461]: Epoch: 044, Loss:0.1151 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:47,469]: Epoch: 045, Loss:0.1342 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:47,477]: Epoch: 046, Loss:0.1127 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:47,486]: Epoch: 047, Loss:0.1221 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:47,495]: Epoch: 048, Loss:0.1048 Train: 0.9917, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0085
[2025-04-01 02:43:47,501]: Epoch: 049, Loss:0.1206 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0063
[2025-04-01 02:43:47,510]: Epoch: 050, Loss:0.1191 Train: 0.9917, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0091
[2025-04-01 02:43:47,518]: Epoch: 051, Loss:0.1078 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:47,526]: Epoch: 052, Loss:0.0966 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:47,534]: Epoch: 053, Loss:0.1075 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:47,541]: Epoch: 054, Loss:0.1274 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:47,550]: Epoch: 055, Loss:0.1135 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:47,557]: Epoch: 056, Loss:0.1051 Train: 0.9917, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:47,564]: Epoch: 057, Loss:0.1387 Train: 0.9833, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:47,571]: Epoch: 058, Loss:0.0779 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:47,578]: Epoch: 059, Loss:0.0993 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:47,585]: Epoch: 060, Loss:0.0830 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:47,593]: Epoch: 061, Loss:0.1744 Train: 0.9583, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:47,601]: Epoch: 062, Loss:0.1253 Train: 0.9667, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0072
[2025-04-01 02:43:47,609]: Epoch: 063, Loss:0.2340 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:47,616]: Epoch: 064, Loss:0.1048 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:47,624]: Epoch: 065, Loss:0.1068 Train: 0.9667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:47,630]: Epoch: 066, Loss:0.1289 Train: 0.9750, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:47,638]: Epoch: 067, Loss:0.1076 Train: 0.9750, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:47,646]: Epoch: 068, Loss:0.1391 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:47,654]: Epoch: 069, Loss:0.0984 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:47,662]: Epoch: 070, Loss:0.1674 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:47,670]: Epoch: 071, Loss:0.1199 Train: 1.0000, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:47,678]: Epoch: 072, Loss:0.1203 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:47,687]: Epoch: 073, Loss:0.1291 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:47,694]: Epoch: 074, Loss:0.0911 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:47,700]: Epoch: 075, Loss:0.0922 Train: 0.9750, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:47,709]: Epoch: 076, Loss:0.1411 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:47,718]: Epoch: 077, Loss:0.1175 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:47,725]: Epoch: 078, Loss:0.1048 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:47,734]: Epoch: 079, Loss:0.0940 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:47,741]: Epoch: 080, Loss:0.1085 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:47,748]: Epoch: 081, Loss:0.0862 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:47,755]: Epoch: 082, Loss:0.1225 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:47,764]: Epoch: 083, Loss:0.1107 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0084
[2025-04-01 02:43:47,770]: Epoch: 084, Loss:0.0969 Train: 0.9667, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0060
[2025-04-01 02:43:47,776]: Epoch: 085, Loss:0.1108 Train: 0.9667, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:47,784]: Epoch: 086, Loss:0.1114 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:47,791]: Epoch: 087, Loss:0.0883 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:47,799]: Epoch: 088, Loss:0.0960 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:47,807]: Epoch: 089, Loss:0.1182 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:47,815]: Epoch: 090, Loss:0.1256 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:47,824]: Epoch: 091, Loss:0.1041 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:47,833]: Epoch: 092, Loss:0.1017 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:47,839]: Epoch: 093, Loss:0.1046 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:47,848]: Epoch: 094, Loss:0.1148 Train: 0.9917, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0087
[2025-04-01 02:43:47,856]: Epoch: 095, Loss:0.1186 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:47,864]: Epoch: 096, Loss:0.0974 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:47,873]: Epoch: 097, Loss:0.1590 Train: 0.9917, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:47,880]: Epoch: 098, Loss:0.1072 Train: 0.9833, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:47,887]: Epoch: 099, Loss:0.1434 Train: 0.9917, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:47,896]: Epoch: 100, Loss:0.1069 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:47,904]: Epoch: 101, Loss:0.1143 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:47,910]: Epoch: 102, Loss:0.0925 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:47,917]: Epoch: 103, Loss:0.1047 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:47,924]: Epoch: 104, Loss:0.0936 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:47,931]: Epoch: 105, Loss:0.1247 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:47,939]: Epoch: 106, Loss:0.0966 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:47,946]: Epoch: 107, Loss:0.0972 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:47,955]: Epoch: 108, Loss:0.0852 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:47,963]: Epoch: 109, Loss:0.0869 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:47,971]: Epoch: 110, Loss:0.1060 Train: 0.9667, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:47,977]: Epoch: 111, Loss:0.0997 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:47,984]: Epoch: 112, Loss:0.1542 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:47,992]: Epoch: 113, Loss:0.0956 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:47,998]: Epoch: 114, Loss:0.1083 Train: 0.9667, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0062
[2025-04-01 02:43:48,006]: Epoch: 115, Loss:0.1208 Train: 0.9583, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:48,012]: Epoch: 116, Loss:0.0765 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0058
[2025-04-01 02:43:48,020]: Epoch: 117, Loss:0.0989 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:48,027]: Epoch: 118, Loss:0.1028 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:48,035]: Epoch: 119, Loss:0.1253 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:48,043]: Epoch: 120, Loss:0.1203 Train: 0.9750, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:48,050]: Epoch: 121, Loss:0.1582 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:48,057]: Epoch: 122, Loss:0.0913 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:48,064]: Epoch: 123, Loss:0.0946 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:48,071]: Epoch: 124, Loss:0.1146 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:48,078]: Epoch: 125, Loss:0.1293 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:48,085]: Epoch: 126, Loss:0.0841 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:48,093]: Epoch: 127, Loss:0.1111 Train: 0.9583, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:48,099]: Epoch: 128, Loss:0.1552 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:48,108]: Epoch: 129, Loss:0.1161 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:48,116]: Epoch: 130, Loss:0.0767 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:48,124]: Epoch: 131, Loss:0.0867 Train: 0.9917, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0076
[2025-04-01 02:43:48,131]: Epoch: 132, Loss:0.1039 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:48,140]: Epoch: 133, Loss:0.0967 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:48,148]: Epoch: 134, Loss:0.1055 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:48,156]: Epoch: 135, Loss:0.0994 Train: 0.9750, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:48,162]: Epoch: 136, Loss:0.0805 Train: 0.9833, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:48,170]: Epoch: 137, Loss:0.0665 Train: 0.9833, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:48,177]: Epoch: 138, Loss:0.0898 Train: 0.9917, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0076
[2025-04-01 02:43:48,185]: Epoch: 139, Loss:0.0901 Train: 0.9917, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:48,193]: Epoch: 140, Loss:0.0839 Train: 0.9917, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0075
[2025-04-01 02:43:48,199]: Epoch: 141, Loss:0.0873 Train: 0.9917, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0062
[2025-04-01 02:43:48,207]: Epoch: 142, Loss:0.0992 Train: 0.9917, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:48,214]: Epoch: 143, Loss:0.0856 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:48,220]: Epoch: 144, Loss:0.0759 Train: 0.9917, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0061
[2025-04-01 02:43:48,229]: Epoch: 145, Loss:0.1052 Train: 1.0000, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:48,238]: Epoch: 146, Loss:0.0679 Train: 1.0000, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0092
[2025-04-01 02:43:48,247]: Epoch: 147, Loss:0.0959 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:48,255]: Epoch: 148, Loss:0.0823 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:48,262]: Epoch: 149, Loss:0.0904 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:48,271]: Epoch: 150, Loss:0.0818 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:48,279]: Epoch: 151, Loss:0.0695 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:48,286]: Epoch: 152, Loss:0.0915 Train: 1.0000, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:48,295]: Epoch: 153, Loss:0.0945 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:48,301]: Epoch: 154, Loss:0.0835 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:48,309]: Epoch: 155, Loss:0.1091 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:48,318]: Epoch: 156, Loss:0.0942 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:43:48,325]: Epoch: 157, Loss:0.0947 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:48,334]: Epoch: 158, Loss:0.0718 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:48,342]: Epoch: 159, Loss:0.1000 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:48,349]: Epoch: 160, Loss:0.0890 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:48,358]: Epoch: 161, Loss:0.0872 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:48,365]: Epoch: 162, Loss:0.0784 Train: 1.0000, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:48,371]: Epoch: 163, Loss:0.0764 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:48,378]: Epoch: 164, Loss:0.0711 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:48,386]: Epoch: 165, Loss:0.1070 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:48,395]: Epoch: 166, Loss:0.0945 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:48,401]: Epoch: 167, Loss:0.0901 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:48,409]: Epoch: 168, Loss:0.0878 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:48,418]: Epoch: 169, Loss:0.1038 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:48,426]: Epoch: 170, Loss:0.0926 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:48,435]: Epoch: 171, Loss:0.0849 Train: 0.9833, Val:0.6125, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:48,440]: Epoch: 172, Loss:0.0993 Train: 0.9917, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0055
[2025-04-01 02:43:48,448]: Epoch: 173, Loss:0.1076 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:48,455]: Epoch: 174, Loss:0.0696 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:48,461]: Epoch: 175, Loss:0.1026 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:48,468]: Epoch: 176, Loss:0.0967 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:48,476]: Epoch: 177, Loss:0.0851 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:48,484]: Epoch: 178, Loss:0.1017 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:48,492]: Epoch: 179, Loss:0.0867 Train: 0.9917, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:48,500]: Epoch: 180, Loss:0.0760 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:48,508]: Epoch: 181, Loss:0.0887 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,515]: Epoch: 182, Loss:0.0846 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:48,523]: Epoch: 183, Loss:0.0959 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:48,531]: Epoch: 184, Loss:0.0863 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:48,537]: Epoch: 185, Loss:0.0931 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:48,545]: Epoch: 186, Loss:0.0879 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:48,553]: Epoch: 187, Loss:0.0594 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,561]: Epoch: 188, Loss:0.0830 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:48,570]: Epoch: 189, Loss:0.0885 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:48,577]: Epoch: 190, Loss:0.0747 Train: 0.9833, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0073
[2025-04-01 02:43:48,585]: Epoch: 191, Loss:0.0656 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:48,592]: Epoch: 192, Loss:0.1091 Train: 0.9917, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:48,600]: Epoch: 193, Loss:0.0731 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:48,608]: Epoch: 194, Loss:0.0904 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:48,616]: Epoch: 195, Loss:0.0724 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,624]: Epoch: 196, Loss:0.0828 Train: 1.0000, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,631]: Epoch: 197, Loss:0.0792 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:48,640]: Epoch: 198, Loss:0.0641 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:48,646]: Epoch: 199, Loss:0.0938 Train: 0.9917, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:48,653]: Epoch: 200, Loss:0.0820 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:48,654]: [Run-1 score] {'train': 0.9666666666666667, 'val': 0.7, 'test': 0.7058823529411765}
[2025-04-01 02:43:48,654]: repeat 2/3
[2025-04-01 02:43:48,654]: Manual random seed:0
[2025-04-01 02:43:48,654]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:48,658]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:48,669]: Epoch: 001, Loss:1.6241 Train: 0.6417, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:43:48,677]: Epoch: 002, Loss:1.0271 Train: 0.7667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:48,684]: Epoch: 003, Loss:0.6196 Train: 0.8667, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:48,690]: Epoch: 004, Loss:0.5607 Train: 0.8583, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0054
[2025-04-01 02:43:48,697]: Epoch: 005, Loss:0.4487 Train: 0.8583, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:48,705]: Epoch: 006, Loss:0.4647 Train: 0.8917, Val:0.6500, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:48,714]: Epoch: 007, Loss:0.3986 Train: 0.9250, Val:0.6625, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:43:48,722]: Epoch: 008, Loss:0.3259 Train: 0.9167, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:48,729]: Epoch: 009, Loss:0.3405 Train: 0.9083, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:48,736]: Epoch: 010, Loss:0.3215 Train: 0.9250, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:48,743]: Epoch: 011, Loss:0.2024 Train: 0.9417, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:48,750]: Epoch: 012, Loss:0.1887 Train: 0.9500, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:48,759]: Epoch: 013, Loss:0.1882 Train: 0.9500, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0090
[2025-04-01 02:43:48,766]: Epoch: 014, Loss:0.1701 Train: 0.9583, Val:0.6000, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:43:48,774]: Epoch: 015, Loss:0.1971 Train: 0.9667, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0081
[2025-04-01 02:43:48,780]: Epoch: 016, Loss:0.2044 Train: 0.9500, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0058
[2025-04-01 02:43:48,789]: Epoch: 017, Loss:0.2184 Train: 0.9667, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0090
[2025-04-01 02:43:48,797]: Epoch: 018, Loss:0.1713 Train: 0.9667, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:48,805]: Epoch: 019, Loss:0.1599 Train: 0.9667, Val:0.6750, Test: 0.7451, Time(s/epoch):0.0084
[2025-04-01 02:43:48,814]: Epoch: 020, Loss:0.1810 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:48,821]: Epoch: 021, Loss:0.1392 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:48,828]: Epoch: 022, Loss:0.1874 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:43:48,837]: Epoch: 023, Loss:0.1555 Train: 0.9667, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0088
[2025-04-01 02:43:48,845]: Epoch: 024, Loss:0.1751 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:48,852]: Epoch: 025, Loss:0.1504 Train: 0.9583, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0067
[2025-04-01 02:43:48,861]: Epoch: 026, Loss:0.1220 Train: 0.9750, Val:0.7000, Test: 0.7451, Time(s/epoch):0.0090
[2025-04-01 02:43:48,868]: Epoch: 027, Loss:0.1257 Train: 0.9667, Val:0.6875, Test: 0.7451, Time(s/epoch):0.0067
[2025-04-01 02:43:48,876]: Epoch: 028, Loss:0.1303 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:48,885]: Epoch: 029, Loss:0.1391 Train: 1.0000, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:48,893]: Epoch: 030, Loss:0.1134 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:48,901]: Epoch: 031, Loss:0.1055 Train: 0.9667, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:43:48,908]: Epoch: 032, Loss:0.1371 Train: 0.9750, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:43:48,917]: Epoch: 033, Loss:0.1494 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:48,925]: Epoch: 034, Loss:0.1454 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:48,932]: Epoch: 035, Loss:0.2323 Train: 0.9750, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:48,939]: Epoch: 036, Loss:0.1244 Train: 0.9750, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:48,947]: Epoch: 037, Loss:0.1240 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:48,955]: Epoch: 038, Loss:0.1053 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,963]: Epoch: 039, Loss:0.1202 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:48,971]: Epoch: 040, Loss:0.1101 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,979]: Epoch: 041, Loss:0.1721 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:48,988]: Epoch: 042, Loss:0.1471 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:48,996]: Epoch: 043, Loss:0.1188 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:49,003]: Epoch: 044, Loss:0.1083 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0072
[2025-04-01 02:43:49,012]: Epoch: 045, Loss:0.1216 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0085
[2025-04-01 02:43:49,019]: Epoch: 046, Loss:0.1238 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:49,027]: Epoch: 047, Loss:0.0988 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:49,035]: Epoch: 048, Loss:0.1064 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:49,042]: Epoch: 049, Loss:0.1299 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:49,050]: Epoch: 050, Loss:0.1255 Train: 1.0000, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:49,058]: Epoch: 051, Loss:0.1152 Train: 1.0000, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:49,066]: Epoch: 052, Loss:0.1094 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:49,074]: Epoch: 053, Loss:0.1179 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:49,087]: Epoch: 054, Loss:0.1285 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0132
[2025-04-01 02:43:49,097]: Epoch: 055, Loss:0.0992 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0094
[2025-04-01 02:43:49,104]: Epoch: 056, Loss:0.1111 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,111]: Epoch: 057, Loss:0.1117 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:49,120]: Epoch: 058, Loss:0.1089 Train: 1.0000, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:49,128]: Epoch: 059, Loss:0.1215 Train: 1.0000, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:49,136]: Epoch: 060, Loss:0.1019 Train: 0.9833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:49,144]: Epoch: 061, Loss:0.1395 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:49,153]: Epoch: 062, Loss:0.0977 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:49,160]: Epoch: 063, Loss:0.0968 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:49,168]: Epoch: 064, Loss:0.1225 Train: 1.0000, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:49,176]: Epoch: 065, Loss:0.0985 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:49,186]: Epoch: 066, Loss:0.1237 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0097
[2025-04-01 02:43:49,195]: Epoch: 067, Loss:0.0924 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:49,201]: Epoch: 068, Loss:0.1051 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:49,209]: Epoch: 069, Loss:0.1004 Train: 0.9917, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:49,217]: Epoch: 070, Loss:0.0969 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:49,225]: Epoch: 071, Loss:0.1221 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:49,236]: Epoch: 072, Loss:0.0867 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0104
[2025-04-01 02:43:49,245]: Epoch: 073, Loss:0.1250 Train: 0.9833, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:49,254]: Epoch: 074, Loss:0.0898 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:49,264]: Epoch: 075, Loss:0.1220 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0101
[2025-04-01 02:43:49,270]: Epoch: 076, Loss:0.1094 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0057
[2025-04-01 02:43:49,279]: Epoch: 077, Loss:0.0969 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:49,288]: Epoch: 078, Loss:0.1290 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0089
[2025-04-01 02:43:49,298]: Epoch: 079, Loss:0.1015 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0097
[2025-04-01 02:43:49,307]: Epoch: 080, Loss:0.0773 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:49,343]: Epoch: 081, Loss:0.0641 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0359
[2025-04-01 02:43:49,355]: Epoch: 082, Loss:0.1159 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0127
[2025-04-01 02:43:49,364]: Epoch: 083, Loss:0.0995 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0087
[2025-04-01 02:43:49,373]: Epoch: 084, Loss:0.0829 Train: 0.9750, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:49,382]: Epoch: 085, Loss:0.0909 Train: 0.9917, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0094
[2025-04-01 02:43:49,392]: Epoch: 086, Loss:0.0908 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0096
[2025-04-01 02:43:49,402]: Epoch: 087, Loss:0.0875 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0100
[2025-04-01 02:43:49,411]: Epoch: 088, Loss:0.0970 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:49,418]: Epoch: 089, Loss:0.1369 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:49,426]: Epoch: 090, Loss:0.1237 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:49,434]: Epoch: 091, Loss:0.0701 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:49,444]: Epoch: 092, Loss:0.1214 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0093
[2025-04-01 02:43:49,450]: Epoch: 093, Loss:0.1128 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:49,457]: Epoch: 094, Loss:0.1217 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:49,466]: Epoch: 095, Loss:0.1181 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:49,474]: Epoch: 096, Loss:0.1012 Train: 1.0000, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:49,482]: Epoch: 097, Loss:0.0900 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:49,488]: Epoch: 098, Loss:0.0811 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:49,497]: Epoch: 099, Loss:0.1029 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:49,505]: Epoch: 100, Loss:0.0867 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:49,515]: Epoch: 101, Loss:0.0821 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0094
[2025-04-01 02:43:49,523]: Epoch: 102, Loss:0.1016 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:49,531]: Epoch: 103, Loss:0.0990 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:49,539]: Epoch: 104, Loss:0.0912 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:49,548]: Epoch: 105, Loss:0.0847 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:49,555]: Epoch: 106, Loss:0.0783 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,564]: Epoch: 107, Loss:0.0937 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:49,571]: Epoch: 108, Loss:0.0830 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:49,578]: Epoch: 109, Loss:0.0759 Train: 1.0000, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:49,586]: Epoch: 110, Loss:0.0651 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:49,594]: Epoch: 111, Loss:0.0815 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:49,600]: Epoch: 112, Loss:0.1090 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:49,608]: Epoch: 113, Loss:0.0904 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:49,614]: Epoch: 114, Loss:0.1271 Train: 0.9917, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:49,621]: Epoch: 115, Loss:0.0776 Train: 0.9750, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:49,629]: Epoch: 116, Loss:0.0803 Train: 0.9833, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:49,637]: Epoch: 117, Loss:0.1089 Train: 1.0000, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:49,644]: Epoch: 118, Loss:0.1129 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:49,652]: Epoch: 119, Loss:0.1060 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,660]: Epoch: 120, Loss:0.0908 Train: 1.0000, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:49,667]: Epoch: 121, Loss:0.0943 Train: 0.9917, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:49,674]: Epoch: 122, Loss:0.0924 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:49,682]: Epoch: 123, Loss:0.0997 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:49,689]: Epoch: 124, Loss:0.0914 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,697]: Epoch: 125, Loss:0.0938 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:49,706]: Epoch: 126, Loss:0.1073 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:49,714]: Epoch: 127, Loss:0.0762 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:49,720]: Epoch: 128, Loss:0.0887 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0060
[2025-04-01 02:43:49,727]: Epoch: 129, Loss:0.0995 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:49,734]: Epoch: 130, Loss:0.0874 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,741]: Epoch: 131, Loss:0.0993 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:49,749]: Epoch: 132, Loss:0.1002 Train: 1.0000, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:49,757]: Epoch: 133, Loss:0.0810 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:49,764]: Epoch: 134, Loss:0.0954 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:49,770]: Epoch: 135, Loss:0.1269 Train: 0.9833, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:49,777]: Epoch: 136, Loss:0.0888 Train: 0.9917, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:49,787]: Epoch: 137, Loss:0.1152 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0094
[2025-04-01 02:43:49,795]: Epoch: 138, Loss:0.1065 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:49,802]: Epoch: 139, Loss:0.0892 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:49,811]: Epoch: 140, Loss:0.0641 Train: 1.0000, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:49,819]: Epoch: 141, Loss:0.1041 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:49,827]: Epoch: 142, Loss:0.0835 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:49,833]: Epoch: 143, Loss:0.1026 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:49,839]: Epoch: 144, Loss:0.0962 Train: 0.9917, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:49,848]: Epoch: 145, Loss:0.0797 Train: 0.9917, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:49,856]: Epoch: 146, Loss:0.1480 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:49,865]: Epoch: 147, Loss:0.0960 Train: 0.9833, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:49,873]: Epoch: 148, Loss:0.1089 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:49,881]: Epoch: 149, Loss:0.0914 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:49,889]: Epoch: 150, Loss:0.1342 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:49,898]: Epoch: 151, Loss:0.0745 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:49,906]: Epoch: 152, Loss:0.0770 Train: 0.9833, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:49,914]: Epoch: 153, Loss:0.0909 Train: 0.9833, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:49,924]: Epoch: 154, Loss:0.1264 Train: 0.9917, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0095
[2025-04-01 02:43:49,932]: Epoch: 155, Loss:0.0770 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:49,940]: Epoch: 156, Loss:0.1063 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:49,948]: Epoch: 157, Loss:0.0817 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:49,957]: Epoch: 158, Loss:0.0836 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:49,966]: Epoch: 159, Loss:0.0785 Train: 1.0000, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:49,974]: Epoch: 160, Loss:0.0998 Train: 0.9917, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:49,980]: Epoch: 161, Loss:0.0935 Train: 0.9917, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0059
[2025-04-01 02:43:49,989]: Epoch: 162, Loss:0.1188 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:49,996]: Epoch: 163, Loss:0.0916 Train: 0.9667, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:50,005]: Epoch: 164, Loss:0.1069 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:50,010]: Epoch: 165, Loss:0.0651 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0055
[2025-04-01 02:43:50,017]: Epoch: 166, Loss:0.0703 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:50,025]: Epoch: 167, Loss:0.0514 Train: 0.9833, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:50,032]: Epoch: 168, Loss:0.1242 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0064
[2025-04-01 02:43:50,038]: Epoch: 169, Loss:0.1159 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:50,045]: Epoch: 170, Loss:0.0924 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:50,052]: Epoch: 171, Loss:0.1156 Train: 0.9833, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,059]: Epoch: 172, Loss:0.0769 Train: 1.0000, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:50,067]: Epoch: 173, Loss:0.0789 Train: 1.0000, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:50,074]: Epoch: 174, Loss:0.0737 Train: 0.9917, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:50,081]: Epoch: 175, Loss:0.1032 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:50,088]: Epoch: 176, Loss:0.0977 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,096]: Epoch: 177, Loss:0.0724 Train: 1.0000, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:50,104]: Epoch: 178, Loss:0.0591 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:50,110]: Epoch: 179, Loss:0.0568 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:50,119]: Epoch: 180, Loss:0.1093 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:50,127]: Epoch: 181, Loss:0.0718 Train: 0.9833, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0081
[2025-04-01 02:43:50,134]: Epoch: 182, Loss:0.0895 Train: 1.0000, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:50,141]: Epoch: 183, Loss:0.1066 Train: 1.0000, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:50,148]: Epoch: 184, Loss:0.0628 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:50,154]: Epoch: 185, Loss:0.0880 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:50,161]: Epoch: 186, Loss:0.1262 Train: 1.0000, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:50,169]: Epoch: 187, Loss:0.0607 Train: 0.9917, Val:0.7125, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:50,176]: Epoch: 188, Loss:0.1207 Train: 0.9917, Val:0.7000, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:50,185]: Epoch: 189, Loss:0.0866 Train: 0.9750, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0085
[2025-04-01 02:43:50,194]: Epoch: 190, Loss:0.1082 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:50,200]: Epoch: 191, Loss:0.1101 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:50,207]: Epoch: 192, Loss:0.0856 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:50,214]: Epoch: 193, Loss:0.0854 Train: 0.9833, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0067
[2025-04-01 02:43:50,221]: Epoch: 194, Loss:0.0859 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:50,230]: Epoch: 195, Loss:0.0967 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:50,239]: Epoch: 196, Loss:0.0984 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0093
[2025-04-01 02:43:50,248]: Epoch: 197, Loss:0.0908 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:50,255]: Epoch: 198, Loss:0.0809 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:50,264]: Epoch: 199, Loss:0.0765 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:50,269]: Epoch: 200, Loss:0.0967 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:50,270]: [Run-2 score] {'train': 0.9916666666666667, 'val': 0.7125, 'test': 0.7254901960784313}
[2025-04-01 02:43:50,270]: repeat 3/3
[2025-04-01 02:43:50,270]: Manual random seed:0
[2025-04-01 02:43:50,270]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:50,275]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:50,286]: Epoch: 001, Loss:1.7219 Train: 0.7000, Val:0.5625, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:43:50,294]: Epoch: 002, Loss:1.1483 Train: 0.7583, Val:0.6125, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:43:50,300]: Epoch: 003, Loss:0.8242 Train: 0.8167, Val:0.6000, Test: 0.5490, Time(s/epoch):0.0055
[2025-04-01 02:43:50,306]: Epoch: 004, Loss:0.5666 Train: 0.8333, Val:0.6375, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:43:50,314]: Epoch: 005, Loss:0.5632 Train: 0.8667, Val:0.6500, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:43:50,320]: Epoch: 006, Loss:0.4029 Train: 0.8833, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0055
[2025-04-01 02:43:50,327]: Epoch: 007, Loss:0.3185 Train: 0.8833, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0072
[2025-04-01 02:43:50,335]: Epoch: 008, Loss:0.3219 Train: 0.8917, Val:0.6125, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:50,343]: Epoch: 009, Loss:0.3264 Train: 0.9167, Val:0.6125, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:50,350]: Epoch: 010, Loss:0.3254 Train: 0.9333, Val:0.6125, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:43:50,359]: Epoch: 011, Loss:0.2276 Train: 0.9417, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0093
[2025-04-01 02:43:50,367]: Epoch: 012, Loss:0.2989 Train: 0.9500, Val:0.5875, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:43:50,375]: Epoch: 013, Loss:0.1846 Train: 0.9500, Val:0.6000, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:43:50,383]: Epoch: 014, Loss:0.2471 Train: 0.9500, Val:0.6250, Test: 0.6078, Time(s/epoch):0.0082
[2025-04-01 02:43:50,390]: Epoch: 015, Loss:0.2634 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:50,397]: Epoch: 016, Loss:0.2259 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,405]: Epoch: 017, Loss:0.1677 Train: 0.9583, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:50,413]: Epoch: 018, Loss:0.1928 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0077
[2025-04-01 02:43:50,421]: Epoch: 019, Loss:0.1796 Train: 0.9667, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:43:50,427]: Epoch: 020, Loss:0.1956 Train: 0.9583, Val:0.6250, Test: 0.6275, Time(s/epoch):0.0061
[2025-04-01 02:43:50,434]: Epoch: 021, Loss:0.1749 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:50,443]: Epoch: 022, Loss:0.1459 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:50,450]: Epoch: 023, Loss:0.1271 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:50,459]: Epoch: 024, Loss:0.1572 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:50,466]: Epoch: 025, Loss:0.1608 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0070
[2025-04-01 02:43:50,474]: Epoch: 026, Loss:0.1260 Train: 0.9583, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:50,480]: Epoch: 027, Loss:0.1320 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0057
[2025-04-01 02:43:50,488]: Epoch: 028, Loss:0.1459 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:50,496]: Epoch: 029, Loss:0.1384 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:50,503]: Epoch: 030, Loss:0.1192 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:50,510]: Epoch: 031, Loss:0.1230 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:50,517]: Epoch: 032, Loss:0.1213 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0067
[2025-04-01 02:43:50,524]: Epoch: 033, Loss:0.1512 Train: 0.9833, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:50,531]: Epoch: 034, Loss:0.1743 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:50,539]: Epoch: 035, Loss:0.1293 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:50,547]: Epoch: 036, Loss:0.1327 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:50,555]: Epoch: 037, Loss:0.1093 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:50,563]: Epoch: 038, Loss:0.0984 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:50,571]: Epoch: 039, Loss:0.0990 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:50,578]: Epoch: 040, Loss:0.1183 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:50,586]: Epoch: 041, Loss:0.0885 Train: 1.0000, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:50,595]: Epoch: 042, Loss:0.1164 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:50,601]: Epoch: 043, Loss:0.1062 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:50,609]: Epoch: 044, Loss:0.1561 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:50,618]: Epoch: 045, Loss:0.1618 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:50,626]: Epoch: 046, Loss:0.1105 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:50,634]: Epoch: 047, Loss:0.1440 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:50,643]: Epoch: 048, Loss:0.1363 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:50,650]: Epoch: 049, Loss:0.1137 Train: 0.9750, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:50,657]: Epoch: 050, Loss:0.1285 Train: 0.9833, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0074
[2025-04-01 02:43:50,664]: Epoch: 051, Loss:0.1155 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:50,671]: Epoch: 052, Loss:0.1046 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:50,679]: Epoch: 053, Loss:0.1412 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:50,687]: Epoch: 054, Loss:0.1044 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:50,693]: Epoch: 055, Loss:0.1785 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:50,700]: Epoch: 056, Loss:0.1306 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:50,709]: Epoch: 057, Loss:0.0784 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:50,717]: Epoch: 058, Loss:0.1509 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:50,723]: Epoch: 059, Loss:0.1495 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:50,730]: Epoch: 060, Loss:0.1240 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:43:50,737]: Epoch: 061, Loss:0.1209 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:50,744]: Epoch: 062, Loss:0.1419 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:50,750]: Epoch: 063, Loss:0.1129 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:50,758]: Epoch: 064, Loss:0.1105 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:50,766]: Epoch: 065, Loss:0.0845 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:50,772]: Epoch: 066, Loss:0.1029 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:43:50,778]: Epoch: 067, Loss:0.1262 Train: 0.9917, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:50,787]: Epoch: 068, Loss:0.1080 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:50,794]: Epoch: 069, Loss:0.0916 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0065
[2025-04-01 02:43:50,800]: Epoch: 070, Loss:0.1167 Train: 0.9917, Val:0.6125, Test: 0.6471, Time(s/epoch):0.0060
[2025-04-01 02:43:50,807]: Epoch: 071, Loss:0.0984 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0070
[2025-04-01 02:43:50,814]: Epoch: 072, Loss:0.0883 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:50,821]: Epoch: 073, Loss:0.1296 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:50,829]: Epoch: 074, Loss:0.1308 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:50,836]: Epoch: 075, Loss:0.1317 Train: 0.9917, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0064
[2025-04-01 02:43:50,844]: Epoch: 076, Loss:0.0804 Train: 0.9833, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:50,852]: Epoch: 077, Loss:0.0993 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:50,860]: Epoch: 078, Loss:0.0688 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:50,867]: Epoch: 079, Loss:0.1186 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,874]: Epoch: 080, Loss:0.1189 Train: 0.9750, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0068
[2025-04-01 02:43:50,880]: Epoch: 081, Loss:0.1033 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:50,888]: Epoch: 082, Loss:0.1902 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:50,897]: Epoch: 083, Loss:0.1020 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:50,903]: Epoch: 084, Loss:0.0918 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:50,912]: Epoch: 085, Loss:0.1372 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:50,919]: Epoch: 086, Loss:0.1184 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:50,926]: Epoch: 087, Loss:0.1247 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,933]: Epoch: 088, Loss:0.1117 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:50,941]: Epoch: 089, Loss:0.1350 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:50,949]: Epoch: 090, Loss:0.1053 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:50,957]: Epoch: 091, Loss:0.1189 Train: 0.9750, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:50,967]: Epoch: 092, Loss:0.1388 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0101
[2025-04-01 02:43:50,988]: Epoch: 093, Loss:0.1171 Train: 0.9750, Val:0.7000, Test: 0.6863, Time(s/epoch):0.0202
[2025-04-01 02:43:51,001]: Epoch: 094, Loss:0.1251 Train: 0.9917, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0133
[2025-04-01 02:43:51,011]: Epoch: 095, Loss:0.0820 Train: 0.9917, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0094
[2025-04-01 02:43:51,023]: Epoch: 096, Loss:0.0933 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0121
[2025-04-01 02:43:51,031]: Epoch: 097, Loss:0.1018 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:51,041]: Epoch: 098, Loss:0.1039 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0091
[2025-04-01 02:43:51,049]: Epoch: 099, Loss:0.1028 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:51,058]: Epoch: 100, Loss:0.0875 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:51,065]: Epoch: 101, Loss:0.1390 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:51,072]: Epoch: 102, Loss:0.1116 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:51,079]: Epoch: 103, Loss:0.1080 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:51,086]: Epoch: 104, Loss:0.1435 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:51,094]: Epoch: 105, Loss:0.0924 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:51,101]: Epoch: 106, Loss:0.1197 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:51,108]: Epoch: 107, Loss:0.1033 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:51,116]: Epoch: 108, Loss:0.0809 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:51,123]: Epoch: 109, Loss:0.1029 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:51,131]: Epoch: 110, Loss:0.1178 Train: 0.9833, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0076
[2025-04-01 02:43:51,139]: Epoch: 111, Loss:0.1269 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:51,147]: Epoch: 112, Loss:0.1148 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:51,155]: Epoch: 113, Loss:0.0915 Train: 0.9750, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:51,161]: Epoch: 114, Loss:0.1108 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:51,170]: Epoch: 115, Loss:0.1041 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:51,178]: Epoch: 116, Loss:0.0891 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:51,185]: Epoch: 117, Loss:0.0966 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:51,192]: Epoch: 118, Loss:0.1064 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:51,200]: Epoch: 119, Loss:0.0996 Train: 0.9833, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:51,208]: Epoch: 120, Loss:0.0949 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:51,216]: Epoch: 121, Loss:0.0708 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:51,223]: Epoch: 122, Loss:0.0980 Train: 0.9833, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:51,232]: Epoch: 123, Loss:0.0954 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:51,241]: Epoch: 124, Loss:0.0948 Train: 0.9917, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:51,248]: Epoch: 125, Loss:0.0813 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:51,256]: Epoch: 126, Loss:0.1066 Train: 0.9917, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:51,264]: Epoch: 127, Loss:0.0824 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:51,272]: Epoch: 128, Loss:0.0970 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:51,280]: Epoch: 129, Loss:0.1024 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:51,287]: Epoch: 130, Loss:0.0879 Train: 1.0000, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:51,295]: Epoch: 131, Loss:0.0869 Train: 0.9917, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:51,302]: Epoch: 132, Loss:0.0744 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:51,308]: Epoch: 133, Loss:0.1099 Train: 1.0000, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:51,315]: Epoch: 134, Loss:0.1088 Train: 0.9917, Val:0.6125, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:51,322]: Epoch: 135, Loss:0.0874 Train: 0.9917, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:51,328]: Epoch: 136, Loss:0.1092 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:51,336]: Epoch: 137, Loss:0.1027 Train: 0.9917, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:51,344]: Epoch: 138, Loss:0.1072 Train: 0.9917, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:51,351]: Epoch: 139, Loss:0.1236 Train: 0.9750, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0067
[2025-04-01 02:43:51,359]: Epoch: 140, Loss:0.0839 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0083
[2025-04-01 02:43:51,368]: Epoch: 141, Loss:0.1021 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0088
[2025-04-01 02:43:51,377]: Epoch: 142, Loss:0.0956 Train: 0.9833, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0086
[2025-04-01 02:43:51,384]: Epoch: 143, Loss:0.0894 Train: 0.9917, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0075
[2025-04-01 02:43:51,393]: Epoch: 144, Loss:0.0760 Train: 0.9833, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:51,401]: Epoch: 145, Loss:0.0746 Train: 0.9917, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:51,409]: Epoch: 146, Loss:0.0892 Train: 1.0000, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:51,418]: Epoch: 147, Loss:0.1200 Train: 1.0000, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:51,427]: Epoch: 148, Loss:0.0646 Train: 0.9917, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:51,436]: Epoch: 149, Loss:0.0998 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0091
[2025-04-01 02:43:51,444]: Epoch: 150, Loss:0.0801 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:51,452]: Epoch: 151, Loss:0.1084 Train: 0.9833, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:51,462]: Epoch: 152, Loss:0.0943 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:51,470]: Epoch: 153, Loss:0.0866 Train: 0.9833, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:51,480]: Epoch: 154, Loss:0.0878 Train: 0.9833, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0093
[2025-04-01 02:43:51,488]: Epoch: 155, Loss:0.1114 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:51,495]: Epoch: 156, Loss:0.0832 Train: 0.9917, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:43:51,503]: Epoch: 157, Loss:0.0732 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:51,511]: Epoch: 158, Loss:0.0887 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:51,518]: Epoch: 159, Loss:0.0962 Train: 0.9833, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:51,526]: Epoch: 160, Loss:0.0859 Train: 0.9917, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:51,533]: Epoch: 161, Loss:0.0843 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:51,540]: Epoch: 162, Loss:0.0842 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:51,548]: Epoch: 163, Loss:0.0885 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:51,556]: Epoch: 164, Loss:0.1263 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:51,565]: Epoch: 165, Loss:0.0985 Train: 0.9833, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:51,573]: Epoch: 166, Loss:0.1172 Train: 0.9917, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:51,581]: Epoch: 167, Loss:0.0892 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:51,589]: Epoch: 168, Loss:0.1184 Train: 0.9917, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:51,597]: Epoch: 169, Loss:0.0786 Train: 0.9917, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:51,605]: Epoch: 170, Loss:0.1230 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:51,614]: Epoch: 171, Loss:0.1826 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:51,622]: Epoch: 172, Loss:0.1071 Train: 1.0000, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:51,628]: Epoch: 173, Loss:0.0823 Train: 1.0000, Val:0.7125, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:51,636]: Epoch: 174, Loss:0.0856 Train: 0.9833, Val:0.7125, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:51,644]: Epoch: 175, Loss:0.1003 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:51,652]: Epoch: 176, Loss:0.0969 Train: 0.9917, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:51,661]: Epoch: 177, Loss:0.0962 Train: 0.9917, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0084
[2025-04-01 02:43:51,667]: Epoch: 178, Loss:0.1079 Train: 0.9667, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0063
[2025-04-01 02:43:51,674]: Epoch: 179, Loss:0.0940 Train: 1.0000, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:51,681]: Epoch: 180, Loss:0.1082 Train: 0.9917, Val:0.6875, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:51,689]: Epoch: 181, Loss:0.0836 Train: 0.9750, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:51,697]: Epoch: 182, Loss:0.0957 Train: 0.9833, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:51,705]: Epoch: 183, Loss:0.0938 Train: 0.9917, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:51,714]: Epoch: 184, Loss:0.0865 Train: 0.9917, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0086
[2025-04-01 02:43:51,722]: Epoch: 185, Loss:0.1535 Train: 0.9917, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:51,728]: Epoch: 186, Loss:0.0754 Train: 0.9833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:51,736]: Epoch: 187, Loss:0.1110 Train: 0.9833, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:51,744]: Epoch: 188, Loss:0.1201 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:51,752]: Epoch: 189, Loss:0.1013 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:51,759]: Epoch: 190, Loss:0.0943 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:51,767]: Epoch: 191, Loss:0.1001 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:51,775]: Epoch: 192, Loss:0.0835 Train: 0.9833, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:51,783]: Epoch: 193, Loss:0.1018 Train: 0.9833, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:51,790]: Epoch: 194, Loss:0.0815 Train: 0.9917, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:51,799]: Epoch: 195, Loss:0.1527 Train: 0.9917, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:51,807]: Epoch: 196, Loss:0.1148 Train: 0.9917, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:51,815]: Epoch: 197, Loss:0.0849 Train: 0.9833, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:51,823]: Epoch: 198, Loss:0.1258 Train: 0.9750, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:51,829]: Epoch: 199, Loss:0.0939 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:51,838]: Epoch: 200, Loss:0.1220 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:51,838]: [Run-3 score] {'train': 1.0, 'val': 0.7125, 'test': 0.6862745098039216}
[2025-04-01 02:43:51,838]: repeat 1/3
[2025-04-01 02:43:51,838]: Manual random seed:0
[2025-04-01 02:43:51,838]: auto fixed data split seed to 0, model init seed to 0

Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0089
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0096
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0091
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0078
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0082
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0102
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0084
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0084
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0102
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0109
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0085
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0089
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0084
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0094
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0099
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0075
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0080
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0081
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0077
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0076
Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0072
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0075
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0080
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0102
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0096
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0080
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0100
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0100
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0079
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0088
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0100
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0100
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0100
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0085
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0094
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0100
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0099
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0100
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0100
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0109
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0100
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0083
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0092
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0103
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0085
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0089
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0078
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0077
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0086
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0089
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0094
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0081
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0075
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0077
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0081
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0084
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.6454 Time(s/epoch):0.0053
Epoch: 002, Supervised Loss:1.5919 Time(s/epoch):0.0056
Epoch: 003, Supervised Loss:1.5536 Time(s/epoch):0.0065
Epoch: 004, Supervised Loss:1.5711 Time(s/epoch):0.0054
Epoch: 005, Supervised Loss:1.6228 Time(s/epoch):0.0057
Epoch: 006, Supervised Loss:1.6664 Time(s/epoch):0.0050
Epoch: 007, Supervised Loss:1.6887 Time(s/epoch):0.0056
Epoch: 008, Supervised Loss:1.6927 Time(s/epoch):0.0055
Epoch: 009, Supervised Loss:1.6841 Time(s/epoch):0.0049
Epoch: 010, Supervised Loss:1.6698 Time(s/epoch):0.0053
Epoch: 011, Supervised Loss:1.6557 Time(s/epoch):0.0054
Epoch: 012, Supervised Loss:1.6448 Time(s/epoch):0.0049
Epoch: 013, Supervised Loss:1.6373 Time(s/epoch):0.0051
Epoch: 014, Supervised Loss:1.6322 Time(s/epoch):0.0053
Epoch: 015, Supervised Loss:1.6286 Time(s/epoch):0.0050
Epoch: 016, Supervised Loss:1.6253 Time(s/epoch):0.0050
Epoch: 017, Supervised Loss:1.6216 Time(s/epoch):0.0050
Epoch: 018, Supervised Loss:1.6176 Time(s/epoch):0.0048
Epoch: 019, Supervised Loss:1.6136 Time(s/epoch):0.0050
Epoch: 020, Supervised Loss:1.6103 Time(s/epoch):0.0051
Epoch: 021, Supervised Loss:1.6077 Time(s/epoch):0.0068
Epoch: 022, Supervised Loss:1.6058 Time(s/epoch):0.0066
Epoch: 023, Supervised Loss:1.6039 Time(s/epoch):0.0066
Epoch: 024, Supervised Loss:1.6016 Time(s/epoch):0.0064
Epoch: 025, Supervised Loss:1.5988 Time(s/epoch):0.0061
Epoch: 026, Supervised Loss:1.5959 Time(s/epoch):0.0066
Epoch: 027, Supervised Loss:1.5938 Time(s/epoch):0.0063
Epoch: 028, Supervised Loss:1.5929 Time(s/epoch):0.0063
Epoch: 029, Supervised Loss:1.5931 Time(s/epoch):0.0062
Epoch: 030, Supervised Loss:1.5940 Time(s/epoch):0.0064
0.8427247 515
Add 426 edges.
Prune 348 edges from torch.Size([2, 515]) to torch.Size([2, 167])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 582], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.5755 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 582], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.6128 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.739
Data(x=[251, 1703], edge_index=[2, 582], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5646 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
preparing label similarity matrix.
candidate set size: tensor(30)
preparing feature similarity matrix.
cpu cpu cpu
Epoch: 001, Reconstruction Loss:0.1114 Time(s/epoch):0.0131
Epoch: 002, Reconstruction Loss:0.1162 Time(s/epoch):0.0086
Epoch: 003, Reconstruction Loss:0.0974 Time(s/epoch):0.0097
Epoch: 004, Reconstruction Loss:0.1007 Time(s/epoch):0.0080
Epoch: 005, Reconstruction Loss:0.0944 Time(s/epoch):0.0090
Epoch: 006, Reconstruction Loss:0.1040 Time(s/epoch):0.0097
Epoch: 007, Reconstruction Loss:0.1223 Time(s/epoch):0.0096
Epoch: 008, Reconstruction Loss:0.1201 Time(s/epoch):0.0070
Epoch: 009, Reconstruction Loss:0.0991 Time(s/epoch):0.0070
Epoch: 010, Reconstruction Loss:0.0760 Time(s/epoch):0.0087
Epoch: 011, Reconstruction Loss:0.0756 Time(s/epoch):0.0089
Epoch: 012, Reconstruction Loss:0.0876 Time(s/epoch):0.0072
Epoch: 013, Reconstruction Loss:0.0961 Time(s/epoch):0.0078
Epoch: 014, Reconstruction Loss:0.1101 Time(s/epoch):0.0087
Epoch: 015, Reconstruction Loss:0.1290 Time(s/epoch):0.0091
Epoch: 016, Reconstruction Loss:0.1436 Time(s/epoch):0.0099
Epoch: 017, Reconstruction Loss:0.1476 Time(s/epoch):0.0102
Epoch: 018, Reconstruction Loss:0.1399 Time(s/epoch):0.0102
Epoch: 019, Reconstruction Loss:0.1232 Time(s/epoch):0.0100
Epoch: 020, Reconstruction Loss:0.1069 Time(s/epoch):0.0102
Epoch: 021, Reconstruction Loss:0.1068 Time(s/epoch):0.0093
Epoch: 022, Reconstruction Loss:0.1227 Time(s/epoch):0.0075
Epoch: 023, Reconstruction Loss:0.1351 Time(s/epoch):0.0075
Epoch: 024, Reconstruction Loss:0.1337 Time(s/epoch):0.0073
Epoch: 025, Reconstruction Loss:0.1232 Time(s/epoch):0.0075
Epoch: 026, Reconstruction Loss:0.1176 Time(s/epoch):0.0071
Epoch: 027, Reconstruction Loss:0.1266 Time(s/epoch):0.0075
Epoch: 028, Reconstruction Loss:0.1437 Time(s/epoch):0.0097
Epoch: 029, Reconstruction Loss:0.1599 Time(s/epoch):0.0100
Epoch: 030, Reconstruction Loss:0.1705 Time(s/epoch):0.0094
Epoch: 031, Reconstruction Loss:0.1734 Time(s/epoch):0.0095
Epoch: 032, Reconstruction Loss:0.1687 Time(s/epoch):0.0084
Epoch: 033, Reconstruction Loss:0.1571 Time(s/epoch):0.0074
Epoch: 034, Reconstruction Loss:0.1407 Time(s/epoch):0.0073
Epoch: 035, Reconstruction Loss:0.1230 Time(s/epoch):0.0094
Epoch: 036, Reconstruction Loss:0.1102 Time(s/epoch):0.0097
Epoch: 037, Reconstruction Loss:0.1091 Time(s/epoch):0.0096
Epoch: 038, Reconstruction Loss:0.1185 Time(s/epoch):0.0097
Epoch: 039, Reconstruction Loss:0.1286 Time(s/epoch):0.0101
Epoch: 040, Reconstruction Loss:0.1318 Time(s/epoch):0.0100
Epoch: 041, Reconstruction Loss:0.1261 Time(s/epoch):0.0101
Epoch: 042, Reconstruction Loss:0.1149 Time(s/epoch):0.0101
Epoch: 043, Reconstruction Loss:0.1061 Time(s/epoch):0.0096
Epoch: 044, Reconstruction Loss:0.1086 Time(s/epoch):0.0101
Epoch: 045, Reconstruction Loss:0.1224 Time(s/epoch):0.0085
Epoch: 046, Reconstruction Loss:0.1393 Time(s/epoch):0.0091
Epoch: 047, Reconstruction Loss:0.1522 Time(s/epoch):0.0102
Epoch: 048, Reconstruction Loss:0.1579 Time(s/epoch):0.0075
Epoch: 049, Reconstruction Loss:0.1554 Time(s/epoch):0.0079
Epoch: 050, Reconstruction Loss:0.1451 Time(s/epoch):0.0102
Epoch: 051, Reconstruction Loss:0.1285 Time(s/epoch):0.0090
Epoch: 052, Reconstruction Loss:0.1090 Time(s/epoch):0.0077
Epoch: 053, Reconstruction Loss:0.0934 Time(s/epoch):0.0088
Epoch: 054, Reconstruction Loss:0.0921 Time(s/epoch):0.0094
Epoch: 055, Reconstruction Loss:0.1063 Time(s/epoch):0.0098
Epoch: 056, Reconstruction Loss:0.1243 Time(s/epoch):0.0098
Epoch: 057, Reconstruction Loss:0.1371 Time(s/epoch):0.0095
Epoch: 058, Reconstruction Loss:0.1418 Time(s/epoch):0.0084
Epoch: 059, Reconstruction Loss:0.1389 Time(s/epoch):0.0072
Epoch: 060, Reconstruction Loss:0.1295 Time(s/epoch):0.0083
Epoch: 061, Reconstruction Loss:0.1159 Time(s/epoch):0.0094
Epoch: 062, Reconstruction Loss:0.1021 Time(s/epoch):0.0096
Epoch: 063, Reconstruction Loss:0.0960 Time(s/epoch):0.0097
Epoch: 064, Reconstruction Loss:0.1040 Time(s/epoch):0.0097
Epoch: 065, Reconstruction Loss:0.1214 Time(s/epoch):0.0096
Epoch: 066, Reconstruction Loss:0.1388 Time(s/epoch):0.0094
Epoch: 067, Reconstruction Loss:0.1502 Time(s/epoch):0.0078
Epoch: 068, Reconstruction Loss:0.1535 Time(s/epoch):0.0076
Epoch: 069, Reconstruction Loss:0.1484 Time(s/epoch):0.0079
Epoch: 070, Reconstruction Loss:0.1356 Time(s/epoch):0.0077
Epoch: 071, Reconstruction Loss:0.1175 Time(s/epoch):0.0080
Epoch: 072, Reconstruction Loss:0.0993 Time(s/epoch):0.0078
Epoch: 073, Reconstruction Loss:0.0896 Time(s/epoch):0.0086
Epoch: 074, Reconstruction Loss:0.0940 Time(s/epoch):0.0094
Epoch: 075, Reconstruction Loss:0.1050 Time(s/epoch):0.0103
Epoch: 076, Reconstruction Loss:0.1131 Time(s/epoch):0.0091
Epoch: 077, Reconstruction Loss:0.1153 Time(s/epoch):0.0079
Epoch: 078, Reconstruction Loss:0.1132 Time(s/epoch):0.0078
Epoch: 079, Reconstruction Loss:0.1102 Time(s/epoch):0.0073
Epoch: 080, Reconstruction Loss:0.1096 Time(s/epoch):0.0091
Epoch: 081, Reconstruction Loss:0.1131 Time(s/epoch):0.0097
Epoch: 082, Reconstruction Loss:0.1202 Time(s/epoch):0.0097
Epoch: 083, Reconstruction Loss:0.1290 Time(s/epoch):0.0093
Epoch: 084, Reconstruction Loss:0.1364 Time(s/epoch):0.0077
Epoch: 085, Reconstruction Loss:0.1395 Time(s/epoch):0.0080
Epoch: 086, Reconstruction Loss:0.1362 Time(s/epoch):0.0074
Epoch: 087, Reconstruction Loss:0.1264 Time(s/epoch):0.0084
Epoch: 088, Reconstruction Loss:0.1122 Time(s/epoch):0.0097
Epoch: 089, Reconstruction Loss:0.0986 Time(s/epoch):0.0096
Epoch: 090, Reconstruction Loss:0.0935 Time(s/epoch):0.0092
Epoch: 091, Reconstruction Loss:0.1000 Time(s/epoch):0.0107
Epoch: 092, Reconstruction Loss:0.1108 Time(s/epoch):0.0106
Epoch: 093, Reconstruction Loss:0.1182 Time(s/epoch):0.0106
Epoch: 094, Reconstruction Loss:0.1194 Time(s/epoch):0.0102
Epoch: 095, Reconstruction Loss:0.1154 Time(s/epoch):0.0091
Epoch: 096, Reconstruction Loss:0.1091 Time(s/epoch):0.0085
Epoch: 097, Reconstruction Loss:0.1048 Time(s/epoch):0.0081
Epoch: 098, Reconstruction Loss:0.1067 Time(s/epoch):0.0080
Epoch: 099, Reconstruction Loss:0.1158 Time(s/epoch):0.0099
Epoch: 100, Reconstruction Loss:0.1285 Time(s/epoch):0.0102
Epoch: 101, Reconstruction Loss:0.1401 Time(s/epoch):0.0100
Epoch: 102, Reconstruction Loss:0.1472 Time(s/epoch):0.0090
Epoch: 103, Reconstruction Loss:0.1478 Time(s/epoch):0.0084
Epoch: 104, Reconstruction Loss:0.1411 Time(s/epoch):0.0100
Epoch: 105, Reconstruction Loss:0.1278 Time(s/epoch):0.0102
Epoch: 106, Reconstruction Loss:0.1110 Time(s/epoch):0.0099
Epoch: 107, Reconstruction Loss:0.0978 Time(s/epoch):0.0098
Epoch: 108, Reconstruction Loss:0.0970 Time(s/epoch):0.0097
Epoch: 109, Reconstruction Loss:0.1072 Time(s/epoch):0.0097
Epoch: 110, Reconstruction Loss:0.1181 Time(s/epoch):0.0099
Epoch: 111, Reconstruction Loss:0.1234 Time(s/epoch):0.0098
Epoch: 112, Reconstruction Loss:0.1223 Time(s/epoch):0.0094
Epoch: 113, Reconstruction Loss:0.1178 Time(s/epoch):0.0096
Epoch: 114, Reconstruction Loss:0.1140 Time(s/epoch):0.0098
Epoch: 115, Reconstruction Loss:0.1148 Time(s/epoch):0.0099
Epoch: 116, Reconstruction Loss:0.1218 Time(s/epoch):0.0090
Epoch: 117, Reconstruction Loss:0.1333 Time(s/epoch):0.0077
Epoch: 118, Reconstruction Loss:0.1462 Time(s/epoch):0.0075
Epoch: 119, Reconstruction Loss:0.1572 Time(s/epoch):0.0071
Epoch: 120, Reconstruction Loss:0.1635 Time(s/epoch):0.0088
Epoch: 121, Reconstruction Loss:0.1634 Time(s/epoch):0.0097
Epoch: 122, Reconstruction Loss:0.1562 Time(s/epoch):0.0082
Epoch: 123, Reconstruction Loss:0.1423 Time(s/epoch):0.0078
Epoch: 124, Reconstruction Loss:0.1244 Time(s/epoch):0.0097
Epoch: 125, Reconstruction Loss:0.1089 Time(s/epoch):0.0112
Epoch: 126, Reconstruction Loss:0.1046 Time(s/epoch):0.0105
Epoch: 127, Reconstruction Loss:0.1116 Time(s/epoch):0.0103
Epoch: 128, Reconstruction Loss:0.1201 Time(s/epoch):0.0098
Epoch: 129, Reconstruction Loss:0.1227 Time(s/epoch):0.0084
Epoch: 130, Reconstruction Loss:0.1190 Time(s/epoch):0.0092
Epoch: 131, Reconstruction Loss:0.1133 Time(s/epoch):0.0079
Epoch: 132, Reconstruction Loss:0.1109 Time(s/epoch):0.0090
Epoch: 133, Reconstruction Loss:0.1149 Time(s/epoch):0.0082
Epoch: 134, Reconstruction Loss:0.1244 Time(s/epoch):0.0085
Epoch: 135, Reconstruction Loss:0.1367 Time(s/epoch):0.0080
Epoch: 136, Reconstruction Loss:0.1489 Time(s/epoch):0.0094
Epoch: 137, Reconstruction Loss:0.1578 Time(s/epoch):0.0108
Epoch: 138, Reconstruction Loss:0.1611 Time(s/epoch):0.0082
Epoch: 139, Reconstruction Loss:0.1578 Time(s/epoch):0.0090
Epoch: 140, Reconstruction Loss:0.1476 Time(s/epoch):0.0104
Epoch: 141, Reconstruction Loss:0.1315 Time(s/epoch):0.0100
Epoch: 142, Reconstruction Loss:0.1124 Time(s/epoch):0.0105
Epoch: 143, Reconstruction Loss:0.0969 Time(s/epoch):0.0100
Epoch: 144, Reconstruction Loss:0.0943 Time(s/epoch):0.0103
Epoch: 145, Reconstruction Loss:0.1052 Time(s/epoch):0.0102
Epoch: 146, Reconstruction Loss:0.1195 Time(s/epoch):0.0081
Epoch: 147, Reconstruction Loss:0.1299 Time(s/epoch):0.0088
Epoch: 148, Reconstruction Loss:0.1341 Time(s/epoch):0.0086
Epoch: 149, Reconstruction Loss:0.1326 Time(s/epoch):0.0086
Epoch: 150, Reconstruction Loss:0.1269 Time(s/epoch):0.0084
Epoch: 151, Reconstruction Loss:0.1191 Time(s/epoch):0.0096
Epoch: 152, Reconstruction Loss:0.1122 Time(s/epoch):0.0099
Epoch: 153, Reconstruction Loss:0.1094 Time(s/epoch):0.0116
Epoch: 154, Reconstruction Loss:0.1134 Time(s/epoch):0.0110
Epoch: 155, Reconstruction Loss:0.1237 Time(s/epoch):0.0108
Epoch: 156, Reconstruction Loss:0.1368 Time(s/epoch):0.0085
Epoch: 157, Reconstruction Loss:0.1485 Time(s/epoch):0.0092
Epoch: 158, Reconstruction Loss:0.1554 Time(s/epoch):0.0091
Epoch: 159, Reconstruction Loss:0.1555 Time(s/epoch):0.0087
Epoch: 160, Reconstruction Loss:0.1486 Time(s/epoch):0.0082
Epoch: 161, Reconstruction Loss:0.1355 Time(s/epoch):0.0096
Epoch: 162, Reconstruction Loss:0.1188 Time(s/epoch):0.0088
Epoch: 163, Reconstruction Loss:0.1037 Time(s/epoch):0.0102
Epoch: 164, Reconstruction Loss:0.0972 Time(s/epoch):0.0085[2025-04-01 02:43:53,875]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:53,884]: Epoch: 001, Loss:1.8073 Train: 0.4833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:43:53,891]: Epoch: 002, Loss:1.5387 Train: 0.7000, Val:0.5875, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:43:53,898]: Epoch: 003, Loss:0.9753 Train: 0.8083, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:53,905]: Epoch: 004, Loss:0.6858 Train: 0.8417, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:53,911]: Epoch: 005, Loss:0.6117 Train: 0.8500, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0062
[2025-04-01 02:43:53,919]: Epoch: 006, Loss:0.5022 Train: 0.8500, Val:0.6250, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:43:53,926]: Epoch: 007, Loss:0.4610 Train: 0.8583, Val:0.6125, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:43:53,935]: Epoch: 008, Loss:0.3982 Train: 0.9000, Val:0.6125, Test: 0.5882, Time(s/epoch):0.0080
[2025-04-01 02:43:53,942]: Epoch: 009, Loss:0.3716 Train: 0.9167, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:53,951]: Epoch: 010, Loss:0.3278 Train: 0.9333, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0084
[2025-04-01 02:43:53,960]: Epoch: 011, Loss:0.2791 Train: 0.9500, Val:0.7000, Test: 0.6667, Time(s/epoch):0.0089
[2025-04-01 02:43:53,968]: Epoch: 012, Loss:0.2424 Train: 0.9417, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:53,976]: Epoch: 013, Loss:0.2538 Train: 0.9417, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:53,982]: Epoch: 014, Loss:0.1891 Train: 0.9417, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:53,990]: Epoch: 015, Loss:0.2005 Train: 0.9417, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:53,997]: Epoch: 016, Loss:0.2305 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:54,005]: Epoch: 017, Loss:0.1808 Train: 0.9583, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:54,013]: Epoch: 018, Loss:0.1753 Train: 0.9500, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:54,021]: Epoch: 019, Loss:0.1655 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:54,030]: Epoch: 020, Loss:0.1443 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:54,037]: Epoch: 021, Loss:0.1306 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:54,045]: Epoch: 022, Loss:0.1599 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:54,053]: Epoch: 023, Loss:0.1397 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:54,062]: Epoch: 024, Loss:0.1396 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:54,070]: Epoch: 025, Loss:0.1491 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:54,080]: Epoch: 026, Loss:0.1143 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:54,088]: Epoch: 027, Loss:0.1835 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:54,096]: Epoch: 028, Loss:0.1146 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:54,104]: Epoch: 029, Loss:0.1138 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:54,113]: Epoch: 030, Loss:0.1307 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0089
[2025-04-01 02:43:54,121]: Epoch: 031, Loss:0.1288 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:54,128]: Epoch: 032, Loss:0.1411 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:54,136]: Epoch: 033, Loss:0.1181 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:54,143]: Epoch: 034, Loss:0.1229 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:54,149]: Epoch: 035, Loss:0.1646 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:43:54,158]: Epoch: 036, Loss:0.1110 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:54,165]: Epoch: 037, Loss:0.1185 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:54,173]: Epoch: 038, Loss:0.1475 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:54,179]: Epoch: 039, Loss:0.1130 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:54,187]: Epoch: 040, Loss:0.1502 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:54,195]: Epoch: 041, Loss:0.1558 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:54,202]: Epoch: 042, Loss:0.1157 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:54,210]: Epoch: 043, Loss:0.1143 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:54,218]: Epoch: 044, Loss:0.1410 Train: 0.9500, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:54,225]: Epoch: 045, Loss:0.1448 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:54,235]: Epoch: 046, Loss:0.1445 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0096
[2025-04-01 02:43:54,243]: Epoch: 047, Loss:0.1338 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:54,251]: Epoch: 048, Loss:0.1371 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:54,259]: Epoch: 049, Loss:0.1366 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:54,267]: Epoch: 050, Loss:0.1332 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:54,274]: Epoch: 051, Loss:0.1272 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:54,281]: Epoch: 052, Loss:0.1026 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:54,288]: Epoch: 053, Loss:0.1123 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:54,295]: Epoch: 054, Loss:0.1202 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:54,303]: Epoch: 055, Loss:0.1268 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:54,311]: Epoch: 056, Loss:0.1641 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:54,318]: Epoch: 057, Loss:0.1188 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:54,325]: Epoch: 058, Loss:0.0899 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:54,332]: Epoch: 059, Loss:0.1333 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:54,339]: Epoch: 060, Loss:0.1134 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:54,346]: Epoch: 061, Loss:0.0977 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:54,353]: Epoch: 062, Loss:0.1189 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:54,360]: Epoch: 063, Loss:0.1563 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:54,368]: Epoch: 064, Loss:0.1100 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:54,375]: Epoch: 065, Loss:0.1249 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:54,384]: Epoch: 066, Loss:0.1454 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:54,391]: Epoch: 067, Loss:0.1493 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:54,398]: Epoch: 068, Loss:0.1099 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:54,406]: Epoch: 069, Loss:0.1590 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:54,414]: Epoch: 070, Loss:0.1180 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:54,421]: Epoch: 071, Loss:0.1208 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:54,429]: Epoch: 072, Loss:0.1178 Train: 0.9583, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:54,435]: Epoch: 073, Loss:0.1558 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:54,441]: Epoch: 074, Loss:0.1144 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0059
[2025-04-01 02:43:54,449]: Epoch: 075, Loss:0.1265 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:54,457]: Epoch: 076, Loss:0.1562 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:54,466]: Epoch: 077, Loss:0.1284 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:54,472]: Epoch: 078, Loss:0.1113 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:43:54,479]: Epoch: 079, Loss:0.1400 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:54,486]: Epoch: 080, Loss:0.1341 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:54,495]: Epoch: 081, Loss:0.1210 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:54,501]: Epoch: 082, Loss:0.2078 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:54,509]: Epoch: 083, Loss:0.1424 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:54,517]: Epoch: 084, Loss:0.1186 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:54,525]: Epoch: 085, Loss:0.1436 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:54,533]: Epoch: 086, Loss:0.1171 Train: 0.9417, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:54,541]: Epoch: 087, Loss:0.1351 Train: 0.9500, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:54,549]: Epoch: 088, Loss:0.1571 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:54,556]: Epoch: 089, Loss:0.1157 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:54,563]: Epoch: 090, Loss:0.1209 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:54,570]: Epoch: 091, Loss:0.1065 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:54,578]: Epoch: 092, Loss:0.1156 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:54,586]: Epoch: 093, Loss:0.0929 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:54,593]: Epoch: 094, Loss:0.1046 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:54,602]: Epoch: 095, Loss:0.1239 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:54,608]: Epoch: 096, Loss:0.1188 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:54,616]: Epoch: 097, Loss:0.1174 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:54,623]: Epoch: 098, Loss:0.1163 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:54,630]: Epoch: 099, Loss:0.1721 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:54,638]: Epoch: 100, Loss:0.1164 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:54,645]: Epoch: 101, Loss:0.1133 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:54,651]: Epoch: 102, Loss:0.1080 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:54,660]: Epoch: 103, Loss:0.1631 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:54,667]: Epoch: 104, Loss:0.1336 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:54,675]: Epoch: 105, Loss:0.1177 Train: 0.9583, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:54,682]: Epoch: 106, Loss:0.1368 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:54,691]: Epoch: 107, Loss:0.1001 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:54,698]: Epoch: 108, Loss:0.1029 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:54,706]: Epoch: 109, Loss:0.0978 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:54,714]: Epoch: 110, Loss:0.1062 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:54,720]: Epoch: 111, Loss:0.1236 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0059
[2025-04-01 02:43:54,728]: Epoch: 112, Loss:0.1404 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:54,736]: Epoch: 113, Loss:0.1239 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:54,743]: Epoch: 114, Loss:0.1375 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:54,750]: Epoch: 115, Loss:0.1000 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:54,758]: Epoch: 116, Loss:0.1250 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:54,765]: Epoch: 117, Loss:0.0951 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:54,772]: Epoch: 118, Loss:0.1468 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:54,780]: Epoch: 119, Loss:0.1137 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:54,787]: Epoch: 120, Loss:0.1167 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:54,794]: Epoch: 121, Loss:0.1225 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:54,802]: Epoch: 122, Loss:0.1035 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:54,808]: Epoch: 123, Loss:0.1001 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:43:54,815]: Epoch: 124, Loss:0.1183 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0069
[2025-04-01 02:43:54,823]: Epoch: 125, Loss:0.1327 Train: 0.9667, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0071
[2025-04-01 02:43:54,830]: Epoch: 126, Loss:0.1307 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0072
[2025-04-01 02:43:54,838]: Epoch: 127, Loss:0.1040 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0076
[2025-04-01 02:43:54,846]: Epoch: 128, Loss:0.0992 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:54,853]: Epoch: 129, Loss:0.0828 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:54,861]: Epoch: 130, Loss:0.0959 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:54,868]: Epoch: 131, Loss:0.0981 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:54,876]: Epoch: 132, Loss:0.1742 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:54,882]: Epoch: 133, Loss:0.1336 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:54,890]: Epoch: 134, Loss:0.1104 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:54,897]: Epoch: 135, Loss:0.1090 Train: 0.9750, Val:0.6250, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:54,906]: Epoch: 136, Loss:0.1416 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:54,914]: Epoch: 137, Loss:0.1553 Train: 0.9750, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:54,921]: Epoch: 138, Loss:0.1568 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:54,930]: Epoch: 139, Loss:0.1437 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:43:54,938]: Epoch: 140, Loss:0.1534 Train: 0.9583, Val:0.6250, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:43:54,945]: Epoch: 141, Loss:0.1232 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:54,951]: Epoch: 142, Loss:0.1092 Train: 0.9750, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:54,959]: Epoch: 143, Loss:0.0895 Train: 0.9750, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0074
[2025-04-01 02:43:54,967]: Epoch: 144, Loss:0.1122 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:54,975]: Epoch: 145, Loss:0.1223 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:54,981]: Epoch: 146, Loss:0.1148 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:54,989]: Epoch: 147, Loss:0.1163 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:54,997]: Epoch: 148, Loss:0.1038 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:55,004]: Epoch: 149, Loss:0.0857 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:55,011]: Epoch: 150, Loss:0.1160 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:55,018]: Epoch: 151, Loss:0.0989 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:55,025]: Epoch: 152, Loss:0.0898 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:55,033]: Epoch: 153, Loss:0.1141 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:55,039]: Epoch: 154, Loss:0.0985 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:55,047]: Epoch: 155, Loss:0.1017 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:55,055]: Epoch: 156, Loss:0.1015 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:55,063]: Epoch: 157, Loss:0.0954 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:55,070]: Epoch: 158, Loss:0.0833 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:55,078]: Epoch: 159, Loss:0.1090 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:55,086]: Epoch: 160, Loss:0.0978 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:55,094]: Epoch: 161, Loss:0.1211 Train: 0.9667, Val:0.6250, Test: 0.7255, Time(s/epoch):0.0076
[2025-04-01 02:43:55,100]: Epoch: 162, Loss:0.0981 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:55,107]: Epoch: 163, Loss:0.1048 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:55,115]: Epoch: 164, Loss:0.1216 Train: 0.9750, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0072
[2025-04-01 02:43:55,121]: Epoch: 165, Loss:0.1278 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:55,129]: Epoch: 166, Loss:0.0913 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:55,136]: Epoch: 167, Loss:0.1413 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:55,145]: Epoch: 168, Loss:0.1536 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:55,153]: Epoch: 169, Loss:0.1005 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0080
[2025-04-01 02:43:55,159]: Epoch: 170, Loss:0.1130 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0063
[2025-04-01 02:43:55,167]: Epoch: 171, Loss:0.1268 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:55,176]: Epoch: 172, Loss:0.1196 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0089
[2025-04-01 02:43:55,182]: Epoch: 173, Loss:0.1262 Train: 0.9583, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0062
[2025-04-01 02:43:55,190]: Epoch: 174, Loss:0.1091 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:55,198]: Epoch: 175, Loss:0.1071 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:55,206]: Epoch: 176, Loss:0.1324 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:55,214]: Epoch: 177, Loss:0.1008 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:55,221]: Epoch: 178, Loss:0.1038 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:55,229]: Epoch: 179, Loss:0.1117 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:55,238]: Epoch: 180, Loss:0.0885 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:55,246]: Epoch: 181, Loss:0.1378 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:55,252]: Epoch: 182, Loss:0.1086 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:55,261]: Epoch: 183, Loss:0.1153 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:55,268]: Epoch: 184, Loss:0.1191 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:55,275]: Epoch: 185, Loss:0.1295 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:55,283]: Epoch: 186, Loss:0.1059 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:55,292]: Epoch: 187, Loss:0.1024 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:55,299]: Epoch: 188, Loss:0.0909 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:55,306]: Epoch: 189, Loss:0.0969 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:55,314]: Epoch: 190, Loss:0.0919 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:55,320]: Epoch: 191, Loss:0.0940 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:55,328]: Epoch: 192, Loss:0.1201 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:55,334]: Epoch: 193, Loss:0.1057 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0060
[2025-04-01 02:43:55,341]: Epoch: 194, Loss:0.1159 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:55,349]: Epoch: 195, Loss:0.1059 Train: 0.9583, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:55,358]: Epoch: 196, Loss:0.0995 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0089
[2025-04-01 02:43:55,366]: Epoch: 197, Loss:0.0926 Train: 0.9583, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:55,373]: Epoch: 198, Loss:0.1140 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:55,381]: Epoch: 199, Loss:0.1208 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:55,390]: Epoch: 200, Loss:0.1039 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:55,390]: [Run-1 score] {'train': 0.9333333333333333, 'val': 0.7, 'test': 0.6666666666666666}
[2025-04-01 02:43:55,390]: repeat 2/3
[2025-04-01 02:43:55,390]: Manual random seed:0
[2025-04-01 02:43:55,390]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:43:55,394]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:55,404]: Epoch: 001, Loss:1.6417 Train: 0.6667, Val:0.5625, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:43:55,410]: Epoch: 002, Loss:1.1329 Train: 0.7250, Val:0.6000, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:55,418]: Epoch: 003, Loss:0.7181 Train: 0.8333, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:55,426]: Epoch: 004, Loss:0.5744 Train: 0.8583, Val:0.6875, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:55,434]: Epoch: 005, Loss:0.4550 Train: 0.8833, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0075
[2025-04-01 02:43:55,441]: Epoch: 006, Loss:0.4268 Train: 0.8750, Val:0.6375, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:43:55,449]: Epoch: 007, Loss:0.3796 Train: 0.9083, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:55,456]: Epoch: 008, Loss:0.3025 Train: 0.9250, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:55,463]: Epoch: 009, Loss:0.2875 Train: 0.9167, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0065
[2025-04-01 02:43:55,471]: Epoch: 010, Loss:0.2987 Train: 0.9167, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:55,479]: Epoch: 011, Loss:0.2447 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:55,487]: Epoch: 012, Loss:0.2097 Train: 0.9333, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:55,493]: Epoch: 013, Loss:0.1877 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:55,500]: Epoch: 014, Loss:0.1823 Train: 0.9417, Val:0.6875, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:55,507]: Epoch: 015, Loss:0.1636 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:55,514]: Epoch: 016, Loss:0.1591 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:55,520]: Epoch: 017, Loss:0.1531 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0058
[2025-04-01 02:43:55,527]: Epoch: 018, Loss:0.1716 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:55,535]: Epoch: 019, Loss:0.1523 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:55,544]: Epoch: 020, Loss:0.1366 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:55,553]: Epoch: 021, Loss:0.1392 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:55,559]: Epoch: 022, Loss:0.1562 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:55,569]: Epoch: 023, Loss:0.1227 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0096
[2025-04-01 02:43:55,578]: Epoch: 024, Loss:0.1244 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:55,586]: Epoch: 025, Loss:0.1305 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:55,594]: Epoch: 026, Loss:0.1348 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:55,604]: Epoch: 027, Loss:0.1195 Train: 0.9667, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0099
[2025-04-01 02:43:55,614]: Epoch: 028, Loss:0.1106 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0095
[2025-04-01 02:43:55,622]: Epoch: 029, Loss:0.1011 Train: 0.9583, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:55,630]: Epoch: 030, Loss:0.1238 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:55,637]: Epoch: 031, Loss:0.1170 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:55,645]: Epoch: 032, Loss:0.1268 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:55,652]: Epoch: 033, Loss:0.1441 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:55,661]: Epoch: 034, Loss:0.1655 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:55,667]: Epoch: 035, Loss:0.1148 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:55,676]: Epoch: 036, Loss:0.1395 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:55,683]: Epoch: 037, Loss:0.1114 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:55,691]: Epoch: 038, Loss:0.1112 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:55,699]: Epoch: 039, Loss:0.1159 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:55,708]: Epoch: 040, Loss:0.1320 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0092
[2025-04-01 02:43:55,717]: Epoch: 041, Loss:0.1212 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0088
[2025-04-01 02:43:55,725]: Epoch: 042, Loss:0.1338 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0074
[2025-04-01 02:43:55,731]: Epoch: 043, Loss:0.1254 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:55,740]: Epoch: 044, Loss:0.1158 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:55,747]: Epoch: 045, Loss:0.1388 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:55,755]: Epoch: 046, Loss:0.1507 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:55,763]: Epoch: 047, Loss:0.1143 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:55,771]: Epoch: 048, Loss:0.1003 Train: 0.9500, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:55,779]: Epoch: 049, Loss:0.1519 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:55,787]: Epoch: 050, Loss:0.1367 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:55,793]: Epoch: 051, Loss:0.1288 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:55,800]: Epoch: 052, Loss:0.1203 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:55,808]: Epoch: 053, Loss:0.1141 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:55,815]: Epoch: 054, Loss:0.1218 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:55,823]: Epoch: 055, Loss:0.1197 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:55,829]: Epoch: 056, Loss:0.1567 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0058
[2025-04-01 02:43:55,836]: Epoch: 057, Loss:0.1381 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:55,845]: Epoch: 058, Loss:0.1283 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:55,852]: Epoch: 059, Loss:0.1254 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:55,860]: Epoch: 060, Loss:0.1325 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:55,868]: Epoch: 061, Loss:0.1412 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:55,876]: Epoch: 062, Loss:0.1098 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:55,882]: Epoch: 063, Loss:0.1090 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:55,890]: Epoch: 064, Loss:0.1174 Train: 0.9583, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0075
[2025-04-01 02:43:55,897]: Epoch: 065, Loss:0.1389 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:55,904]: Epoch: 066, Loss:0.1763 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:55,910]: Epoch: 067, Loss:0.1150 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:43:55,918]: Epoch: 068, Loss:0.1338 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:55,925]: Epoch: 069, Loss:0.1898 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:55,931]: Epoch: 070, Loss:0.1434 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:55,940]: Epoch: 071, Loss:0.1209 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:55,948]: Epoch: 072, Loss:0.1615 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:55,955]: Epoch: 073, Loss:0.1221 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:55,964]: Epoch: 074, Loss:0.1009 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:55,970]: Epoch: 075, Loss:0.1240 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:55,978]: Epoch: 076, Loss:0.1532 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:55,985]: Epoch: 077, Loss:0.1322 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:55,994]: Epoch: 078, Loss:0.1014 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:56,001]: Epoch: 079, Loss:0.1037 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:56,009]: Epoch: 080, Loss:0.1451 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0074
[2025-04-01 02:43:56,016]: Epoch: 081, Loss:0.1247 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:56,023]: Epoch: 082, Loss:0.1217 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:56,031]: Epoch: 083, Loss:0.1298 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:56,039]: Epoch: 084, Loss:0.1116 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:56,047]: Epoch: 085, Loss:0.1175 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:56,054]: Epoch: 086, Loss:0.0976 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:56,060]: Epoch: 087, Loss:0.0895 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:56,068]: Epoch: 088, Loss:0.1265 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0072
[2025-04-01 02:43:56,075]: Epoch: 089, Loss:0.1025 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:56,081]: Epoch: 090, Loss:0.1056 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:56,088]: Epoch: 091, Loss:0.1191 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0066
[2025-04-01 02:43:56,096]: Epoch: 092, Loss:0.1833 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:56,104]: Epoch: 093, Loss:0.1102 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:56,110]: Epoch: 094, Loss:0.1282 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:56,116]: Epoch: 095, Loss:0.1431 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:56,124]: Epoch: 096, Loss:0.1109 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:56,133]: Epoch: 097, Loss:0.1571 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:56,141]: Epoch: 098, Loss:0.1047 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:56,149]: Epoch: 099, Loss:0.1151 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:56,159]: Epoch: 100, Loss:0.1181 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0095
[2025-04-01 02:43:56,167]: Epoch: 101, Loss:0.1229 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:56,175]: Epoch: 102, Loss:0.1353 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:56,184]: Epoch: 103, Loss:0.1532 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:56,192]: Epoch: 104, Loss:0.1126 Train: 0.9500, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:56,201]: Epoch: 105, Loss:0.1618 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:56,209]: Epoch: 106, Loss:0.0948 Train: 0.9500, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:56,217]: Epoch: 107, Loss:0.1768 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0080
[2025-04-01 02:43:56,224]: Epoch: 108, Loss:0.1290 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:56,235]: Epoch: 109, Loss:0.1117 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0102
[2025-04-01 02:43:56,243]: Epoch: 110, Loss:0.1066 Train: 0.9583, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:56,252]: Epoch: 111, Loss:0.1521 Train: 0.9583, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:56,260]: Epoch: 112, Loss:0.1380 Train: 0.9667, Val:0.6250, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:56,268]: Epoch: 113, Loss:0.1210 Train: 0.9750, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:56,276]: Epoch: 114, Loss:0.1237 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0076
[2025-04-01 02:43:56,284]: Epoch: 115, Loss:0.1122 Train: 0.9500, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:56,291]: Epoch: 116, Loss:0.1442 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:56,298]: Epoch: 117, Loss:0.1273 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:56,307]: Epoch: 118, Loss:0.1177 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:56,314]: Epoch: 119, Loss:0.1344 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:56,320]: Epoch: 120, Loss:0.1318 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0062
[2025-04-01 02:43:56,328]: Epoch: 121, Loss:0.1059 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:56,334]: Epoch: 122, Loss:0.0876 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:56,341]: Epoch: 123, Loss:0.1402 Train: 0.9500, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0069
[2025-04-01 02:43:56,349]: Epoch: 124, Loss:0.1248 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:56,358]: Epoch: 125, Loss:0.1152 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0081
[2025-04-01 02:43:56,366]: Epoch: 126, Loss:0.1091 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:56,374]: Epoch: 127, Loss:0.1333 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:56,380]: Epoch: 128, Loss:0.1438 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0054
[2025-04-01 02:43:56,388]: Epoch: 129, Loss:0.1763 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:56,397]: Epoch: 130, Loss:0.0993 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:56,406]: Epoch: 131, Loss:0.0993 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:56,413]: Epoch: 132, Loss:0.1168 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:56,419]: Epoch: 133, Loss:0.1338 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0058
[2025-04-01 02:43:56,428]: Epoch: 134, Loss:0.1087 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:56,435]: Epoch: 135, Loss:0.1316 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0075
[2025-04-01 02:43:56,441]: Epoch: 136, Loss:0.1021 Train: 0.9667, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0058
[2025-04-01 02:43:56,449]: Epoch: 137, Loss:0.1088 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0072
[2025-04-01 02:43:56,456]: Epoch: 138, Loss:0.1026 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0070
[2025-04-01 02:43:56,463]: Epoch: 139, Loss:0.0897 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0067
[2025-04-01 02:43:56,470]: Epoch: 140, Loss:0.1082 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0067
[2025-04-01 02:43:56,476]: Epoch: 141, Loss:0.0927 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:56,483]: Epoch: 142, Loss:0.1013 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:56,490]: Epoch: 143, Loss:0.1022 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0065
[2025-04-01 02:43:56,498]: Epoch: 144, Loss:0.0891 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:56,506]: Epoch: 145, Loss:0.0836 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0076
[2025-04-01 02:43:56,513]: Epoch: 146, Loss:0.1194 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:56,520]: Epoch: 147, Loss:0.1283 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0066
[2025-04-01 02:43:56,528]: Epoch: 148, Loss:0.0891 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:56,536]: Epoch: 149, Loss:0.1130 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:56,544]: Epoch: 150, Loss:0.1230 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:56,551]: Epoch: 151, Loss:0.1188 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0065
[2025-04-01 02:43:56,558]: Epoch: 152, Loss:0.1107 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:56,566]: Epoch: 153, Loss:0.0997 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:56,576]: Epoch: 154, Loss:0.1192 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:56,583]: Epoch: 155, Loss:0.0807 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0075
[2025-04-01 02:43:56,591]: Epoch: 156, Loss:0.1046 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:56,598]: Epoch: 157, Loss:0.0914 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:56,605]: Epoch: 158, Loss:0.1106 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0066
[2025-04-01 02:43:56,612]: Epoch: 159, Loss:0.1102 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0065
[2025-04-01 02:43:56,619]: Epoch: 160, Loss:0.0977 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0070
[2025-04-01 02:43:56,626]: Epoch: 161, Loss:0.1220 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0073
[2025-04-01 02:43:56,635]: Epoch: 162, Loss:0.1102 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:56,642]: Epoch: 163, Loss:0.1284 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:56,650]: Epoch: 164, Loss:0.0916 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0074
[2025-04-01 02:43:56,658]: Epoch: 165, Loss:0.1139 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0077
[2025-04-01 02:43:56,666]: Epoch: 166, Loss:0.0864 Train: 0.9583, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0082
[2025-04-01 02:43:56,672]: Epoch: 167, Loss:0.1104 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:56,680]: Epoch: 168, Loss:0.1359 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:56,686]: Epoch: 169, Loss:0.1342 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0065
[2025-04-01 02:43:56,695]: Epoch: 170, Loss:0.1139 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:56,700]: Epoch: 171, Loss:0.1406 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0053
[2025-04-01 02:43:56,710]: Epoch: 172, Loss:0.1224 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0091
[2025-04-01 02:43:56,718]: Epoch: 173, Loss:0.1380 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:56,725]: Epoch: 174, Loss:0.1105 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:56,734]: Epoch: 175, Loss:0.1392 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0089
[2025-04-01 02:43:56,743]: Epoch: 176, Loss:0.1202 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:56,751]: Epoch: 177, Loss:0.1192 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0082
[2025-04-01 02:43:56,760]: Epoch: 178, Loss:0.1149 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:56,768]: Epoch: 179, Loss:0.1377 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:56,775]: Epoch: 180, Loss:0.1160 Train: 0.9667, Val:0.6500, Test: 0.7451, Time(s/epoch):0.0069
[2025-04-01 02:43:56,783]: Epoch: 181, Loss:0.1335 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:56,790]: Epoch: 182, Loss:0.0977 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0059
[2025-04-01 02:43:56,797]: Epoch: 183, Loss:0.1247 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:56,804]: Epoch: 184, Loss:0.0954 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:56,810]: Epoch: 185, Loss:0.1031 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0054
[2025-04-01 02:43:56,818]: Epoch: 186, Loss:0.1086 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:56,825]: Epoch: 187, Loss:0.1218 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:56,833]: Epoch: 188, Loss:0.0784 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:56,840]: Epoch: 189, Loss:0.0911 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:56,847]: Epoch: 190, Loss:0.0946 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:56,855]: Epoch: 191, Loss:0.1254 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:56,863]: Epoch: 192, Loss:0.1000 Train: 0.9667, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0081
[2025-04-01 02:43:56,870]: Epoch: 193, Loss:0.1242 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:56,878]: Epoch: 194, Loss:0.1002 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:56,885]: Epoch: 195, Loss:0.0954 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:56,892]: Epoch: 196, Loss:0.0978 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:56,900]: Epoch: 197, Loss:0.1127 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:56,908]: Epoch: 198, Loss:0.0973 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0073
[2025-04-01 02:43:56,915]: Epoch: 199, Loss:0.1284 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0067
[2025-04-01 02:43:56,921]: Epoch: 200, Loss:0.0838 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:56,921]: [Run-2 score] {'train': 0.8333333333333334, 'val': 0.6875, 'test': 0.7058823529411765}
[2025-04-01 02:43:56,921]: repeat 3/3
[2025-04-01 02:43:56,921]: Manual random seed:0
[2025-04-01 02:43:56,922]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:43:56,925]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:43:56,936]: Epoch: 001, Loss:1.7135 Train: 0.6583, Val:0.6000, Test: 0.6471, Time(s/epoch):0.0079
[2025-04-01 02:43:56,943]: Epoch: 002, Loss:1.0651 Train: 0.7750, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:43:56,951]: Epoch: 003, Loss:0.7714 Train: 0.8167, Val:0.6875, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:56,959]: Epoch: 004, Loss:0.5406 Train: 0.8583, Val:0.7000, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:56,967]: Epoch: 005, Loss:0.4755 Train: 0.9083, Val:0.6875, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:56,976]: Epoch: 006, Loss:0.4209 Train: 0.9083, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:56,982]: Epoch: 007, Loss:0.3606 Train: 0.9167, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0061
[2025-04-01 02:43:56,990]: Epoch: 008, Loss:0.2935 Train: 0.9000, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0078
[2025-04-01 02:43:56,997]: Epoch: 009, Loss:0.3092 Train: 0.9250, Val:0.6750, Test: 0.6471, Time(s/epoch):0.0066
[2025-04-01 02:43:57,005]: Epoch: 010, Loss:0.2635 Train: 0.9250, Val:0.6750, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:57,013]: Epoch: 011, Loss:0.2430 Train: 0.9417, Val:0.6625, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:43:57,020]: Epoch: 012, Loss:0.2175 Train: 0.9417, Val:0.6500, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:43:57,028]: Epoch: 013, Loss:0.1999 Train: 0.9417, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0073
[2025-04-01 02:43:57,035]: Epoch: 014, Loss:0.2393 Train: 0.9583, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:57,041]: Epoch: 015, Loss:0.1668 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:57,048]: Epoch: 016, Loss:0.1611 Train: 0.9500, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:57,055]: Epoch: 017, Loss:0.1547 Train: 0.9500, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:57,063]: Epoch: 018, Loss:0.2012 Train: 0.9500, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:57,071]: Epoch: 019, Loss:0.1523 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:57,078]: Epoch: 020, Loss:0.1536 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0071
[2025-04-01 02:43:57,085]: Epoch: 021, Loss:0.1631 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:57,093]: Epoch: 022, Loss:0.1213 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:57,099]: Epoch: 023, Loss:0.1574 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0061
[2025-04-01 02:43:57,107]: Epoch: 024, Loss:0.1876 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:57,115]: Epoch: 025, Loss:0.1195 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:57,124]: Epoch: 026, Loss:0.1072 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0082
[2025-04-01 02:43:57,131]: Epoch: 027, Loss:0.1206 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:57,139]: Epoch: 028, Loss:0.1603 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:57,147]: Epoch: 029, Loss:0.1352 Train: 0.9500, Val:0.6625, Test: 0.6471, Time(s/epoch):0.0080
[2025-04-01 02:43:57,156]: Epoch: 030, Loss:0.1141 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:57,164]: Epoch: 031, Loss:0.1726 Train: 0.9667, Val:0.6500, Test: 0.6471, Time(s/epoch):0.0085
[2025-04-01 02:43:57,173]: Epoch: 032, Loss:0.1231 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:57,179]: Epoch: 033, Loss:0.1331 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0054
[2025-04-01 02:43:57,186]: Epoch: 034, Loss:0.1776 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:57,195]: Epoch: 035, Loss:0.1651 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:57,203]: Epoch: 036, Loss:0.1792 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:57,209]: Epoch: 037, Loss:0.1145 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0056
[2025-04-01 02:43:57,216]: Epoch: 038, Loss:0.1109 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:57,224]: Epoch: 039, Loss:0.1486 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:57,233]: Epoch: 040, Loss:0.1436 Train: 0.9583, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0092
[2025-04-01 02:43:57,240]: Epoch: 041, Loss:0.1150 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0069
[2025-04-01 02:43:57,249]: Epoch: 042, Loss:0.1100 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:57,256]: Epoch: 043, Loss:0.1223 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:57,264]: Epoch: 044, Loss:0.1351 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0081
[2025-04-01 02:43:57,271]: Epoch: 045, Loss:0.1553 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:57,278]: Epoch: 046, Loss:0.1285 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0065
[2025-04-01 02:43:57,287]: Epoch: 047, Loss:0.1674 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:57,295]: Epoch: 048, Loss:0.1199 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:57,303]: Epoch: 049, Loss:0.1484 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:57,311]: Epoch: 050, Loss:0.1166 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0080
[2025-04-01 02:43:57,319]: Epoch: 051, Loss:0.1404 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0075
[2025-04-01 02:43:57,326]: Epoch: 052, Loss:0.1081 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0070
[2025-04-01 02:43:57,333]: Epoch: 053, Loss:0.1362 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:57,341]: Epoch: 054, Loss:0.1252 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:57,349]: Epoch: 055, Loss:0.1454 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0077
[2025-04-01 02:43:57,357]: Epoch: 056, Loss:0.1559 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:57,365]: Epoch: 057, Loss:0.1139 Train: 0.9667, Val:0.6375, Test: 0.6471, Time(s/epoch):0.0076
[2025-04-01 02:43:57,374]: Epoch: 058, Loss:0.1284 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0089
[2025-04-01 02:43:57,382]: Epoch: 059, Loss:0.1203 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:57,390]: Epoch: 060, Loss:0.1067 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:43:57,398]: Epoch: 061, Loss:0.1186 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:57,407]: Epoch: 062, Loss:0.1386 Train: 0.9750, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:57,413]: Epoch: 063, Loss:0.1477 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0063
[2025-04-01 02:43:57,421]: Epoch: 064, Loss:0.1054 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:57,429]: Epoch: 065, Loss:0.1103 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:57,435]: Epoch: 066, Loss:0.1034 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0064
[2025-04-01 02:43:57,444]: Epoch: 067, Loss:0.1014 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0090
[2025-04-01 02:43:57,452]: Epoch: 068, Loss:0.1012 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:57,458]: Epoch: 069, Loss:0.0929 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0061
[2025-04-01 02:43:57,466]: Epoch: 070, Loss:0.1096 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:57,473]: Epoch: 071, Loss:0.1191 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0067
[2025-04-01 02:43:57,479]: Epoch: 072, Loss:0.1190 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0062
[2025-04-01 02:43:57,486]: Epoch: 073, Loss:0.0953 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0071
[2025-04-01 02:43:57,495]: Epoch: 074, Loss:0.1148 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0081
[2025-04-01 02:43:57,504]: Epoch: 075, Loss:0.1569 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:57,509]: Epoch: 076, Loss:0.1223 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0054
[2025-04-01 02:43:57,517]: Epoch: 077, Loss:0.1052 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:57,523]: Epoch: 078, Loss:0.1373 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0068
[2025-04-01 02:43:57,530]: Epoch: 079, Loss:0.1200 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0060
[2025-04-01 02:43:57,537]: Epoch: 080, Loss:0.1470 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0068
[2025-04-01 02:43:57,544]: Epoch: 081, Loss:0.1075 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0068
[2025-04-01 02:43:57,552]: Epoch: 082, Loss:0.1373 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:57,558]: Epoch: 083, Loss:0.1194 Train: 0.9583, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0062
[2025-04-01 02:43:57,567]: Epoch: 084, Loss:0.1418 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:57,575]: Epoch: 085, Loss:0.1351 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:57,584]: Epoch: 086, Loss:0.1358 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:57,590]: Epoch: 087, Loss:0.1264 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0064
[2025-04-01 02:43:57,597]: Epoch: 088, Loss:0.1475 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0068
[2025-04-01 02:43:57,604]: Epoch: 089, Loss:0.1454 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:57,611]: Epoch: 090, Loss:0.1282 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0069
[2025-04-01 02:43:57,619]: Epoch: 091, Loss:0.1126 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0078
[2025-04-01 02:43:57,629]: Epoch: 092, Loss:0.1654 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0096
[2025-04-01 02:43:57,638]: Epoch: 093, Loss:0.1137 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0091
[2025-04-01 02:43:57,647]: Epoch: 094, Loss:0.1376 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:57,655]: Epoch: 095, Loss:0.1223 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:57,663]: Epoch: 096, Loss:0.1379 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0079
[2025-04-01 02:43:57,673]: Epoch: 097, Loss:0.1541 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0098
[2025-04-01 02:43:57,682]: Epoch: 098, Loss:0.1473 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:57,692]: Epoch: 099, Loss:0.1659 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0101
[2025-04-01 02:43:57,701]: Epoch: 100, Loss:0.1035 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:57,712]: Epoch: 101, Loss:0.1088 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0107
[2025-04-01 02:43:57,723]: Epoch: 102, Loss:0.1409 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0108
[2025-04-01 02:43:57,734]: Epoch: 103, Loss:0.1013 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0112
[2025-04-01 02:43:57,746]: Epoch: 104, Loss:0.1304 Train: 0.9750, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0112
[2025-04-01 02:43:57,756]: Epoch: 105, Loss:0.1468 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0104
[2025-04-01 02:43:57,765]: Epoch: 106, Loss:0.1175 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:57,775]: Epoch: 107, Loss:0.1434 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0093
[2025-04-01 02:43:57,783]: Epoch: 108, Loss:0.1122 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:57,793]: Epoch: 109, Loss:0.0985 Train: 0.9667, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0090
[2025-04-01 02:43:57,801]: Epoch: 110, Loss:0.1179 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:57,809]: Epoch: 111, Loss:0.1094 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:57,819]: Epoch: 112, Loss:0.1146 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0097
[2025-04-01 02:43:57,828]: Epoch: 113, Loss:0.1936 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0086
[2025-04-01 02:43:57,835]: Epoch: 114, Loss:0.1364 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:57,844]: Epoch: 115, Loss:0.0967 Train: 0.9667, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:57,852]: Epoch: 116, Loss:0.1196 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0073
[2025-04-01 02:43:57,863]: Epoch: 117, Loss:0.1383 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0110
[2025-04-01 02:43:57,873]: Epoch: 118, Loss:0.1338 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0093
[2025-04-01 02:43:57,884]: Epoch: 119, Loss:0.1166 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0115
[2025-04-01 02:43:57,894]: Epoch: 120, Loss:0.1382 Train: 0.9583, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:57,901]: Epoch: 121, Loss:0.1243 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:57,911]: Epoch: 122, Loss:0.1449 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0095
[2025-04-01 02:43:57,919]: Epoch: 123, Loss:0.1312 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0086
[2025-04-01 02:43:57,927]: Epoch: 124, Loss:0.0952 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0076
[2025-04-01 02:43:57,935]: Epoch: 125, Loss:0.1489 Train: 0.9583, Val:0.6750, Test: 0.6863, Time(s/epoch):0.0079
[2025-04-01 02:43:57,944]: Epoch: 126, Loss:0.1022 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0083
[2025-04-01 02:43:57,953]: Epoch: 127, Loss:0.1179 Train: 0.9667, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:57,966]: Epoch: 128, Loss:0.1091 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0124
[2025-04-01 02:43:57,976]: Epoch: 129, Loss:0.1216 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0107
[2025-04-01 02:43:57,985]: Epoch: 130, Loss:0.0960 Train: 0.9667, Val:0.6250, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:57,994]: Epoch: 131, Loss:0.1021 Train: 0.9667, Val:0.6250, Test: 0.6863, Time(s/epoch):0.0095
[2025-04-01 02:43:58,004]: Epoch: 132, Loss:0.1069 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0092
[2025-04-01 02:43:58,014]: Epoch: 133, Loss:0.1192 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0098
[2025-04-01 02:43:58,023]: Epoch: 134, Loss:0.1033 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0089
[2025-04-01 02:43:58,032]: Epoch: 135, Loss:0.0997 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:58,040]: Epoch: 136, Loss:0.1039 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:58,051]: Epoch: 137, Loss:0.1358 Train: 0.9667, Val:0.6750, Test: 0.7255, Time(s/epoch):0.0103
[2025-04-01 02:43:58,060]: Epoch: 138, Loss:0.1331 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0092
[2025-04-01 02:43:58,069]: Epoch: 139, Loss:0.1233 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0087
[2025-04-01 02:43:58,078]: Epoch: 140, Loss:0.1233 Train: 0.9583, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:58,086]: Epoch: 141, Loss:0.1333 Train: 0.9667, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:58,094]: Epoch: 142, Loss:0.0923 Train: 0.9583, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0082
[2025-04-01 02:43:58,102]: Epoch: 143, Loss:0.1223 Train: 0.9500, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0079
[2025-04-01 02:43:58,111]: Epoch: 144, Loss:0.1422 Train: 0.9667, Val:0.6250, Test: 0.7059, Time(s/epoch):0.0084
[2025-04-01 02:43:58,119]: Epoch: 145, Loss:0.0898 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0078
[2025-04-01 02:43:58,128]: Epoch: 146, Loss:0.1223 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:58,136]: Epoch: 147, Loss:0.1127 Train: 0.9583, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:58,144]: Epoch: 148, Loss:0.1329 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:58,186]: Epoch: 149, Loss:0.0908 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0419
[2025-04-01 02:43:58,201]: Epoch: 150, Loss:0.1372 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0142
[2025-04-01 02:43:58,209]: Epoch: 151, Loss:0.1080 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0085
[2025-04-01 02:43:58,218]: Epoch: 152, Loss:0.1383 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0085
[2025-04-01 02:43:58,230]: Epoch: 153, Loss:0.1086 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0118
[2025-04-01 02:43:58,250]: Epoch: 154, Loss:0.1072 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0194
[2025-04-01 02:43:58,259]: Epoch: 155, Loss:0.1119 Train: 0.9750, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0087
[2025-04-01 02:43:58,269]: Epoch: 156, Loss:0.0893 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0098
[2025-04-01 02:43:58,278]: Epoch: 157, Loss:0.1241 Train: 0.9667, Val:0.6750, Test: 0.7059, Time(s/epoch):0.0093
[2025-04-01 02:43:58,289]: Epoch: 158, Loss:0.1107 Train: 0.9667, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0109
[2025-04-01 02:43:58,299]: Epoch: 159, Loss:0.1195 Train: 0.9667, Val:0.6625, Test: 0.7451, Time(s/epoch):0.0099
[2025-04-01 02:43:58,309]: Epoch: 160, Loss:0.1054 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0093
[2025-04-01 02:43:58,318]: Epoch: 161, Loss:0.1401 Train: 0.9583, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0088
[2025-04-01 02:43:58,327]: Epoch: 162, Loss:0.0975 Train: 0.9583, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0090
[2025-04-01 02:43:58,335]: Epoch: 163, Loss:0.1386 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:58,344]: Epoch: 164, Loss:0.1102 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0084
[2025-04-01 02:43:58,353]: Epoch: 165, Loss:0.1063 Train: 0.9667, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0091
[2025-04-01 02:43:58,362]: Epoch: 166, Loss:0.1720 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0084
[2025-04-01 02:43:58,373]: Epoch: 167, Loss:0.1305 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0106
[2025-04-01 02:43:58,383]: Epoch: 168, Loss:0.1291 Train: 0.9667, Val:0.6500, Test: 0.7255, Time(s/epoch):0.0104
[2025-04-01 02:43:58,393]: Epoch: 169, Loss:0.0977 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0098
[2025-04-01 02:43:58,402]: Epoch: 170, Loss:0.1036 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0086
[2025-04-01 02:43:58,411]: Epoch: 171, Loss:0.1169 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0087
[2025-04-01 02:43:58,420]: Epoch: 172, Loss:0.1152 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:58,427]: Epoch: 173, Loss:0.1175 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0071
[2025-04-01 02:43:58,435]: Epoch: 174, Loss:0.1035 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:58,442]: Epoch: 175, Loss:0.0950 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0065
[2025-04-01 02:43:58,450]: Epoch: 176, Loss:0.1067 Train: 0.9750, Val:0.6500, Test: 0.6863, Time(s/epoch):0.0083
[2025-04-01 02:43:58,457]: Epoch: 177, Loss:0.0965 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0072
[2025-04-01 02:43:58,466]: Epoch: 178, Loss:0.1064 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:58,474]: Epoch: 179, Loss:0.0993 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0078
[2025-04-01 02:43:58,480]: Epoch: 180, Loss:0.1090 Train: 0.9750, Val:0.6375, Test: 0.7255, Time(s/epoch):0.0063
[2025-04-01 02:43:58,488]: Epoch: 181, Loss:0.0928 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0072
[2025-04-01 02:43:58,497]: Epoch: 182, Loss:0.1116 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0085
[2025-04-01 02:43:58,505]: Epoch: 183, Loss:0.0947 Train: 0.9750, Val:0.6625, Test: 0.7255, Time(s/epoch):0.0083
[2025-04-01 02:43:58,513]: Epoch: 184, Loss:0.0926 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0080
[2025-04-01 02:43:58,521]: Epoch: 185, Loss:0.1182 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0074
[2025-04-01 02:43:58,529]: Epoch: 186, Loss:0.1211 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:58,537]: Epoch: 187, Loss:0.0978 Train: 0.9750, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0077
[2025-04-01 02:43:58,544]: Epoch: 188, Loss:0.1097 Train: 0.9667, Val:0.6625, Test: 0.7059, Time(s/epoch):0.0070
[2025-04-01 02:43:58,551]: Epoch: 189, Loss:0.0933 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0071
[2025-04-01 02:43:58,560]: Epoch: 190, Loss:0.1009 Train: 0.9750, Val:0.6625, Test: 0.6863, Time(s/epoch):0.0081
[2025-04-01 02:43:58,568]: Epoch: 191, Loss:0.1205 Train: 0.9750, Val:0.6500, Test: 0.6667, Time(s/epoch):0.0077
[2025-04-01 02:43:58,575]: Epoch: 192, Loss:0.1320 Train: 0.9750, Val:0.6375, Test: 0.6667, Time(s/epoch):0.0072
[2025-04-01 02:43:58,582]: Epoch: 193, Loss:0.1074 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0069
[2025-04-01 02:43:58,589]: Epoch: 194, Loss:0.0918 Train: 0.9750, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0066
[2025-04-01 02:43:58,597]: Epoch: 195, Loss:0.1373 Train: 0.9667, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0083
[2025-04-01 02:43:58,604]: Epoch: 196, Loss:0.1029 Train: 0.9667, Val:0.6375, Test: 0.6863, Time(s/epoch):0.0067
[2025-04-01 02:43:58,611]: Epoch: 197, Loss:0.1180 Train: 0.9750, Val:0.6375, Test: 0.7059, Time(s/epoch):0.0061
[2025-04-01 02:43:58,619]: Epoch: 198, Loss:0.1017 Train: 0.9750, Val:0.6500, Test: 0.7059, Time(s/epoch):0.0082
[2025-04-01 02:43:58,627]: Epoch: 199, Loss:0.1058 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0078
[2025-04-01 02:43:58,635]: Epoch: 200, Loss:0.1099 Train: 0.9667, Val:0.6625, Test: 0.6667, Time(s/epoch):0.0080
[2025-04-01 02:43:58,635]: [Run-3 score] {'train': 0.8583333333333333, 'val': 0.7, 'test': 0.7058823529411765}
[2025-04-01 02:43:58,636]: [Average Score] {'train': np.float64(0.9094444444444444), 'val': np.float64(0.7224999999999998), 'test': np.float64(0.6640522875816994)} 
[2025-04-01 02:43:58,636]: [std Score] {'train': np.float64(0.07050523139683314), 'val': np.float64(0.031191612120354838), 'test': np.float64(0.07723488233580365)} 
[2025-04-01 02:43:58,636]: [Best Score] {'train': 1.0, 'val': 0.7875, 'test': 0.8235294117647058}
[2025-04-01 02:43:58,636]: [Best test run] 4

Epoch: 165, Reconstruction Loss:0.1023 Time(s/epoch):0.0082
Epoch: 166, Reconstruction Loss:0.1132 Time(s/epoch):0.0107
Epoch: 167, Reconstruction Loss:0.1230 Time(s/epoch):0.0096
Epoch: 168, Reconstruction Loss:0.1289 Time(s/epoch):0.0078
Epoch: 169, Reconstruction Loss:0.1308 Time(s/epoch):0.0085
Epoch: 170, Reconstruction Loss:0.1297 Time(s/epoch):0.0077
Epoch: 171, Reconstruction Loss:0.1267 Time(s/epoch):0.0082
Epoch: 172, Reconstruction Loss:0.1227 Time(s/epoch):0.0074
Epoch: 173, Reconstruction Loss:0.1188 Time(s/epoch):0.0091
Epoch: 174, Reconstruction Loss:0.1161 Time(s/epoch):0.0090
Epoch: 175, Reconstruction Loss:0.1159 Time(s/epoch):0.0086
Epoch: 176, Reconstruction Loss:0.1181 Time(s/epoch):0.0091
Epoch: 177, Reconstruction Loss:0.1214 Time(s/epoch):0.0094
Epoch: 178, Reconstruction Loss:0.1234 Time(s/epoch):0.0098
Epoch: 179, Reconstruction Loss:0.1221 Time(s/epoch):0.0089
Epoch: 180, Reconstruction Loss:0.1169 Time(s/epoch):0.0095
Epoch: 181, Reconstruction Loss:0.1085 Time(s/epoch):0.0107
Epoch: 182, Reconstruction Loss:0.0994 Time(s/epoch):0.0090
Epoch: 183, Reconstruction Loss:0.0936 Time(s/epoch):0.0105
Epoch: 184, Reconstruction Loss:0.0942 Time(s/epoch):0.0130
Epoch: 185, Reconstruction Loss:0.1002 Time(s/epoch):0.0095
Epoch: 186, Reconstruction Loss:0.1076 Time(s/epoch):0.0111
Epoch: 187, Reconstruction Loss:0.1130 Time(s/epoch):0.0096
Epoch: 188, Reconstruction Loss:0.1150 Time(s/epoch):0.0092
Epoch: 189, Reconstruction Loss:0.1137 Time(s/epoch):0.0095
Epoch: 190, Reconstruction Loss:0.1096 Time(s/epoch):0.0100
Epoch: 191, Reconstruction Loss:0.1036 Time(s/epoch):0.0100
Epoch: 192, Reconstruction Loss:0.0967 Time(s/epoch):0.0103
Epoch: 193, Reconstruction Loss:0.0910 Time(s/epoch):0.0101
Epoch: 194, Reconstruction Loss:0.0887 Time(s/epoch):0.0083
Epoch: 195, Reconstruction Loss:0.0913 Time(s/epoch):0.0079
Epoch: 196, Reconstruction Loss:0.0976 Time(s/epoch):0.0105
Epoch: 197, Reconstruction Loss:0.1045 Time(s/epoch):0.0081
Epoch: 198, Reconstruction Loss:0.1090 Time(s/epoch):0.0082
Epoch: 199, Reconstruction Loss:0.1091 Time(s/epoch):0.0091
Epoch: 200, Reconstruction Loss:0.1040 Time(s/epoch):0.0090
[Train] best epoch:11 | min reconstruction loss:0.0756
Begin finetuning...
Epoch: 001, Supervised Loss:1.6818 Time(s/epoch):0.0073
Epoch: 002, Supervised Loss:1.6121 Time(s/epoch):0.0078
Epoch: 003, Supervised Loss:1.5352 Time(s/epoch):0.0060
Epoch: 004, Supervised Loss:1.4937 Time(s/epoch):0.0059
Epoch: 005, Supervised Loss:1.5007 Time(s/epoch):0.0053
Epoch: 006, Supervised Loss:1.5295 Time(s/epoch):0.0071
Epoch: 007, Supervised Loss:1.5471 Time(s/epoch):0.0070
Epoch: 008, Supervised Loss:1.5416 Time(s/epoch):0.0053
Epoch: 009, Supervised Loss:1.5203 Time(s/epoch):0.0068
Epoch: 010, Supervised Loss:1.5043 Time(s/epoch):0.0079
Epoch: 011, Supervised Loss:1.5021 Time(s/epoch):0.0070
Epoch: 012, Supervised Loss:1.5063 Time(s/epoch):0.0075
Epoch: 013, Supervised Loss:1.5092 Time(s/epoch):0.0056
Epoch: 014, Supervised Loss:1.5111 Time(s/epoch):0.0049
Epoch: 015, Supervised Loss:1.5156 Time(s/epoch):0.0056
Epoch: 016, Supervised Loss:1.5195 Time(s/epoch):0.0050
Epoch: 017, Supervised Loss:1.5187 Time(s/epoch):0.0055
Epoch: 018, Supervised Loss:1.5153 Time(s/epoch):0.0055
Epoch: 019, Supervised Loss:1.5107 Time(s/epoch):0.0055
Epoch: 020, Supervised Loss:1.5032 Time(s/epoch):0.0050
Epoch: 021, Supervised Loss:1.4933 Time(s/epoch):0.0065
Epoch: 022, Supervised Loss:1.4840 Time(s/epoch):0.0068
Epoch: 023, Supervised Loss:1.4789 Time(s/epoch):0.0073
Epoch: 024, Supervised Loss:1.4785 Time(s/epoch):0.0068
Epoch: 025, Supervised Loss:1.4807 Time(s/epoch):0.0070
Epoch: 026, Supervised Loss:1.4839 Time(s/epoch):0.0057
Epoch: 027, Supervised Loss:1.4876 Time(s/epoch):0.0054
Epoch: 028, Supervised Loss:1.4914 Time(s/epoch):0.0061
Epoch: 029, Supervised Loss:1.4946 Time(s/epoch):0.0066
Epoch: 030, Supervised Loss:1.4958 Time(s/epoch):0.0068
0.9652736 515
Add 343 edges.
Prune 355 edges from torch.Size([2, 515]) to torch.Size([2, 160])
save graph to: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.674
Data(x=[251, 1703], edge_index=[2, 497], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:3.5479 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.674
Data(x=[251, 1703], edge_index=[2, 497], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5283 s
[Old Data] Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
load graph from: ../ckpt/data_Wisconsin_new_0.3.dat
Homophily ratio change from 0.196 to 0.674
Data(x=[251, 1703], edge_index=[2, 497], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.7111 s
Namespace(model='GCN', num_layer=2, repeat=3, num_epoch=200, dataset='Wisconsin', data_dir='../data', gpu='0', running_id='0', log_dir=None, hidden=64, moment=1, seed=0, gnn_seed=None, auto_fixed_seed=False, use_center_moment=True, lr=0.01, wd=0.005, graph_learn=True, rewiring_type='original', lr_gl=0.001, wd_gl=0.005, thres_min_deg=3, thres_min_deg_ratio=1.0, save_dir_gl='../ckpt/', window=[10000, 10000], shuffle=[False, False], epoch_train_gl=200, epoch_finetune_gl=30, seed_gl=0, k=8, cat_self=True, drop_last=[False, False], prunning=True, epsilon=None, thres_prunning=0.3, use_cpu_cache=False, drop_edge=False, prob_drop_edge=0.0)

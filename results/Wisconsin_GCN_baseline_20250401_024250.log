[2025-04-01 02:44:08,595]: repeat 1/3
[2025-04-01 02:44:08,595]: Manual random seed:0
[2025-04-01 02:44:08,596]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:08,602]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:08,641]: Epoch: 001, Loss:1.6921 Train: 0.4250, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0131
[2025-04-01 02:44:08,646]: Epoch: 002, Loss:1.9859 Train: 0.5833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0049
[2025-04-01 02:44:08,652]: Epoch: 003, Loss:1.3584 Train: 0.6333, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:08,658]: Epoch: 004, Loss:1.1538 Train: 0.6583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:08,665]: Epoch: 005, Loss:1.0602 Train: 0.6750, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:08,672]: Epoch: 006, Loss:1.0910 Train: 0.6750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:08,679]: Epoch: 007, Loss:1.0487 Train: 0.6833, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:08,687]: Epoch: 008, Loss:0.9404 Train: 0.6917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:08,694]: Epoch: 009, Loss:0.9468 Train: 0.6667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:08,701]: Epoch: 010, Loss:0.8333 Train: 0.6833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:08,708]: Epoch: 011, Loss:0.8996 Train: 0.7167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:08,715]: Epoch: 012, Loss:0.7880 Train: 0.7333, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:08,720]: Epoch: 013, Loss:0.8421 Train: 0.7500, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0055
[2025-04-01 02:44:08,727]: Epoch: 014, Loss:0.7808 Train: 0.7583, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:08,734]: Epoch: 015, Loss:0.8006 Train: 0.7500, Val:0.4500, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:08,741]: Epoch: 016, Loss:0.6911 Train: 0.6917, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:08,749]: Epoch: 017, Loss:0.7681 Train: 0.6833, Val:0.2875, Test: 0.3137, Time(s/epoch):0.0082
[2025-04-01 02:44:08,756]: Epoch: 018, Loss:0.7969 Train: 0.7583, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:08,764]: Epoch: 019, Loss:0.8291 Train: 0.7667, Val:0.4625, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:08,771]: Epoch: 020, Loss:0.7394 Train: 0.7750, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:08,779]: Epoch: 021, Loss:0.6785 Train: 0.7167, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:08,787]: Epoch: 022, Loss:0.7333 Train: 0.7333, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:08,794]: Epoch: 023, Loss:0.7020 Train: 0.7833, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:08,800]: Epoch: 024, Loss:0.6140 Train: 0.8083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0056
[2025-04-01 02:44:08,807]: Epoch: 025, Loss:0.6542 Train: 0.8000, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:08,814]: Epoch: 026, Loss:0.6403 Train: 0.7667, Val:0.4875, Test: 0.3529, Time(s/epoch):0.0069
[2025-04-01 02:44:08,821]: Epoch: 027, Loss:0.5697 Train: 0.7917, Val:0.4625, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:08,828]: Epoch: 028, Loss:0.5941 Train: 0.8000, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:08,835]: Epoch: 029, Loss:0.6217 Train: 0.8167, Val:0.4750, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:08,843]: Epoch: 030, Loss:0.5717 Train: 0.7833, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:08,849]: Epoch: 031, Loss:0.5729 Train: 0.8083, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:08,857]: Epoch: 032, Loss:0.5717 Train: 0.8083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:08,864]: Epoch: 033, Loss:0.5931 Train: 0.8083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:08,872]: Epoch: 034, Loss:0.5617 Train: 0.8167, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:08,879]: Epoch: 035, Loss:0.5347 Train: 0.8250, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:08,886]: Epoch: 036, Loss:0.5223 Train: 0.8333, Val:0.4750, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:08,892]: Epoch: 037, Loss:0.5347 Train: 0.8417, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:08,899]: Epoch: 038, Loss:0.5605 Train: 0.8417, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:08,905]: Epoch: 039, Loss:0.5223 Train: 0.8500, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:08,911]: Epoch: 040, Loss:0.5224 Train: 0.8583, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:08,918]: Epoch: 041, Loss:0.5094 Train: 0.8500, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:08,925]: Epoch: 042, Loss:0.5090 Train: 0.8333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:08,931]: Epoch: 043, Loss:0.5321 Train: 0.8250, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:08,939]: Epoch: 044, Loss:0.4887 Train: 0.8333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:08,947]: Epoch: 045, Loss:0.5112 Train: 0.8583, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:08,954]: Epoch: 046, Loss:0.4784 Train: 0.8500, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:08,962]: Epoch: 047, Loss:0.5178 Train: 0.8417, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:08,970]: Epoch: 048, Loss:0.4687 Train: 0.8667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:08,977]: Epoch: 049, Loss:0.4601 Train: 0.8500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:08,984]: Epoch: 050, Loss:0.4599 Train: 0.8583, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:08,990]: Epoch: 051, Loss:0.4969 Train: 0.8667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:08,997]: Epoch: 052, Loss:0.4814 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:09,005]: Epoch: 053, Loss:0.4767 Train: 0.8667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:09,012]: Epoch: 054, Loss:0.4526 Train: 0.8417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:09,018]: Epoch: 055, Loss:0.5074 Train: 0.8750, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:09,025]: Epoch: 056, Loss:0.4296 Train: 0.8917, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:09,033]: Epoch: 057, Loss:0.3935 Train: 0.8750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:09,039]: Epoch: 058, Loss:0.4456 Train: 0.8833, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:09,047]: Epoch: 059, Loss:0.4505 Train: 0.9000, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:09,054]: Epoch: 060, Loss:0.4204 Train: 0.8583, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:09,061]: Epoch: 061, Loss:0.4536 Train: 0.8417, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:09,068]: Epoch: 062, Loss:0.4191 Train: 0.8750, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:09,075]: Epoch: 063, Loss:0.4433 Train: 0.8583, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:09,081]: Epoch: 064, Loss:0.5370 Train: 0.8750, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:09,088]: Epoch: 065, Loss:0.4771 Train: 0.8750, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:09,096]: Epoch: 066, Loss:0.4368 Train: 0.8667, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:09,104]: Epoch: 067, Loss:0.3943 Train: 0.8750, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:09,111]: Epoch: 068, Loss:0.4934 Train: 0.8750, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:09,117]: Epoch: 069, Loss:0.4561 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:09,124]: Epoch: 070, Loss:0.4157 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:09,131]: Epoch: 071, Loss:0.4673 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:09,138]: Epoch: 072, Loss:0.3909 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:09,144]: Epoch: 073, Loss:0.4482 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:09,151]: Epoch: 074, Loss:0.3886 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:09,157]: Epoch: 075, Loss:0.4006 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:09,165]: Epoch: 076, Loss:0.3928 Train: 0.8750, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:44:09,171]: Epoch: 077, Loss:0.4357 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:09,179]: Epoch: 078, Loss:0.4716 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:09,185]: Epoch: 079, Loss:0.4003 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:09,192]: Epoch: 080, Loss:0.4338 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:09,199]: Epoch: 081, Loss:0.4577 Train: 0.8833, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:09,207]: Epoch: 082, Loss:0.3890 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:09,213]: Epoch: 083, Loss:0.3712 Train: 0.8917, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:09,221]: Epoch: 084, Loss:0.4049 Train: 0.8667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:09,228]: Epoch: 085, Loss:0.4031 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:09,236]: Epoch: 086, Loss:0.3883 Train: 0.8750, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:09,244]: Epoch: 087, Loss:0.5522 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:09,250]: Epoch: 088, Loss:0.4190 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:09,257]: Epoch: 089, Loss:0.3910 Train: 0.8833, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:09,264]: Epoch: 090, Loss:0.4562 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:09,270]: Epoch: 091, Loss:0.4074 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:09,277]: Epoch: 092, Loss:0.3783 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:09,285]: Epoch: 093, Loss:0.4620 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:09,292]: Epoch: 094, Loss:0.4095 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:09,299]: Epoch: 095, Loss:0.3851 Train: 0.8417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:09,306]: Epoch: 096, Loss:0.4681 Train: 0.8583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:09,314]: Epoch: 097, Loss:0.4275 Train: 0.8917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:09,320]: Epoch: 098, Loss:0.3434 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0058
[2025-04-01 02:44:09,327]: Epoch: 099, Loss:0.4609 Train: 0.8667, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:09,334]: Epoch: 100, Loss:0.4282 Train: 0.8917, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:09,342]: Epoch: 101, Loss:0.3925 Train: 0.8583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:09,349]: Epoch: 102, Loss:0.3740 Train: 0.8500, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:09,356]: Epoch: 103, Loss:0.5295 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:09,364]: Epoch: 104, Loss:0.4333 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:09,370]: Epoch: 105, Loss:0.4095 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:09,377]: Epoch: 106, Loss:0.3866 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:09,385]: Epoch: 107, Loss:0.4327 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:09,392]: Epoch: 108, Loss:0.4124 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:09,398]: Epoch: 109, Loss:0.3922 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:09,406]: Epoch: 110, Loss:0.3859 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:09,414]: Epoch: 111, Loss:0.4144 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:09,420]: Epoch: 112, Loss:0.4225 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:09,426]: Epoch: 113, Loss:0.3521 Train: 0.8917, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:09,432]: Epoch: 114, Loss:0.3370 Train: 0.9000, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:09,439]: Epoch: 115, Loss:0.4053 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:09,446]: Epoch: 116, Loss:0.3905 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:09,453]: Epoch: 117, Loss:0.3467 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:09,460]: Epoch: 118, Loss:0.3596 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:09,468]: Epoch: 119, Loss:0.3612 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:09,475]: Epoch: 120, Loss:0.3512 Train: 0.9333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:09,482]: Epoch: 121, Loss:0.3649 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:09,488]: Epoch: 122, Loss:0.3575 Train: 0.8750, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:09,496]: Epoch: 123, Loss:0.4635 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:09,502]: Epoch: 124, Loss:0.3667 Train: 0.8750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:09,509]: Epoch: 125, Loss:0.4746 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:09,517]: Epoch: 126, Loss:0.4634 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:09,524]: Epoch: 127, Loss:0.3749 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:09,532]: Epoch: 128, Loss:0.4335 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:09,539]: Epoch: 129, Loss:0.3708 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:09,547]: Epoch: 130, Loss:0.3906 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:09,553]: Epoch: 131, Loss:0.3830 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:09,559]: Epoch: 132, Loss:0.3554 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:09,566]: Epoch: 133, Loss:0.3567 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:09,573]: Epoch: 134, Loss:0.4018 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:09,579]: Epoch: 135, Loss:0.3733 Train: 0.8750, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:09,586]: Epoch: 136, Loss:0.3730 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:09,594]: Epoch: 137, Loss:0.3415 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:09,600]: Epoch: 138, Loss:0.3526 Train: 0.9083, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:09,607]: Epoch: 139, Loss:0.3375 Train: 0.9083, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:09,615]: Epoch: 140, Loss:0.3312 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:09,621]: Epoch: 141, Loss:0.3237 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:09,628]: Epoch: 142, Loss:0.3359 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:09,635]: Epoch: 143, Loss:0.3239 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:09,644]: Epoch: 144, Loss:0.3538 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:09,651]: Epoch: 145, Loss:0.3588 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:09,658]: Epoch: 146, Loss:0.3364 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:09,666]: Epoch: 147, Loss:0.4459 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:09,674]: Epoch: 148, Loss:0.3773 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:09,681]: Epoch: 149, Loss:0.3644 Train: 0.9167, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:09,689]: Epoch: 150, Loss:0.4067 Train: 0.9333, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:09,697]: Epoch: 151, Loss:0.3401 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:09,703]: Epoch: 152, Loss:0.3860 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:09,711]: Epoch: 153, Loss:0.3658 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:09,719]: Epoch: 154, Loss:0.3689 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:09,728]: Epoch: 155, Loss:0.3593 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:09,734]: Epoch: 156, Loss:0.3266 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:09,740]: Epoch: 157, Loss:0.3379 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:09,748]: Epoch: 158, Loss:0.3480 Train: 0.9333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:09,756]: Epoch: 159, Loss:0.3984 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:09,762]: Epoch: 160, Loss:0.3471 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:09,770]: Epoch: 161, Loss:0.3825 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:09,777]: Epoch: 162, Loss:0.3487 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:09,785]: Epoch: 163, Loss:0.3371 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:09,792]: Epoch: 164, Loss:0.3957 Train: 0.9167, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:09,800]: Epoch: 165, Loss:0.3478 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:09,807]: Epoch: 166, Loss:0.3601 Train: 0.9333, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:09,814]: Epoch: 167, Loss:0.3133 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:09,820]: Epoch: 168, Loss:0.3710 Train: 0.9250, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:09,827]: Epoch: 169, Loss:0.3292 Train: 0.9500, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:09,834]: Epoch: 170, Loss:0.3211 Train: 0.9500, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:09,842]: Epoch: 171, Loss:0.3099 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:09,849]: Epoch: 172, Loss:0.3860 Train: 0.9083, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:09,856]: Epoch: 173, Loss:0.3281 Train: 0.9333, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:09,864]: Epoch: 174, Loss:0.3600 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:09,870]: Epoch: 175, Loss:0.3472 Train: 0.9250, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:09,876]: Epoch: 176, Loss:0.3286 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:09,883]: Epoch: 177, Loss:0.3757 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:09,889]: Epoch: 178, Loss:0.3933 Train: 0.9417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:09,897]: Epoch: 179, Loss:0.2983 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:09,904]: Epoch: 180, Loss:0.3938 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:09,911]: Epoch: 181, Loss:0.4059 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:09,920]: Epoch: 182, Loss:0.4103 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:09,928]: Epoch: 183, Loss:0.3734 Train: 0.8583, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:09,935]: Epoch: 184, Loss:0.4405 Train: 0.8667, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:09,941]: Epoch: 185, Loss:0.3568 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:09,949]: Epoch: 186, Loss:0.3354 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:09,956]: Epoch: 187, Loss:0.3500 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:09,964]: Epoch: 188, Loss:0.3722 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:09,971]: Epoch: 189, Loss:0.3395 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:09,979]: Epoch: 190, Loss:0.3423 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:09,986]: Epoch: 191, Loss:0.3898 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:09,994]: Epoch: 192, Loss:0.4281 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:10,000]: Epoch: 193, Loss:0.3862 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0057
[2025-04-01 02:44:10,008]: Epoch: 194, Loss:0.3771 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:10,014]: Epoch: 195, Loss:0.3532 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:10,023]: Epoch: 196, Loss:0.3220 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:10,030]: Epoch: 197, Loss:0.3925 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:10,037]: Epoch: 198, Loss:0.3396 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:10,045]: Epoch: 199, Loss:0.3498 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:10,054]: Epoch: 200, Loss:0.3416 Train: 0.9417, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:10,054]: [Run-1 score] {'train': 0.425, 'val': 0.5125, 'test': 0.5294117647058824}
[2025-04-01 02:44:10,054]: repeat 2/3
[2025-04-01 02:44:10,054]: Manual random seed:0
[2025-04-01 02:44:10,054]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:10,057]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:10,067]: Epoch: 001, Loss:1.6484 Train: 0.4750, Val:0.5375, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:10,074]: Epoch: 002, Loss:1.7944 Train: 0.4500, Val:0.3875, Test: 0.2941, Time(s/epoch):0.0068
[2025-04-01 02:44:10,083]: Epoch: 003, Loss:1.3712 Train: 0.6333, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:10,091]: Epoch: 004, Loss:1.1480 Train: 0.6167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:10,099]: Epoch: 005, Loss:1.0050 Train: 0.5667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:10,107]: Epoch: 006, Loss:1.1040 Train: 0.6333, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:10,116]: Epoch: 007, Loss:1.0216 Train: 0.6583, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:10,123]: Epoch: 008, Loss:1.0341 Train: 0.6667, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:10,131]: Epoch: 009, Loss:0.9762 Train: 0.7000, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:10,138]: Epoch: 010, Loss:0.9546 Train: 0.7000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,146]: Epoch: 011, Loss:0.8768 Train: 0.6667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:10,154]: Epoch: 012, Loss:0.7876 Train: 0.7167, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:10,161]: Epoch: 013, Loss:0.9261 Train: 0.7083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:10,169]: Epoch: 014, Loss:0.8998 Train: 0.7167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:10,177]: Epoch: 015, Loss:0.8507 Train: 0.7083, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:10,185]: Epoch: 016, Loss:0.7448 Train: 0.7333, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:10,192]: Epoch: 017, Loss:0.7479 Train: 0.7333, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:10,200]: Epoch: 018, Loss:0.7855 Train: 0.7500, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:10,207]: Epoch: 019, Loss:0.7292 Train: 0.7583, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:10,214]: Epoch: 020, Loss:0.7352 Train: 0.7583, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:10,221]: Epoch: 021, Loss:0.6867 Train: 0.7500, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:10,229]: Epoch: 022, Loss:0.6623 Train: 0.7750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:10,238]: Epoch: 023, Loss:0.7311 Train: 0.7833, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:10,246]: Epoch: 024, Loss:0.5948 Train: 0.7667, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:10,253]: Epoch: 025, Loss:0.6398 Train: 0.8000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:10,261]: Epoch: 026, Loss:0.6211 Train: 0.7583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:10,269]: Epoch: 027, Loss:0.6445 Train: 0.7667, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:10,276]: Epoch: 028, Loss:0.5733 Train: 0.7750, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:44:10,284]: Epoch: 029, Loss:0.6131 Train: 0.8000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:10,291]: Epoch: 030, Loss:0.5748 Train: 0.8000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:10,298]: Epoch: 031, Loss:0.5211 Train: 0.8000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:10,305]: Epoch: 032, Loss:0.5870 Train: 0.8083, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:10,311]: Epoch: 033, Loss:0.6114 Train: 0.8083, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:10,319]: Epoch: 034, Loss:0.5624 Train: 0.8083, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:10,327]: Epoch: 035, Loss:0.5518 Train: 0.8250, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:10,333]: Epoch: 036, Loss:0.5316 Train: 0.8417, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:10,339]: Epoch: 037, Loss:0.5218 Train: 0.8333, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:10,347]: Epoch: 038, Loss:0.5526 Train: 0.8250, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:10,353]: Epoch: 039, Loss:0.5499 Train: 0.8417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:10,360]: Epoch: 040, Loss:0.4954 Train: 0.8333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,367]: Epoch: 041, Loss:0.5220 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:10,373]: Epoch: 042, Loss:0.5128 Train: 0.8333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:10,381]: Epoch: 043, Loss:0.5388 Train: 0.8250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:10,389]: Epoch: 044, Loss:0.5560 Train: 0.8250, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:10,396]: Epoch: 045, Loss:0.5176 Train: 0.8333, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:10,402]: Epoch: 046, Loss:0.5057 Train: 0.8167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:44:10,409]: Epoch: 047, Loss:0.4543 Train: 0.8333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:10,416]: Epoch: 048, Loss:0.5417 Train: 0.8500, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:10,424]: Epoch: 049, Loss:0.4923 Train: 0.8333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:10,431]: Epoch: 050, Loss:0.4652 Train: 0.8500, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:10,439]: Epoch: 051, Loss:0.4950 Train: 0.8417, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:10,446]: Epoch: 052, Loss:0.5087 Train: 0.8583, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:44:10,452]: Epoch: 053, Loss:0.4973 Train: 0.8667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:10,459]: Epoch: 054, Loss:0.4722 Train: 0.8583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,466]: Epoch: 055, Loss:0.4836 Train: 0.8250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:10,472]: Epoch: 056, Loss:0.4908 Train: 0.8250, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:10,480]: Epoch: 057, Loss:0.5162 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:10,488]: Epoch: 058, Loss:0.4461 Train: 0.8583, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:10,496]: Epoch: 059, Loss:0.4963 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:10,504]: Epoch: 060, Loss:0.4825 Train: 0.8583, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:10,511]: Epoch: 061, Loss:0.4122 Train: 0.8750, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:10,520]: Epoch: 062, Loss:0.4524 Train: 0.8833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:10,527]: Epoch: 063, Loss:0.4482 Train: 0.8917, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:10,533]: Epoch: 064, Loss:0.4817 Train: 0.8750, Val:0.4500, Test: 0.3725, Time(s/epoch):0.0062
[2025-04-01 02:44:10,541]: Epoch: 065, Loss:0.4321 Train: 0.8667, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:10,548]: Epoch: 066, Loss:0.4849 Train: 0.8667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:10,556]: Epoch: 067, Loss:0.4450 Train: 0.8667, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:10,564]: Epoch: 068, Loss:0.4196 Train: 0.8750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:10,571]: Epoch: 069, Loss:0.4202 Train: 0.8750, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:10,579]: Epoch: 070, Loss:0.4001 Train: 0.9000, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:10,586]: Epoch: 071, Loss:0.4070 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:10,593]: Epoch: 072, Loss:0.5158 Train: 0.8333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:10,601]: Epoch: 073, Loss:0.4339 Train: 0.8417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:10,608]: Epoch: 074, Loss:0.4434 Train: 0.8583, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:10,615]: Epoch: 075, Loss:0.3942 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:10,621]: Epoch: 076, Loss:0.4356 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:10,629]: Epoch: 077, Loss:0.4868 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:10,637]: Epoch: 078, Loss:0.4338 Train: 0.8333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:10,646]: Epoch: 079, Loss:0.4582 Train: 0.8417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:10,653]: Epoch: 080, Loss:0.4821 Train: 0.8583, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:10,661]: Epoch: 081, Loss:0.4109 Train: 0.8750, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:10,669]: Epoch: 082, Loss:0.4386 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:10,677]: Epoch: 083, Loss:0.4359 Train: 0.8500, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:10,685]: Epoch: 084, Loss:0.3985 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:10,693]: Epoch: 085, Loss:0.4221 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:10,700]: Epoch: 086, Loss:0.4158 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:10,708]: Epoch: 087, Loss:0.4326 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:10,716]: Epoch: 088, Loss:0.3900 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:10,723]: Epoch: 089, Loss:0.4800 Train: 0.8667, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:10,729]: Epoch: 090, Loss:0.4152 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:10,736]: Epoch: 091, Loss:0.4490 Train: 0.9250, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:10,744]: Epoch: 092, Loss:0.4809 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:10,752]: Epoch: 093, Loss:0.4692 Train: 0.8750, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:10,759]: Epoch: 094, Loss:0.4314 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:10,766]: Epoch: 095, Loss:0.4157 Train: 0.8833, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:10,773]: Epoch: 096, Loss:0.4205 Train: 0.8583, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:10,780]: Epoch: 097, Loss:0.5344 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,787]: Epoch: 098, Loss:0.3677 Train: 0.8667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:10,794]: Epoch: 099, Loss:0.4326 Train: 0.8667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:10,801]: Epoch: 100, Loss:0.4057 Train: 0.8750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:10,808]: Epoch: 101, Loss:0.4087 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:10,815]: Epoch: 102, Loss:0.3710 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,823]: Epoch: 103, Loss:0.4259 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:10,830]: Epoch: 104, Loss:0.3805 Train: 0.9000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:10,836]: Epoch: 105, Loss:0.3981 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:10,844]: Epoch: 106, Loss:0.3861 Train: 0.8833, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:10,851]: Epoch: 107, Loss:0.4154 Train: 0.8917, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:10,858]: Epoch: 108, Loss:0.3857 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:10,865]: Epoch: 109, Loss:0.3725 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:10,873]: Epoch: 110, Loss:0.4281 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:10,879]: Epoch: 111, Loss:0.3911 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:10,886]: Epoch: 112, Loss:0.3129 Train: 0.8833, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:10,894]: Epoch: 113, Loss:0.4501 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:10,901]: Epoch: 114, Loss:0.3801 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:10,908]: Epoch: 115, Loss:0.4985 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:10,915]: Epoch: 116, Loss:0.4360 Train: 0.8750, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:10,922]: Epoch: 117, Loss:0.3974 Train: 0.8417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:10,930]: Epoch: 118, Loss:0.4111 Train: 0.8417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:10,938]: Epoch: 119, Loss:0.3748 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:10,946]: Epoch: 120, Loss:0.3746 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:10,953]: Epoch: 121, Loss:0.4373 Train: 0.9000, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:10,959]: Epoch: 122, Loss:0.4069 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:10,966]: Epoch: 123, Loss:0.3696 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:10,973]: Epoch: 124, Loss:0.3066 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:10,980]: Epoch: 125, Loss:0.3932 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:10,987]: Epoch: 126, Loss:0.3786 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:10,993]: Epoch: 127, Loss:0.3680 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:11,001]: Epoch: 128, Loss:0.3720 Train: 0.8833, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:11,008]: Epoch: 129, Loss:0.3702 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:11,015]: Epoch: 130, Loss:0.4042 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:11,023]: Epoch: 131, Loss:0.4381 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:11,029]: Epoch: 132, Loss:0.3525 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:11,037]: Epoch: 133, Loss:0.3179 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:11,044]: Epoch: 134, Loss:0.3453 Train: 0.9250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:11,051]: Epoch: 135, Loss:0.3674 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:11,058]: Epoch: 136, Loss:0.3720 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:11,066]: Epoch: 137, Loss:0.3361 Train: 0.9250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:11,074]: Epoch: 138, Loss:0.2992 Train: 0.9250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:11,080]: Epoch: 139, Loss:0.3658 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0056
[2025-04-01 02:44:11,087]: Epoch: 140, Loss:0.3542 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:11,094]: Epoch: 141, Loss:0.4262 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:11,100]: Epoch: 142, Loss:0.3538 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:11,107]: Epoch: 143, Loss:0.3931 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:11,113]: Epoch: 144, Loss:0.3904 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:11,120]: Epoch: 145, Loss:0.3667 Train: 0.8750, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:11,127]: Epoch: 146, Loss:0.3847 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:11,134]: Epoch: 147, Loss:0.4091 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:11,141]: Epoch: 148, Loss:0.4054 Train: 0.9167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:11,149]: Epoch: 149, Loss:0.3908 Train: 0.8833, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:11,156]: Epoch: 150, Loss:0.4432 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:11,165]: Epoch: 151, Loss:0.4935 Train: 0.9000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:11,174]: Epoch: 152, Loss:0.3414 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:11,182]: Epoch: 153, Loss:0.3892 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:11,190]: Epoch: 154, Loss:0.4010 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:11,198]: Epoch: 155, Loss:0.3601 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:11,207]: Epoch: 156, Loss:0.3582 Train: 0.8833, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:11,213]: Epoch: 157, Loss:0.4190 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:11,221]: Epoch: 158, Loss:0.3297 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:11,228]: Epoch: 159, Loss:0.3555 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:11,236]: Epoch: 160, Loss:0.3561 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:11,244]: Epoch: 161, Loss:0.3452 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:11,253]: Epoch: 162, Loss:0.3057 Train: 0.9083, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:11,261]: Epoch: 163, Loss:0.3098 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:11,267]: Epoch: 164, Loss:0.3463 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:11,275]: Epoch: 165, Loss:0.4055 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:11,284]: Epoch: 166, Loss:0.3672 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:11,292]: Epoch: 167, Loss:0.3468 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:11,298]: Epoch: 168, Loss:0.3737 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:11,305]: Epoch: 169, Loss:0.3353 Train: 0.8667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:11,311]: Epoch: 170, Loss:0.4391 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:11,319]: Epoch: 171, Loss:0.3746 Train: 0.8750, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:11,326]: Epoch: 172, Loss:0.4066 Train: 0.8500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:11,334]: Epoch: 173, Loss:0.4853 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:11,342]: Epoch: 174, Loss:0.3969 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:11,349]: Epoch: 175, Loss:0.4470 Train: 0.8583, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:11,357]: Epoch: 176, Loss:0.4234 Train: 0.8417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:11,364]: Epoch: 177, Loss:0.4070 Train: 0.8750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:11,371]: Epoch: 178, Loss:0.3541 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:11,377]: Epoch: 179, Loss:0.4010 Train: 0.9167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:11,384]: Epoch: 180, Loss:0.3902 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:11,391]: Epoch: 181, Loss:0.5223 Train: 0.9250, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:11,398]: Epoch: 182, Loss:0.3697 Train: 0.8833, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:11,405]: Epoch: 183, Loss:0.3606 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:11,413]: Epoch: 184, Loss:0.3258 Train: 0.9000, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:11,419]: Epoch: 185, Loss:0.3773 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:11,427]: Epoch: 186, Loss:0.3928 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:11,436]: Epoch: 187, Loss:0.3598 Train: 0.8750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:11,443]: Epoch: 188, Loss:0.3758 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:11,450]: Epoch: 189, Loss:0.3678 Train: 0.8833, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:11,458]: Epoch: 190, Loss:0.3716 Train: 0.9167, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:11,465]: Epoch: 191, Loss:0.4381 Train: 0.9167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:11,473]: Epoch: 192, Loss:0.3811 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:11,479]: Epoch: 193, Loss:0.3944 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:11,487]: Epoch: 194, Loss:0.3615 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:11,493]: Epoch: 195, Loss:0.3910 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:11,500]: Epoch: 196, Loss:0.3320 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:11,507]: Epoch: 197, Loss:0.3746 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:11,514]: Epoch: 198, Loss:0.3078 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:11,521]: Epoch: 199, Loss:0.3435 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:11,528]: Epoch: 200, Loss:0.3788 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:11,528]: [Run-2 score] {'train': 0.475, 'val': 0.5375, 'test': 0.5490196078431373}
[2025-04-01 02:44:11,528]: repeat 3/3
[2025-04-01 02:44:11,528]: Manual random seed:0
[2025-04-01 02:44:11,529]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:11,531]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:11,541]: Epoch: 001, Loss:1.6794 Train: 0.5417, Val:0.5750, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:44:11,548]: Epoch: 002, Loss:1.3121 Train: 0.5750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:11,554]: Epoch: 003, Loss:1.1491 Train: 0.6167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:11,561]: Epoch: 004, Loss:1.0551 Train: 0.6750, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:11,568]: Epoch: 005, Loss:0.9940 Train: 0.6833, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:11,577]: Epoch: 006, Loss:0.8548 Train: 0.7083, Val:0.5500, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:44:11,584]: Epoch: 007, Loss:0.8844 Train: 0.7167, Val:0.5375, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:44:11,590]: Epoch: 008, Loss:0.8582 Train: 0.7250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0056
[2025-04-01 02:44:11,597]: Epoch: 009, Loss:0.8401 Train: 0.7417, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:11,604]: Epoch: 010, Loss:0.7354 Train: 0.7333, Val:0.4000, Test: 0.3333, Time(s/epoch):0.0074
[2025-04-01 02:44:11,612]: Epoch: 011, Loss:0.8767 Train: 0.7667, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:11,620]: Epoch: 012, Loss:0.7120 Train: 0.7667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:11,627]: Epoch: 013, Loss:0.8251 Train: 0.7500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:11,635]: Epoch: 014, Loss:0.7533 Train: 0.7833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:11,643]: Epoch: 015, Loss:0.6296 Train: 0.7667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:11,650]: Epoch: 016, Loss:0.6250 Train: 0.7500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:11,658]: Epoch: 017, Loss:0.6773 Train: 0.7583, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:11,666]: Epoch: 018, Loss:0.6402 Train: 0.7750, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:11,674]: Epoch: 019, Loss:0.6059 Train: 0.7750, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:11,681]: Epoch: 020, Loss:0.5865 Train: 0.8000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:11,689]: Epoch: 021, Loss:0.5552 Train: 0.7833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:11,697]: Epoch: 022, Loss:0.5471 Train: 0.7917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:11,706]: Epoch: 023, Loss:0.5505 Train: 0.8250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0095
[2025-04-01 02:44:11,715]: Epoch: 024, Loss:0.6116 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:11,721]: Epoch: 025, Loss:0.5900 Train: 0.8667, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0054
[2025-04-01 02:44:11,729]: Epoch: 026, Loss:0.5656 Train: 0.8500, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:11,737]: Epoch: 027, Loss:0.5226 Train: 0.8667, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:11,745]: Epoch: 028, Loss:0.5404 Train: 0.8250, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:11,753]: Epoch: 029, Loss:0.5266 Train: 0.8417, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:11,761]: Epoch: 030, Loss:0.4927 Train: 0.8667, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:11,769]: Epoch: 031, Loss:0.5504 Train: 0.8667, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:11,777]: Epoch: 032, Loss:0.4933 Train: 0.8417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:11,784]: Epoch: 033, Loss:0.4757 Train: 0.8333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:11,792]: Epoch: 034, Loss:0.5583 Train: 0.8333, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:11,800]: Epoch: 035, Loss:0.6649 Train: 0.8500, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:11,808]: Epoch: 036, Loss:0.4860 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:11,817]: Epoch: 037, Loss:0.5200 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:11,825]: Epoch: 038, Loss:0.5058 Train: 0.8667, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:11,833]: Epoch: 039, Loss:0.4981 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:11,839]: Epoch: 040, Loss:0.5010 Train: 0.8667, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0057
[2025-04-01 02:44:11,846]: Epoch: 041, Loss:0.4579 Train: 0.8583, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:11,852]: Epoch: 042, Loss:0.4886 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:11,859]: Epoch: 043, Loss:0.4298 Train: 0.8583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:11,866]: Epoch: 044, Loss:0.4784 Train: 0.8333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:11,873]: Epoch: 045, Loss:0.4838 Train: 0.8417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:11,879]: Epoch: 046, Loss:0.4889 Train: 0.8333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:11,886]: Epoch: 047, Loss:0.4654 Train: 0.8417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:11,893]: Epoch: 048, Loss:0.4457 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:11,900]: Epoch: 049, Loss:0.4372 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:11,907]: Epoch: 050, Loss:0.4613 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:11,915]: Epoch: 051, Loss:0.4506 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:11,923]: Epoch: 052, Loss:0.4073 Train: 0.8833, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:11,930]: Epoch: 053, Loss:0.3974 Train: 0.8583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:11,937]: Epoch: 054, Loss:0.4136 Train: 0.8583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:11,944]: Epoch: 055, Loss:0.4189 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:11,952]: Epoch: 056, Loss:0.3758 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:11,960]: Epoch: 057, Loss:0.3912 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:11,967]: Epoch: 058, Loss:0.4241 Train: 0.9000, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:11,975]: Epoch: 059, Loss:0.4208 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:11,981]: Epoch: 060, Loss:0.4152 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:11,989]: Epoch: 061, Loss:0.4010 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:11,995]: Epoch: 062, Loss:0.4278 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:12,003]: Epoch: 063, Loss:0.4175 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:12,009]: Epoch: 064, Loss:0.4211 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:12,017]: Epoch: 065, Loss:0.3958 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:12,025]: Epoch: 066, Loss:0.4113 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:12,031]: Epoch: 067, Loss:0.4880 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:12,038]: Epoch: 068, Loss:0.4055 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:12,045]: Epoch: 069, Loss:0.4030 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:12,051]: Epoch: 070, Loss:0.4238 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:12,058]: Epoch: 071, Loss:0.4173 Train: 0.8917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:12,066]: Epoch: 072, Loss:0.3563 Train: 0.8667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:12,074]: Epoch: 073, Loss:0.3932 Train: 0.8583, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:12,080]: Epoch: 074, Loss:0.4620 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:44:12,087]: Epoch: 075, Loss:0.3969 Train: 0.9083, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:12,094]: Epoch: 076, Loss:0.3465 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:12,103]: Epoch: 077, Loss:0.4544 Train: 0.8750, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:12,110]: Epoch: 078, Loss:0.4472 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:12,118]: Epoch: 079, Loss:0.4194 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:12,126]: Epoch: 080, Loss:0.5079 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:12,132]: Epoch: 081, Loss:0.3755 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:12,141]: Epoch: 082, Loss:0.4679 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:12,150]: Epoch: 083, Loss:0.4207 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:12,159]: Epoch: 084, Loss:0.3625 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:12,168]: Epoch: 085, Loss:0.4053 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:12,178]: Epoch: 086, Loss:0.4191 Train: 0.8833, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:12,187]: Epoch: 087, Loss:0.3831 Train: 0.8833, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:12,196]: Epoch: 088, Loss:0.4110 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:12,206]: Epoch: 089, Loss:0.3924 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0100
[2025-04-01 02:44:12,215]: Epoch: 090, Loss:0.3432 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0089
[2025-04-01 02:44:12,225]: Epoch: 091, Loss:0.4317 Train: 0.8833, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0093
[2025-04-01 02:44:12,234]: Epoch: 092, Loss:0.3777 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:12,243]: Epoch: 093, Loss:0.3672 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0095
[2025-04-01 02:44:12,252]: Epoch: 094, Loss:0.3715 Train: 0.9333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:12,265]: Epoch: 095, Loss:0.3210 Train: 0.9333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0125
[2025-04-01 02:44:12,276]: Epoch: 096, Loss:0.3549 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0110
[2025-04-01 02:44:12,287]: Epoch: 097, Loss:0.3413 Train: 0.9167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0110
[2025-04-01 02:44:12,296]: Epoch: 098, Loss:0.4150 Train: 0.9333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:12,304]: Epoch: 099, Loss:0.3883 Train: 0.9250, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:12,312]: Epoch: 100, Loss:0.3677 Train: 0.9250, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:12,320]: Epoch: 101, Loss:0.3378 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:12,328]: Epoch: 102, Loss:0.4165 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:12,336]: Epoch: 103, Loss:0.3743 Train: 0.9000, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:12,343]: Epoch: 104, Loss:0.3660 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:12,351]: Epoch: 105, Loss:0.3711 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:12,359]: Epoch: 106, Loss:0.3201 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:12,367]: Epoch: 107, Loss:0.3188 Train: 0.9000, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:12,376]: Epoch: 108, Loss:0.4388 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0090
[2025-04-01 02:44:12,384]: Epoch: 109, Loss:0.3687 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:12,391]: Epoch: 110, Loss:0.3550 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:12,399]: Epoch: 111, Loss:0.3414 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:12,408]: Epoch: 112, Loss:0.3826 Train: 0.8750, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:12,414]: Epoch: 113, Loss:0.3575 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:12,423]: Epoch: 114, Loss:0.3531 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:12,430]: Epoch: 115, Loss:0.4997 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:12,439]: Epoch: 116, Loss:0.4312 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:12,445]: Epoch: 117, Loss:0.3947 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:12,453]: Epoch: 118, Loss:0.4203 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:12,460]: Epoch: 119, Loss:0.3916 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:12,468]: Epoch: 120, Loss:0.3726 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:12,476]: Epoch: 121, Loss:0.3883 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:12,484]: Epoch: 122, Loss:0.3845 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:12,491]: Epoch: 123, Loss:0.3858 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:12,500]: Epoch: 124, Loss:0.3440 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:12,508]: Epoch: 125, Loss:0.3793 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:12,515]: Epoch: 126, Loss:0.4733 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:12,523]: Epoch: 127, Loss:0.4152 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:12,530]: Epoch: 128, Loss:0.5351 Train: 0.8750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:12,537]: Epoch: 129, Loss:0.3550 Train: 0.8583, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:12,545]: Epoch: 130, Loss:0.4311 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:12,552]: Epoch: 131, Loss:0.3963 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:12,561]: Epoch: 132, Loss:0.4314 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:12,569]: Epoch: 133, Loss:0.4046 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:12,577]: Epoch: 134, Loss:0.3716 Train: 0.9000, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:12,584]: Epoch: 135, Loss:0.3618 Train: 0.8917, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:12,592]: Epoch: 136, Loss:0.3590 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:12,599]: Epoch: 137, Loss:0.3689 Train: 0.8750, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:12,606]: Epoch: 138, Loss:0.3339 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:12,612]: Epoch: 139, Loss:0.3500 Train: 0.8917, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0057
[2025-04-01 02:44:12,620]: Epoch: 140, Loss:0.3420 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:12,627]: Epoch: 141, Loss:0.3240 Train: 0.9167, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:12,635]: Epoch: 142, Loss:0.3957 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:12,643]: Epoch: 143, Loss:0.3122 Train: 0.9417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:12,650]: Epoch: 144, Loss:0.3478 Train: 0.9333, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:12,657]: Epoch: 145, Loss:0.3390 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:12,665]: Epoch: 146, Loss:0.3690 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:12,672]: Epoch: 147, Loss:0.3693 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:12,680]: Epoch: 148, Loss:0.3878 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:12,688]: Epoch: 149, Loss:0.2989 Train: 0.8833, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:12,696]: Epoch: 150, Loss:0.3669 Train: 0.9000, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:12,704]: Epoch: 151, Loss:0.3336 Train: 0.9167, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:12,713]: Epoch: 152, Loss:0.3480 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:12,721]: Epoch: 153, Loss:0.3738 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:12,730]: Epoch: 154, Loss:0.3836 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:12,738]: Epoch: 155, Loss:0.3735 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:12,746]: Epoch: 156, Loss:0.3703 Train: 0.8750, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:12,754]: Epoch: 157, Loss:0.5223 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:12,760]: Epoch: 158, Loss:0.4187 Train: 0.8917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:12,770]: Epoch: 159, Loss:0.4874 Train: 0.8667, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0091
[2025-04-01 02:44:12,778]: Epoch: 160, Loss:0.4525 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:12,787]: Epoch: 161, Loss:0.4102 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0091
[2025-04-01 02:44:12,795]: Epoch: 162, Loss:0.3926 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:12,803]: Epoch: 163, Loss:0.3808 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:12,812]: Epoch: 164, Loss:0.3939 Train: 0.8417, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:12,820]: Epoch: 165, Loss:0.3808 Train: 0.8667, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:12,828]: Epoch: 166, Loss:0.3808 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:12,836]: Epoch: 167, Loss:0.3899 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:12,842]: Epoch: 168, Loss:0.3946 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0056
[2025-04-01 02:44:12,849]: Epoch: 169, Loss:0.3779 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:12,857]: Epoch: 170, Loss:0.3315 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:12,864]: Epoch: 171, Loss:0.3413 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:12,871]: Epoch: 172, Loss:0.3307 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:12,878]: Epoch: 173, Loss:0.3778 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:12,886]: Epoch: 174, Loss:0.3279 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:12,893]: Epoch: 175, Loss:0.3706 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:12,901]: Epoch: 176, Loss:0.3712 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:12,910]: Epoch: 177, Loss:0.3681 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:12,916]: Epoch: 178, Loss:0.4083 Train: 0.9083, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:12,923]: Epoch: 179, Loss:0.4003 Train: 0.8750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:12,931]: Epoch: 180, Loss:0.4259 Train: 0.8667, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:12,939]: Epoch: 181, Loss:0.4009 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:12,948]: Epoch: 182, Loss:0.4014 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:12,954]: Epoch: 183, Loss:0.3683 Train: 0.8667, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:12,961]: Epoch: 184, Loss:0.4438 Train: 0.8500, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:12,970]: Epoch: 185, Loss:0.4415 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:12,977]: Epoch: 186, Loss:0.4288 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:12,984]: Epoch: 187, Loss:0.4106 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:12,992]: Epoch: 188, Loss:0.3621 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:13,000]: Epoch: 189, Loss:0.4051 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:13,008]: Epoch: 190, Loss:0.3499 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:13,016]: Epoch: 191, Loss:0.4134 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:13,022]: Epoch: 192, Loss:0.3844 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:13,029]: Epoch: 193, Loss:0.3495 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:13,037]: Epoch: 194, Loss:0.3489 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:13,043]: Epoch: 195, Loss:0.3588 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:13,050]: Epoch: 196, Loss:0.3375 Train: 0.9250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:13,057]: Epoch: 197, Loss:0.3539 Train: 0.8917, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:44:13,064]: Epoch: 198, Loss:0.3692 Train: 0.9000, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0072
[2025-04-01 02:44:13,072]: Epoch: 199, Loss:0.3112 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:13,080]: Epoch: 200, Loss:0.3892 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:13,080]: [Run-3 score] {'train': 0.5416666666666666, 'val': 0.575, 'test': 0.5686274509803921}
[2025-04-01 02:44:13,080]: repeat 1/3
[2025-04-01 02:44:13,080]: Manual random seed:0
[2025-04-01 02:44:13,081]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:13,083]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:13,092]: Epoch: 001, Loss:1.6669 Train: 0.4583, Val:0.3875, Test: 0.6275, Time(s/epoch):0.0067
[2025-04-01 02:44:13,099]: Epoch: 002, Loss:1.9657 Train: 0.5917, Val:0.5125, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:44:13,107]: Epoch: 003, Loss:1.3083 Train: 0.5750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:13,114]: Epoch: 004, Loss:1.1600 Train: 0.6417, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:13,122]: Epoch: 005, Loss:0.9860 Train: 0.6667, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:13,130]: Epoch: 006, Loss:0.9437 Train: 0.6583, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:13,138]: Epoch: 007, Loss:0.8967 Train: 0.7083, Val:0.4125, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:13,146]: Epoch: 008, Loss:0.8727 Train: 0.7083, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:44:13,154]: Epoch: 009, Loss:0.9300 Train: 0.7083, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:13,162]: Epoch: 010, Loss:0.7893 Train: 0.7167, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:13,169]: Epoch: 011, Loss:0.7804 Train: 0.7500, Val:0.4125, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:44:13,177]: Epoch: 012, Loss:0.7508 Train: 0.7500, Val:0.4125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:44:13,184]: Epoch: 013, Loss:0.7297 Train: 0.7583, Val:0.4125, Test: 0.6078, Time(s/epoch):0.0068
[2025-04-01 02:44:13,191]: Epoch: 014, Loss:0.7004 Train: 0.7750, Val:0.4250, Test: 0.6275, Time(s/epoch):0.0064
[2025-04-01 02:44:13,200]: Epoch: 015, Loss:0.7178 Train: 0.7667, Val:0.4375, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:44:13,208]: Epoch: 016, Loss:0.6645 Train: 0.7917, Val:0.4375, Test: 0.6078, Time(s/epoch):0.0084
[2025-04-01 02:44:13,216]: Epoch: 017, Loss:0.6180 Train: 0.8083, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:13,225]: Epoch: 018, Loss:0.6391 Train: 0.8250, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0085
[2025-04-01 02:44:13,231]: Epoch: 019, Loss:0.6626 Train: 0.8167, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:44:13,238]: Epoch: 020, Loss:0.5946 Train: 0.7750, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:44:13,247]: Epoch: 021, Loss:0.6053 Train: 0.7750, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:44:13,253]: Epoch: 022, Loss:0.6166 Train: 0.8000, Val:0.3750, Test: 0.6078, Time(s/epoch):0.0067
[2025-04-01 02:44:13,262]: Epoch: 023, Loss:0.5635 Train: 0.8333, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:13,271]: Epoch: 024, Loss:0.5703 Train: 0.8750, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:13,279]: Epoch: 025, Loss:0.5512 Train: 0.8333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:13,288]: Epoch: 026, Loss:0.5665 Train: 0.8417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:13,296]: Epoch: 027, Loss:0.6326 Train: 0.8333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:13,304]: Epoch: 028, Loss:0.5201 Train: 0.8333, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:13,312]: Epoch: 029, Loss:0.5588 Train: 0.8333, Val:0.3875, Test: 0.6078, Time(s/epoch):0.0070
[2025-04-01 02:44:13,320]: Epoch: 030, Loss:0.4899 Train: 0.8417, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:44:13,328]: Epoch: 031, Loss:0.5075 Train: 0.8583, Val:0.3625, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:44:13,336]: Epoch: 032, Loss:0.4642 Train: 0.8500, Val:0.3500, Test: 0.6078, Time(s/epoch):0.0075
[2025-04-01 02:44:13,343]: Epoch: 033, Loss:0.5077 Train: 0.8667, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:13,350]: Epoch: 034, Loss:0.4792 Train: 0.8667, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:13,357]: Epoch: 035, Loss:0.4909 Train: 0.8750, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:13,363]: Epoch: 036, Loss:0.4432 Train: 0.8417, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:13,371]: Epoch: 037, Loss:0.4832 Train: 0.8750, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:13,379]: Epoch: 038, Loss:0.5591 Train: 0.8667, Val:0.3625, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:13,387]: Epoch: 039, Loss:0.4782 Train: 0.9000, Val:0.3500, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:44:13,394]: Epoch: 040, Loss:0.4673 Train: 0.8917, Val:0.3500, Test: 0.5882, Time(s/epoch):0.0069
[2025-04-01 02:44:13,402]: Epoch: 041, Loss:0.4649 Train: 0.9083, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:13,410]: Epoch: 042, Loss:0.5144 Train: 0.9083, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:13,416]: Epoch: 043, Loss:0.4630 Train: 0.9000, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:13,423]: Epoch: 044, Loss:0.4205 Train: 0.8833, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:13,431]: Epoch: 045, Loss:0.4820 Train: 0.8750, Val:0.3625, Test: 0.6078, Time(s/epoch):0.0079
[2025-04-01 02:44:13,439]: Epoch: 046, Loss:0.4409 Train: 0.8417, Val:0.3500, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:44:13,447]: Epoch: 047, Loss:0.4490 Train: 0.8417, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:13,454]: Epoch: 048, Loss:0.5044 Train: 0.8917, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:13,462]: Epoch: 049, Loss:0.4258 Train: 0.8917, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:13,469]: Epoch: 050, Loss:0.5273 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:13,477]: Epoch: 051, Loss:0.6258 Train: 0.8917, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:44:13,484]: Epoch: 052, Loss:0.4408 Train: 0.9083, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:44:13,490]: Epoch: 053, Loss:0.4177 Train: 0.8667, Val:0.3625, Test: 0.6078, Time(s/epoch):0.0066
[2025-04-01 02:44:13,498]: Epoch: 054, Loss:0.4260 Train: 0.8333, Val:0.3625, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:44:13,505]: Epoch: 055, Loss:0.4547 Train: 0.8333, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:44:13,513]: Epoch: 056, Loss:0.4628 Train: 0.8500, Val:0.3500, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:44:13,521]: Epoch: 057, Loss:0.4217 Train: 0.9083, Val:0.4500, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:13,529]: Epoch: 058, Loss:0.4068 Train: 0.9083, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:13,536]: Epoch: 059, Loss:0.3839 Train: 0.8917, Val:0.4500, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:44:13,543]: Epoch: 060, Loss:0.3818 Train: 0.8917, Val:0.4250, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:44:13,552]: Epoch: 061, Loss:0.4047 Train: 0.8583, Val:0.3750, Test: 0.6078, Time(s/epoch):0.0086
[2025-04-01 02:44:13,560]: Epoch: 062, Loss:0.4080 Train: 0.8750, Val:0.3625, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:44:13,569]: Epoch: 063, Loss:0.4189 Train: 0.8917, Val:0.3500, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:44:13,575]: Epoch: 064, Loss:0.3898 Train: 0.9000, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:13,584]: Epoch: 065, Loss:0.3924 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:13,591]: Epoch: 066, Loss:0.3835 Train: 0.9083, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:13,601]: Epoch: 067, Loss:0.3490 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:13,608]: Epoch: 068, Loss:0.4730 Train: 0.9167, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0074
[2025-04-01 02:44:13,617]: Epoch: 069, Loss:0.3593 Train: 0.8750, Val:0.3500, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:44:13,625]: Epoch: 070, Loss:0.4283 Train: 0.9000, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:44:13,633]: Epoch: 071, Loss:0.3923 Train: 0.9167, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:44:13,641]: Epoch: 072, Loss:0.3902 Train: 0.9167, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:13,649]: Epoch: 073, Loss:0.3740 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:13,657]: Epoch: 074, Loss:0.4309 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:13,666]: Epoch: 075, Loss:0.3266 Train: 0.9083, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:44:13,673]: Epoch: 076, Loss:0.3984 Train: 0.9083, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:44:13,682]: Epoch: 077, Loss:0.3517 Train: 0.9083, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:13,689]: Epoch: 078, Loss:0.3362 Train: 0.9167, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:13,698]: Epoch: 079, Loss:0.3236 Train: 0.9250, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:44:13,707]: Epoch: 080, Loss:0.4044 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0094
[2025-04-01 02:44:13,716]: Epoch: 081, Loss:0.4101 Train: 0.9417, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:13,723]: Epoch: 082, Loss:0.3669 Train: 0.9417, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:13,732]: Epoch: 083, Loss:0.2680 Train: 0.9167, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0087
[2025-04-01 02:44:13,740]: Epoch: 084, Loss:0.3938 Train: 0.9167, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:13,748]: Epoch: 085, Loss:0.3355 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:13,756]: Epoch: 086, Loss:0.3178 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:13,765]: Epoch: 087, Loss:0.3418 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:13,772]: Epoch: 088, Loss:0.3489 Train: 0.9417, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:13,780]: Epoch: 089, Loss:0.3237 Train: 0.8917, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:44:13,788]: Epoch: 090, Loss:0.3191 Train: 0.8917, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:44:13,794]: Epoch: 091, Loss:0.3287 Train: 0.9500, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0057
[2025-04-01 02:44:13,801]: Epoch: 092, Loss:0.3165 Train: 0.9417, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:13,808]: Epoch: 093, Loss:0.3024 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:13,817]: Epoch: 094, Loss:0.3296 Train: 0.8917, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:13,824]: Epoch: 095, Loss:0.3316 Train: 0.8917, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:13,832]: Epoch: 096, Loss:0.3854 Train: 0.9083, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:13,840]: Epoch: 097, Loss:0.3348 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:13,847]: Epoch: 098, Loss:0.3413 Train: 0.9000, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:13,854]: Epoch: 099, Loss:0.3911 Train: 0.9333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:13,861]: Epoch: 100, Loss:0.3588 Train: 0.9417, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:13,870]: Epoch: 101, Loss:0.3703 Train: 0.8917, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:13,877]: Epoch: 102, Loss:0.3981 Train: 0.8833, Val:0.3500, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:44:13,885]: Epoch: 103, Loss:0.4052 Train: 0.9333, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:13,893]: Epoch: 104, Loss:0.3668 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:13,899]: Epoch: 105, Loss:0.3385 Train: 0.9583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:13,907]: Epoch: 106, Loss:0.3314 Train: 0.9500, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:13,914]: Epoch: 107, Loss:0.3533 Train: 0.9250, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:13,922]: Epoch: 108, Loss:0.3235 Train: 0.9083, Val:0.3375, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:44:13,929]: Epoch: 109, Loss:0.3774 Train: 0.9167, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:13,938]: Epoch: 110, Loss:0.2936 Train: 0.9583, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:13,945]: Epoch: 111, Loss:0.2890 Train: 0.9250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:13,954]: Epoch: 112, Loss:0.3162 Train: 0.9333, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:13,962]: Epoch: 113, Loss:0.3533 Train: 0.9500, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:13,969]: Epoch: 114, Loss:0.2682 Train: 0.9333, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:13,976]: Epoch: 115, Loss:0.2990 Train: 0.9000, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:13,985]: Epoch: 116, Loss:0.3088 Train: 0.9250, Val:0.3500, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:13,992]: Epoch: 117, Loss:0.3144 Train: 0.9500, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:13,999]: Epoch: 118, Loss:0.3133 Train: 0.9167, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:14,008]: Epoch: 119, Loss:0.3697 Train: 0.9083, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:14,015]: Epoch: 120, Loss:0.4084 Train: 0.9167, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:44:14,024]: Epoch: 121, Loss:0.3812 Train: 0.9000, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:14,032]: Epoch: 122, Loss:0.3388 Train: 0.9167, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:14,040]: Epoch: 123, Loss:0.3085 Train: 0.9000, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:14,047]: Epoch: 124, Loss:0.3513 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:14,054]: Epoch: 125, Loss:0.2791 Train: 0.9333, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:14,063]: Epoch: 126, Loss:0.3207 Train: 0.9417, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:14,071]: Epoch: 127, Loss:0.3023 Train: 0.9250, Val:0.3625, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:44:14,077]: Epoch: 128, Loss:0.3360 Train: 0.9417, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:44:14,086]: Epoch: 129, Loss:0.2565 Train: 0.9500, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:14,092]: Epoch: 130, Loss:0.3606 Train: 0.9417, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0059
[2025-04-01 02:44:14,099]: Epoch: 131, Loss:0.3169 Train: 0.9333, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:44:14,107]: Epoch: 132, Loss:0.3346 Train: 0.9250, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:14,115]: Epoch: 133, Loss:0.3327 Train: 0.9417, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:14,123]: Epoch: 134, Loss:0.2898 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:14,130]: Epoch: 135, Loss:0.3332 Train: 0.9333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:14,138]: Epoch: 136, Loss:0.3356 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:14,146]: Epoch: 137, Loss:0.3534 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:14,153]: Epoch: 138, Loss:0.2634 Train: 0.9333, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:14,161]: Epoch: 139, Loss:0.2556 Train: 0.8917, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:14,169]: Epoch: 140, Loss:0.4291 Train: 0.9500, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:14,177]: Epoch: 141, Loss:0.2758 Train: 0.9750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:14,186]: Epoch: 142, Loss:0.3194 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:14,194]: Epoch: 143, Loss:0.3254 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:14,202]: Epoch: 144, Loss:0.2872 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:14,210]: Epoch: 145, Loss:0.2771 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:14,219]: Epoch: 146, Loss:0.2560 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0089
[2025-04-01 02:44:14,228]: Epoch: 147, Loss:0.3382 Train: 0.9500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:14,236]: Epoch: 148, Loss:0.2993 Train: 0.9583, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:14,246]: Epoch: 149, Loss:0.2582 Train: 0.9417, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:14,255]: Epoch: 150, Loss:0.3257 Train: 0.9500, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:14,270]: Epoch: 151, Loss:0.2557 Train: 0.9500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0147
[2025-04-01 02:44:14,280]: Epoch: 152, Loss:0.2897 Train: 0.9500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0104
[2025-04-01 02:44:14,291]: Epoch: 153, Loss:0.2591 Train: 0.9417, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0104
[2025-04-01 02:44:14,301]: Epoch: 154, Loss:0.3116 Train: 0.9333, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0099
[2025-04-01 02:44:14,310]: Epoch: 155, Loss:0.2371 Train: 0.9583, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:14,319]: Epoch: 156, Loss:0.2792 Train: 0.9667, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:14,328]: Epoch: 157, Loss:0.2915 Train: 0.9500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:14,337]: Epoch: 158, Loss:0.2409 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:14,347]: Epoch: 159, Loss:0.2818 Train: 0.9333, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0096
[2025-04-01 02:44:14,355]: Epoch: 160, Loss:0.2769 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:14,363]: Epoch: 161, Loss:0.3275 Train: 0.9583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:14,370]: Epoch: 162, Loss:0.4239 Train: 0.9250, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:14,379]: Epoch: 163, Loss:0.3314 Train: 0.8417, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:14,388]: Epoch: 164, Loss:0.4228 Train: 0.9083, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:14,397]: Epoch: 165, Loss:0.3033 Train: 0.9167, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:14,405]: Epoch: 166, Loss:0.3347 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:14,414]: Epoch: 167, Loss:0.3034 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:14,421]: Epoch: 168, Loss:0.3432 Train: 0.9167, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:14,429]: Epoch: 169, Loss:0.3077 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:14,438]: Epoch: 170, Loss:0.2775 Train: 0.9667, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:14,444]: Epoch: 171, Loss:0.2752 Train: 0.9417, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:14,451]: Epoch: 172, Loss:0.3527 Train: 0.9000, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:14,460]: Epoch: 173, Loss:0.3077 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:14,468]: Epoch: 174, Loss:0.3493 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:14,476]: Epoch: 175, Loss:0.3110 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:14,484]: Epoch: 176, Loss:0.2842 Train: 0.9000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:14,490]: Epoch: 177, Loss:0.4010 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:14,498]: Epoch: 178, Loss:0.3461 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:14,505]: Epoch: 179, Loss:0.2762 Train: 0.8833, Val:0.3375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:14,512]: Epoch: 180, Loss:0.3448 Train: 0.8833, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:14,521]: Epoch: 181, Loss:0.3584 Train: 0.9667, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:14,527]: Epoch: 182, Loss:0.2488 Train: 0.9500, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:14,534]: Epoch: 183, Loss:0.2680 Train: 0.9000, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:14,542]: Epoch: 184, Loss:0.3729 Train: 0.9500, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:14,550]: Epoch: 185, Loss:0.2495 Train: 0.9583, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:14,557]: Epoch: 186, Loss:0.2670 Train: 0.9583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:14,564]: Epoch: 187, Loss:0.2468 Train: 0.9167, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:14,572]: Epoch: 188, Loss:0.3028 Train: 0.9000, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:14,581]: Epoch: 189, Loss:0.2994 Train: 0.9333, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:14,589]: Epoch: 190, Loss:0.2646 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:14,597]: Epoch: 191, Loss:0.2279 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:14,603]: Epoch: 192, Loss:0.4537 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:14,611]: Epoch: 193, Loss:0.2615 Train: 0.9417, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:14,619]: Epoch: 194, Loss:0.2907 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:14,627]: Epoch: 195, Loss:0.3045 Train: 0.9500, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:14,635]: Epoch: 196, Loss:0.3275 Train: 0.9417, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:14,643]: Epoch: 197, Loss:0.2667 Train: 0.9500, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:14,651]: Epoch: 198, Loss:0.3422 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:14,659]: Epoch: 199, Loss:0.2263 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:14,668]: Epoch: 200, Loss:0.2476 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:14,668]: [Run-1 score] {'train': 0.5916666666666667, 'val': 0.5125, 'test': 0.5686274509803921}
[2025-04-01 02:44:14,668]: repeat 2/3
[2025-04-01 02:44:14,668]: Manual random seed:0
[2025-04-01 02:44:14,669]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:14,671]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:14,681]: Epoch: 001, Loss:1.6456 Train: 0.4917, Val:0.4125, Test: 0.6275, Time(s/epoch):0.0073
[2025-04-01 02:44:14,688]: Epoch: 002, Loss:1.6782 Train: 0.4583, Val:0.3375, Test: 0.3529, Time(s/epoch):0.0070
[2025-04-01 02:44:14,694]: Epoch: 003, Loss:1.4870 Train: 0.6167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:14,701]: Epoch: 004, Loss:1.0274 Train: 0.6417, Val:0.4250, Test: 0.5686, Time(s/epoch):0.0070
[2025-04-01 02:44:14,708]: Epoch: 005, Loss:0.9535 Train: 0.6500, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:14,715]: Epoch: 006, Loss:1.0758 Train: 0.6417, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:14,722]: Epoch: 007, Loss:1.0269 Train: 0.6667, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:14,730]: Epoch: 008, Loss:0.9844 Train: 0.6917, Val:0.3875, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:44:14,738]: Epoch: 009, Loss:0.8313 Train: 0.7500, Val:0.4125, Test: 0.6078, Time(s/epoch):0.0080
[2025-04-01 02:44:14,745]: Epoch: 010, Loss:0.7986 Train: 0.7417, Val:0.4250, Test: 0.5882, Time(s/epoch):0.0064
[2025-04-01 02:44:14,753]: Epoch: 011, Loss:0.7322 Train: 0.7250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:14,759]: Epoch: 012, Loss:0.7131 Train: 0.7083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:14,768]: Epoch: 013, Loss:0.9599 Train: 0.7417, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:14,777]: Epoch: 014, Loss:0.8969 Train: 0.7917, Val:0.4250, Test: 0.6078, Time(s/epoch):0.0091
[2025-04-01 02:44:14,786]: Epoch: 015, Loss:0.7212 Train: 0.7250, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0091
[2025-04-01 02:44:14,794]: Epoch: 016, Loss:0.6708 Train: 0.7167, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0078
[2025-04-01 02:44:14,801]: Epoch: 017, Loss:0.7378 Train: 0.7167, Val:0.3875, Test: 0.6275, Time(s/epoch):0.0069
[2025-04-01 02:44:14,810]: Epoch: 018, Loss:0.7404 Train: 0.7417, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0085
[2025-04-01 02:44:14,818]: Epoch: 019, Loss:0.6544 Train: 0.8333, Val:0.4000, Test: 0.6471, Time(s/epoch):0.0082
[2025-04-01 02:44:14,827]: Epoch: 020, Loss:0.6233 Train: 0.8417, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0084
[2025-04-01 02:44:14,836]: Epoch: 021, Loss:0.6064 Train: 0.8083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:14,844]: Epoch: 022, Loss:0.5319 Train: 0.7917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:14,853]: Epoch: 023, Loss:0.6971 Train: 0.7917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:14,860]: Epoch: 024, Loss:0.5919 Train: 0.8167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:14,868]: Epoch: 025, Loss:0.5343 Train: 0.8333, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:14,875]: Epoch: 026, Loss:0.5509 Train: 0.8333, Val:0.4000, Test: 0.6078, Time(s/epoch):0.0069
[2025-04-01 02:44:14,881]: Epoch: 027, Loss:0.5904 Train: 0.8167, Val:0.4125, Test: 0.5882, Time(s/epoch):0.0062
[2025-04-01 02:44:14,888]: Epoch: 028, Loss:0.5795 Train: 0.8083, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:44:14,895]: Epoch: 029, Loss:0.5532 Train: 0.8083, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:44:14,903]: Epoch: 030, Loss:0.5510 Train: 0.8417, Val:0.3500, Test: 0.6078, Time(s/epoch):0.0076
[2025-04-01 02:44:14,911]: Epoch: 031, Loss:0.5385 Train: 0.8500, Val:0.3875, Test: 0.5882, Time(s/epoch):0.0079
[2025-04-01 02:44:14,920]: Epoch: 032, Loss:0.4925 Train: 0.8583, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0091
[2025-04-01 02:44:14,929]: Epoch: 033, Loss:0.4872 Train: 0.8583, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:14,937]: Epoch: 034, Loss:0.4867 Train: 0.8667, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:14,945]: Epoch: 035, Loss:0.4935 Train: 0.8667, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:14,953]: Epoch: 036, Loss:0.4903 Train: 0.8917, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:14,961]: Epoch: 037, Loss:0.4965 Train: 0.8667, Val:0.4000, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:44:14,968]: Epoch: 038, Loss:0.4960 Train: 0.8750, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:44:14,975]: Epoch: 039, Loss:0.4756 Train: 0.8833, Val:0.4000, Test: 0.5686, Time(s/epoch):0.0062
[2025-04-01 02:44:14,983]: Epoch: 040, Loss:0.5160 Train: 0.8750, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:14,989]: Epoch: 041, Loss:0.4434 Train: 0.8667, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:14,997]: Epoch: 042, Loss:0.4356 Train: 0.8417, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:44:15,005]: Epoch: 043, Loss:0.3996 Train: 0.8500, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:44:15,013]: Epoch: 044, Loss:0.4834 Train: 0.8583, Val:0.3875, Test: 0.5882, Time(s/epoch):0.0083
[2025-04-01 02:44:15,019]: Epoch: 045, Loss:0.4823 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0057
[2025-04-01 02:44:15,026]: Epoch: 046, Loss:0.4736 Train: 0.8667, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:15,035]: Epoch: 047, Loss:0.4560 Train: 0.8750, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:15,043]: Epoch: 048, Loss:0.5025 Train: 0.8750, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:15,050]: Epoch: 049, Loss:0.4688 Train: 0.9000, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:44:15,059]: Epoch: 050, Loss:0.3895 Train: 0.8583, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0080
[2025-04-01 02:44:15,066]: Epoch: 051, Loss:0.4342 Train: 0.8500, Val:0.3750, Test: 0.6275, Time(s/epoch):0.0076
[2025-04-01 02:44:15,074]: Epoch: 052, Loss:0.4458 Train: 0.8833, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:15,082]: Epoch: 053, Loss:0.4772 Train: 0.9000, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:44:15,089]: Epoch: 054, Loss:0.4123 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:15,095]: Epoch: 055, Loss:0.4396 Train: 0.8583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:15,103]: Epoch: 056, Loss:0.4528 Train: 0.8833, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:15,111]: Epoch: 057, Loss:0.4288 Train: 0.8917, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:15,118]: Epoch: 058, Loss:0.3903 Train: 0.8917, Val:0.4125, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:44:15,126]: Epoch: 059, Loss:0.3855 Train: 0.8917, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:44:15,132]: Epoch: 060, Loss:0.4225 Train: 0.8833, Val:0.3375, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:44:15,139]: Epoch: 061, Loss:0.4584 Train: 0.9250, Val:0.4000, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:44:15,147]: Epoch: 062, Loss:0.3898 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:15,153]: Epoch: 063, Loss:0.3691 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:15,160]: Epoch: 064, Loss:0.3742 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:15,167]: Epoch: 065, Loss:0.3948 Train: 0.9083, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:15,174]: Epoch: 066, Loss:0.4598 Train: 0.8583, Val:0.3375, Test: 0.5686, Time(s/epoch):0.0066
[2025-04-01 02:44:15,182]: Epoch: 067, Loss:0.4123 Train: 0.8667, Val:0.3375, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:15,188]: Epoch: 068, Loss:0.4193 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:15,195]: Epoch: 069, Loss:0.3746 Train: 0.8833, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:15,202]: Epoch: 070, Loss:0.4408 Train: 0.8667, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:15,210]: Epoch: 071, Loss:0.4333 Train: 0.9250, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:15,216]: Epoch: 072, Loss:0.4259 Train: 0.9167, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:15,224]: Epoch: 073, Loss:0.4002 Train: 0.9000, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:15,232]: Epoch: 074, Loss:0.3542 Train: 0.9167, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:15,240]: Epoch: 075, Loss:0.3893 Train: 0.9250, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:15,248]: Epoch: 076, Loss:0.3863 Train: 0.9167, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:15,255]: Epoch: 077, Loss:0.3521 Train: 0.9333, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:15,265]: Epoch: 078, Loss:0.4137 Train: 0.9000, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0100
[2025-04-01 02:44:15,274]: Epoch: 079, Loss:0.3795 Train: 0.9167, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:44:15,282]: Epoch: 080, Loss:0.3453 Train: 0.9083, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:44:15,290]: Epoch: 081, Loss:0.3154 Train: 0.9083, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:15,299]: Epoch: 082, Loss:0.3949 Train: 0.8667, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:15,307]: Epoch: 083, Loss:0.5941 Train: 0.9000, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:15,316]: Epoch: 084, Loss:0.3632 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:15,323]: Epoch: 085, Loss:0.3712 Train: 0.8750, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:15,330]: Epoch: 086, Loss:0.3907 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:15,338]: Epoch: 087, Loss:0.3832 Train: 0.8917, Val:0.3250, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:44:15,345]: Epoch: 088, Loss:0.4328 Train: 0.9000, Val:0.3375, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:15,354]: Epoch: 089, Loss:0.3570 Train: 0.9333, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:15,362]: Epoch: 090, Loss:0.3326 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:15,370]: Epoch: 091, Loss:0.3460 Train: 0.8917, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:15,377]: Epoch: 092, Loss:0.4349 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:15,384]: Epoch: 093, Loss:0.3150 Train: 0.9083, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:15,392]: Epoch: 094, Loss:0.3579 Train: 0.9167, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:15,400]: Epoch: 095, Loss:0.3553 Train: 0.9250, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:15,408]: Epoch: 096, Loss:0.3513 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:15,414]: Epoch: 097, Loss:0.4080 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:15,422]: Epoch: 098, Loss:0.3634 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:15,429]: Epoch: 099, Loss:0.2957 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:15,438]: Epoch: 100, Loss:0.3032 Train: 0.9250, Val:0.3500, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:44:15,446]: Epoch: 101, Loss:0.3312 Train: 0.9000, Val:0.3375, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:44:15,453]: Epoch: 102, Loss:0.3523 Train: 0.9333, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:15,460]: Epoch: 103, Loss:0.3435 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:15,468]: Epoch: 104, Loss:0.2931 Train: 0.8833, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:15,476]: Epoch: 105, Loss:0.4496 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:15,483]: Epoch: 106, Loss:0.3153 Train: 0.9500, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:44:15,491]: Epoch: 107, Loss:0.3040 Train: 0.9333, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:44:15,498]: Epoch: 108, Loss:0.3223 Train: 0.9167, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:15,504]: Epoch: 109, Loss:0.2987 Train: 0.9083, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0059
[2025-04-01 02:44:15,511]: Epoch: 110, Loss:0.3354 Train: 0.9167, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:15,519]: Epoch: 111, Loss:0.2894 Train: 0.9250, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:15,527]: Epoch: 112, Loss:0.3592 Train: 0.9167, Val:0.4250, Test: 0.5686, Time(s/epoch):0.0074
[2025-04-01 02:44:15,536]: Epoch: 113, Loss:0.4169 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0091
[2025-04-01 02:44:15,545]: Epoch: 114, Loss:0.3025 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:15,555]: Epoch: 115, Loss:0.2820 Train: 0.9417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0092
[2025-04-01 02:44:15,562]: Epoch: 116, Loss:0.4883 Train: 0.9000, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:15,569]: Epoch: 117, Loss:0.3156 Train: 0.8583, Val:0.3375, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:15,578]: Epoch: 118, Loss:0.4006 Train: 0.8500, Val:0.3125, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:15,584]: Epoch: 119, Loss:0.3635 Train: 0.8833, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:15,593]: Epoch: 120, Loss:0.3480 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:15,600]: Epoch: 121, Loss:0.4131 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:15,608]: Epoch: 122, Loss:0.3151 Train: 0.9250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:15,614]: Epoch: 123, Loss:0.3499 Train: 0.9250, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:44:15,622]: Epoch: 124, Loss:0.3915 Train: 0.9500, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:15,629]: Epoch: 125, Loss:0.3635 Train: 0.9333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:15,638]: Epoch: 126, Loss:0.3202 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:15,645]: Epoch: 127, Loss:0.3804 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:15,653]: Epoch: 128, Loss:0.2878 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:15,662]: Epoch: 129, Loss:0.2908 Train: 0.8917, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:15,670]: Epoch: 130, Loss:0.3257 Train: 0.8917, Val:0.4000, Test: 0.5882, Time(s/epoch):0.0087
[2025-04-01 02:44:15,679]: Epoch: 131, Loss:0.3420 Train: 0.9250, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:15,687]: Epoch: 132, Loss:0.3246 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:15,695]: Epoch: 133, Loss:0.3215 Train: 0.9250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:15,703]: Epoch: 134, Loss:0.2937 Train: 0.9333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:15,711]: Epoch: 135, Loss:0.3289 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:15,718]: Epoch: 136, Loss:0.3338 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:15,725]: Epoch: 137, Loss:0.3035 Train: 0.9583, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:15,732]: Epoch: 138, Loss:0.2765 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:15,741]: Epoch: 139, Loss:0.3277 Train: 0.9500, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:15,749]: Epoch: 140, Loss:0.2952 Train: 0.9417, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:15,757]: Epoch: 141, Loss:0.3776 Train: 0.9250, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:15,765]: Epoch: 142, Loss:0.2680 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:15,774]: Epoch: 143, Loss:0.3204 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0090
[2025-04-01 02:44:15,782]: Epoch: 144, Loss:0.3330 Train: 0.9250, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:15,791]: Epoch: 145, Loss:0.2716 Train: 0.9083, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0088
[2025-04-01 02:44:15,800]: Epoch: 146, Loss:0.2946 Train: 0.9167, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0084
[2025-04-01 02:44:15,808]: Epoch: 147, Loss:0.3085 Train: 0.9250, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:44:15,816]: Epoch: 148, Loss:0.3391 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:15,824]: Epoch: 149, Loss:0.2977 Train: 0.9500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:15,831]: Epoch: 150, Loss:0.2925 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:15,839]: Epoch: 151, Loss:0.6543 Train: 0.9250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:15,847]: Epoch: 152, Loss:0.2696 Train: 0.8667, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:15,854]: Epoch: 153, Loss:0.3742 Train: 0.9000, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:15,862]: Epoch: 154, Loss:0.4497 Train: 0.9333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:15,871]: Epoch: 155, Loss:0.3134 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:15,885]: Epoch: 156, Loss:0.4608 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0143
[2025-04-01 02:44:15,903]: Epoch: 157, Loss:0.4286 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0171
[2025-04-01 02:44:15,918]: Epoch: 158, Loss:0.3305 Train: 0.8833, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0152
[2025-04-01 02:44:15,927]: Epoch: 159, Loss:0.4462 Train: 0.8833, Val:0.3500, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:15,938]: Epoch: 160, Loss:0.3900 Train: 0.9000, Val:0.3375, Test: 0.5294, Time(s/epoch):0.0109
[2025-04-01 02:44:15,947]: Epoch: 161, Loss:0.3461 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0094
[2025-04-01 02:44:15,955]: Epoch: 162, Loss:0.3676 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:15,964]: Epoch: 163, Loss:0.2805 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:15,971]: Epoch: 164, Loss:0.2882 Train: 0.9167, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:15,978]: Epoch: 165, Loss:0.3588 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:15,985]: Epoch: 166, Loss:0.2989 Train: 0.9333, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:15,994]: Epoch: 167, Loss:0.3812 Train: 0.9333, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:16,001]: Epoch: 168, Loss:0.3701 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:16,009]: Epoch: 169, Loss:0.2715 Train: 0.9167, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:16,016]: Epoch: 170, Loss:0.3311 Train: 0.9333, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:16,023]: Epoch: 171, Loss:0.2839 Train: 0.9333, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:16,032]: Epoch: 172, Loss:0.3079 Train: 0.9583, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:16,039]: Epoch: 173, Loss:0.2901 Train: 0.9500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:16,047]: Epoch: 174, Loss:0.2484 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:16,055]: Epoch: 175, Loss:0.2858 Train: 0.9583, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:16,061]: Epoch: 176, Loss:0.2737 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:16,069]: Epoch: 177, Loss:0.2460 Train: 0.9167, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:16,078]: Epoch: 178, Loss:0.2761 Train: 0.8833, Val:0.3500, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:16,086]: Epoch: 179, Loss:0.4198 Train: 0.9250, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:16,094]: Epoch: 180, Loss:0.3445 Train: 0.9417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:16,100]: Epoch: 181, Loss:0.3477 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:16,109]: Epoch: 182, Loss:0.2785 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:16,116]: Epoch: 183, Loss:0.3125 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:16,125]: Epoch: 184, Loss:0.2922 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:16,131]: Epoch: 185, Loss:0.3048 Train: 0.8917, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:44:16,139]: Epoch: 186, Loss:0.3522 Train: 0.8917, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:16,148]: Epoch: 187, Loss:0.3180 Train: 0.9500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:16,155]: Epoch: 188, Loss:0.3240 Train: 0.9000, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:16,162]: Epoch: 189, Loss:0.4054 Train: 0.9083, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:16,170]: Epoch: 190, Loss:0.2898 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:16,177]: Epoch: 191, Loss:0.4105 Train: 0.9250, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:16,184]: Epoch: 192, Loss:0.3194 Train: 0.8500, Val:0.3500, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:44:16,193]: Epoch: 193, Loss:0.6104 Train: 0.9000, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:16,201]: Epoch: 194, Loss:0.2925 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:16,209]: Epoch: 195, Loss:0.3516 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:16,217]: Epoch: 196, Loss:0.4158 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:16,224]: Epoch: 197, Loss:0.3476 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:16,233]: Epoch: 198, Loss:0.2997 Train: 0.9083, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:16,240]: Epoch: 199, Loss:0.3484 Train: 0.9000, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:16,248]: Epoch: 200, Loss:0.3648 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:16,248]: [Run-2 score] {'train': 0.725, 'val': 0.4625, 'test': 0.43137254901960786}
[2025-04-01 02:44:16,249]: repeat 3/3
[2025-04-01 02:44:16,249]: Manual random seed:0
[2025-04-01 02:44:16,249]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:16,252]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:16,262]: Epoch: 001, Loss:1.6889 Train: 0.5750, Val:0.4625, Test: 0.6667, Time(s/epoch):0.0079
[2025-04-01 02:44:16,271]: Epoch: 002, Loss:1.2398 Train: 0.6417, Val:0.4875, Test: 0.6078, Time(s/epoch):0.0085
[2025-04-01 02:44:16,282]: Epoch: 003, Loss:1.1734 Train: 0.5583, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0104
[2025-04-01 02:44:16,291]: Epoch: 004, Loss:1.0708 Train: 0.6833, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0093
[2025-04-01 02:44:16,300]: Epoch: 005, Loss:1.1854 Train: 0.7167, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0085
[2025-04-01 02:44:16,310]: Epoch: 006, Loss:0.8781 Train: 0.6917, Val:0.4000, Test: 0.6471, Time(s/epoch):0.0094
[2025-04-01 02:44:16,318]: Epoch: 007, Loss:0.8773 Train: 0.6750, Val:0.4500, Test: 0.6275, Time(s/epoch):0.0083
[2025-04-01 02:44:16,326]: Epoch: 008, Loss:0.9231 Train: 0.7000, Val:0.3875, Test: 0.6471, Time(s/epoch):0.0081
[2025-04-01 02:44:16,334]: Epoch: 009, Loss:0.8971 Train: 0.7333, Val:0.3875, Test: 0.6275, Time(s/epoch):0.0074
[2025-04-01 02:44:16,343]: Epoch: 010, Loss:0.7407 Train: 0.7333, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0093
[2025-04-01 02:44:16,354]: Epoch: 011, Loss:0.8868 Train: 0.7333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0103
[2025-04-01 02:44:16,362]: Epoch: 012, Loss:0.8811 Train: 0.7500, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:16,371]: Epoch: 013, Loss:0.7031 Train: 0.7583, Val:0.3375, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:16,379]: Epoch: 014, Loss:0.7077 Train: 0.7583, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:16,388]: Epoch: 015, Loss:0.6357 Train: 0.7917, Val:0.3500, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:44:16,396]: Epoch: 016, Loss:0.6517 Train: 0.7750, Val:0.3500, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:44:16,405]: Epoch: 017, Loss:0.6413 Train: 0.8000, Val:0.3750, Test: 0.6078, Time(s/epoch):0.0089
[2025-04-01 02:44:16,412]: Epoch: 018, Loss:0.6421 Train: 0.8250, Val:0.4000, Test: 0.6275, Time(s/epoch):0.0068
[2025-04-01 02:44:16,421]: Epoch: 019, Loss:0.5824 Train: 0.8000, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:44:16,429]: Epoch: 020, Loss:0.6044 Train: 0.8250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:16,437]: Epoch: 021, Loss:0.5739 Train: 0.8417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:16,445]: Epoch: 022, Loss:0.5521 Train: 0.8417, Val:0.4000, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:44:16,452]: Epoch: 023, Loss:0.5151 Train: 0.8500, Val:0.3875, Test: 0.5882, Time(s/epoch):0.0071
[2025-04-01 02:44:16,459]: Epoch: 024, Loss:0.5999 Train: 0.8583, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0066
[2025-04-01 02:44:16,467]: Epoch: 025, Loss:0.5418 Train: 0.8583, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0077
[2025-04-01 02:44:16,475]: Epoch: 026, Loss:0.4618 Train: 0.8833, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:16,483]: Epoch: 027, Loss:0.6006 Train: 0.8750, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:44:16,490]: Epoch: 028, Loss:0.4936 Train: 0.8667, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0068
[2025-04-01 02:44:16,498]: Epoch: 029, Loss:0.4806 Train: 0.8417, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0084
[2025-04-01 02:44:16,506]: Epoch: 030, Loss:0.4593 Train: 0.8417, Val:0.3625, Test: 0.6078, Time(s/epoch):0.0077
[2025-04-01 02:44:16,514]: Epoch: 031, Loss:0.5396 Train: 0.8417, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0074
[2025-04-01 02:44:16,522]: Epoch: 032, Loss:0.5071 Train: 0.8667, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:16,530]: Epoch: 033, Loss:0.4919 Train: 0.8583, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:16,538]: Epoch: 034, Loss:0.4806 Train: 0.8750, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:16,547]: Epoch: 035, Loss:0.5091 Train: 0.8667, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:16,555]: Epoch: 036, Loss:0.5594 Train: 0.8583, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:16,563]: Epoch: 037, Loss:0.4559 Train: 0.8750, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:44:16,570]: Epoch: 038, Loss:0.5117 Train: 0.8917, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:16,578]: Epoch: 039, Loss:0.4229 Train: 0.9000, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:44:16,586]: Epoch: 040, Loss:0.5071 Train: 0.8833, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:16,594]: Epoch: 041, Loss:0.4536 Train: 0.9167, Val:0.3750, Test: 0.6078, Time(s/epoch):0.0081
[2025-04-01 02:44:16,602]: Epoch: 042, Loss:0.4298 Train: 0.9000, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:16,611]: Epoch: 043, Loss:0.3961 Train: 0.9250, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:16,619]: Epoch: 044, Loss:0.4180 Train: 0.9083, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:16,625]: Epoch: 045, Loss:0.3780 Train: 0.9167, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:16,633]: Epoch: 046, Loss:0.4296 Train: 0.9083, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:16,640]: Epoch: 047, Loss:0.3967 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:16,648]: Epoch: 048, Loss:0.4286 Train: 0.9250, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:16,655]: Epoch: 049, Loss:0.3866 Train: 0.9250, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:16,663]: Epoch: 050, Loss:0.4240 Train: 0.9000, Val:0.3625, Test: 0.5882, Time(s/epoch):0.0076
[2025-04-01 02:44:16,670]: Epoch: 051, Loss:0.3787 Train: 0.9083, Val:0.3625, Test: 0.5686, Time(s/epoch):0.0069
[2025-04-01 02:44:16,677]: Epoch: 052, Loss:0.3434 Train: 0.9167, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:16,684]: Epoch: 053, Loss:0.3586 Train: 0.9083, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:16,691]: Epoch: 054, Loss:0.3951 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:16,699]: Epoch: 055, Loss:0.4079 Train: 0.9167, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:16,709]: Epoch: 056, Loss:0.3357 Train: 0.9250, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0094
[2025-04-01 02:44:16,717]: Epoch: 057, Loss:0.3899 Train: 0.9000, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:16,725]: Epoch: 058, Loss:0.3737 Train: 0.9333, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:16,734]: Epoch: 059, Loss:0.3724 Train: 0.9250, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:16,742]: Epoch: 060, Loss:0.3698 Train: 0.9333, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:16,750]: Epoch: 061, Loss:0.3424 Train: 0.9000, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:16,756]: Epoch: 062, Loss:0.3267 Train: 0.9167, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:16,765]: Epoch: 063, Loss:0.3607 Train: 0.9250, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:16,771]: Epoch: 064, Loss:0.3611 Train: 0.9250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:16,779]: Epoch: 065, Loss:0.3502 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:16,787]: Epoch: 066, Loss:0.3759 Train: 0.9167, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:16,794]: Epoch: 067, Loss:0.3896 Train: 0.9250, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:16,803]: Epoch: 068, Loss:0.3524 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:16,810]: Epoch: 069, Loss:0.3697 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:16,817]: Epoch: 070, Loss:0.3517 Train: 0.8667, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0073
[2025-04-01 02:44:16,825]: Epoch: 071, Loss:0.3928 Train: 0.8667, Val:0.3375, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:44:16,834]: Epoch: 072, Loss:0.3682 Train: 0.8917, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:16,841]: Epoch: 073, Loss:0.3143 Train: 0.9250, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:16,849]: Epoch: 074, Loss:0.3926 Train: 0.9083, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:16,858]: Epoch: 075, Loss:0.3890 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:16,866]: Epoch: 076, Loss:0.3127 Train: 0.9250, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:16,875]: Epoch: 077, Loss:0.3113 Train: 0.9333, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:44:16,882]: Epoch: 078, Loss:0.3680 Train: 0.9417, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:16,890]: Epoch: 079, Loss:0.3832 Train: 0.9417, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:16,899]: Epoch: 080, Loss:0.3690 Train: 0.9333, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:16,907]: Epoch: 081, Loss:0.2874 Train: 0.9500, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:16,916]: Epoch: 082, Loss:0.3865 Train: 0.9000, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:16,923]: Epoch: 083, Loss:0.3242 Train: 0.8917, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:16,932]: Epoch: 084, Loss:0.3750 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0091
[2025-04-01 02:44:16,941]: Epoch: 085, Loss:0.2868 Train: 0.9250, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:16,950]: Epoch: 086, Loss:0.3138 Train: 0.9167, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:16,958]: Epoch: 087, Loss:0.3451 Train: 0.9417, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:16,967]: Epoch: 088, Loss:0.4056 Train: 0.9000, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:44:16,974]: Epoch: 089, Loss:0.3129 Train: 0.8667, Val:0.3875, Test: 0.6078, Time(s/epoch):0.0073
[2025-04-01 02:44:16,983]: Epoch: 090, Loss:0.3661 Train: 0.9000, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:16,990]: Epoch: 091, Loss:0.3783 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:16,998]: Epoch: 092, Loss:0.3197 Train: 0.8667, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:17,005]: Epoch: 093, Loss:0.3851 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:17,011]: Epoch: 094, Loss:0.3608 Train: 0.9250, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:17,019]: Epoch: 095, Loss:0.2978 Train: 0.8250, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:17,026]: Epoch: 096, Loss:0.3312 Train: 0.8250, Val:0.3750, Test: 0.5882, Time(s/epoch):0.0070
[2025-04-01 02:44:17,033]: Epoch: 097, Loss:0.3137 Train: 0.8667, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:17,041]: Epoch: 098, Loss:0.3255 Train: 0.8833, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:17,049]: Epoch: 099, Loss:0.2820 Train: 0.9000, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:17,057]: Epoch: 100, Loss:0.4395 Train: 0.9250, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:17,065]: Epoch: 101, Loss:0.3945 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:17,073]: Epoch: 102, Loss:0.3585 Train: 0.9250, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:17,080]: Epoch: 103, Loss:0.4583 Train: 0.9333, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:17,088]: Epoch: 104, Loss:0.2895 Train: 0.8917, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:17,097]: Epoch: 105, Loss:0.4125 Train: 0.8917, Val:0.3750, Test: 0.5490, Time(s/epoch):0.0089
[2025-04-01 02:44:17,106]: Epoch: 106, Loss:0.3726 Train: 0.9417, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:17,117]: Epoch: 107, Loss:0.3270 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0104
[2025-04-01 02:44:17,124]: Epoch: 108, Loss:0.4052 Train: 0.9500, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:17,132]: Epoch: 109, Loss:0.3228 Train: 0.9500, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:17,140]: Epoch: 110, Loss:0.3029 Train: 0.9083, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:17,147]: Epoch: 111, Loss:0.2821 Train: 0.9000, Val:0.3750, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:17,156]: Epoch: 112, Loss:0.3701 Train: 0.9083, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:17,163]: Epoch: 113, Loss:0.3106 Train: 0.9333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:17,170]: Epoch: 114, Loss:0.4204 Train: 0.9083, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:17,178]: Epoch: 115, Loss:0.3210 Train: 0.9167, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:17,186]: Epoch: 116, Loss:0.3900 Train: 0.9250, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:17,194]: Epoch: 117, Loss:0.3842 Train: 0.9167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:17,202]: Epoch: 118, Loss:0.3173 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:17,210]: Epoch: 119, Loss:0.3514 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:17,218]: Epoch: 120, Loss:0.3271 Train: 0.9500, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:17,226]: Epoch: 121, Loss:0.3121 Train: 0.9333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:17,234]: Epoch: 122, Loss:0.3928 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:17,242]: Epoch: 123, Loss:0.3580 Train: 0.9333, Val:0.4125, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:17,250]: Epoch: 124, Loss:0.2587 Train: 0.9083, Val:0.3875, Test: 0.5686, Time(s/epoch):0.0086
[2025-04-01 02:44:17,258]: Epoch: 125, Loss:0.3301 Train: 0.9167, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:17,267]: Epoch: 126, Loss:0.4631 Train: 0.9583, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:17,276]: Epoch: 127, Loss:0.3036 Train: 0.8750, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0091
[2025-04-01 02:44:17,285]: Epoch: 128, Loss:1.1036 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:17,295]: Epoch: 129, Loss:0.3325 Train: 0.9250, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0096
[2025-04-01 02:44:17,304]: Epoch: 130, Loss:0.3983 Train: 0.9250, Val:0.3500, Test: 0.5686, Time(s/epoch):0.0090
[2025-04-01 02:44:17,312]: Epoch: 131, Loss:0.4391 Train: 0.9250, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:44:17,320]: Epoch: 132, Loss:0.3521 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:17,329]: Epoch: 133, Loss:0.3738 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:17,335]: Epoch: 134, Loss:0.3380 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:17,342]: Epoch: 135, Loss:0.4023 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:17,351]: Epoch: 136, Loss:0.3125 Train: 0.9333, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:17,358]: Epoch: 137, Loss:0.3966 Train: 0.9333, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:17,365]: Epoch: 138, Loss:0.3425 Train: 0.9250, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:17,374]: Epoch: 139, Loss:0.3529 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:17,380]: Epoch: 140, Loss:0.3323 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0056
[2025-04-01 02:44:17,388]: Epoch: 141, Loss:0.2979 Train: 0.9250, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:17,396]: Epoch: 142, Loss:0.3096 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:17,404]: Epoch: 143, Loss:0.4061 Train: 0.9417, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:17,414]: Epoch: 144, Loss:0.3169 Train: 0.9333, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:17,421]: Epoch: 145, Loss:0.3736 Train: 0.9500, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:17,430]: Epoch: 146, Loss:0.2847 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:17,437]: Epoch: 147, Loss:0.2944 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:17,444]: Epoch: 148, Loss:0.3228 Train: 0.9500, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:17,453]: Epoch: 149, Loss:0.2922 Train: 0.9583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:17,461]: Epoch: 150, Loss:0.2377 Train: 0.9500, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:17,469]: Epoch: 151, Loss:0.3176 Train: 0.9083, Val:0.3625, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:17,476]: Epoch: 152, Loss:0.3895 Train: 0.9583, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:17,484]: Epoch: 153, Loss:0.3053 Train: 0.9417, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:17,490]: Epoch: 154, Loss:0.3020 Train: 0.9667, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:17,498]: Epoch: 155, Loss:0.4576 Train: 0.9333, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:17,506]: Epoch: 156, Loss:0.3469 Train: 0.8583, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:17,514]: Epoch: 157, Loss:0.4952 Train: 0.8917, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:17,523]: Epoch: 158, Loss:0.4206 Train: 0.9167, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:17,532]: Epoch: 159, Loss:0.3006 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:17,542]: Epoch: 160, Loss:0.3575 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0105
[2025-04-01 02:44:17,551]: Epoch: 161, Loss:0.3730 Train: 0.9333, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:17,560]: Epoch: 162, Loss:0.3134 Train: 0.9333, Val:0.3625, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:17,567]: Epoch: 163, Loss:0.3122 Train: 0.9083, Val:0.3000, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:17,575]: Epoch: 164, Loss:0.3545 Train: 0.8750, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:17,583]: Epoch: 165, Loss:0.3418 Train: 0.8917, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:17,590]: Epoch: 166, Loss:0.2931 Train: 0.9083, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:17,597]: Epoch: 167, Loss:0.3591 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:17,605]: Epoch: 168, Loss:0.2941 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:17,613]: Epoch: 169, Loss:0.3087 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:17,620]: Epoch: 170, Loss:0.3282 Train: 0.9583, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:17,629]: Epoch: 171, Loss:0.3320 Train: 0.9667, Val:0.3625, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:17,636]: Epoch: 172, Loss:0.2786 Train: 0.9500, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:17,644]: Epoch: 173, Loss:0.3742 Train: 0.9750, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:17,652]: Epoch: 174, Loss:0.2946 Train: 0.9583, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:17,659]: Epoch: 175, Loss:0.4153 Train: 0.9417, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:17,667]: Epoch: 176, Loss:0.3156 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:17,674]: Epoch: 177, Loss:0.2777 Train: 0.9583, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:17,681]: Epoch: 178, Loss:0.3262 Train: 0.9500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:17,690]: Epoch: 179, Loss:0.2881 Train: 0.9333, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:17,697]: Epoch: 180, Loss:0.3099 Train: 0.9250, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:17,705]: Epoch: 181, Loss:0.3141 Train: 0.9500, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:17,720]: Epoch: 182, Loss:0.3904 Train: 0.9583, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0155
[2025-04-01 02:44:17,731]: Epoch: 183, Loss:0.2613 Train: 0.9000, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0105
[2025-04-01 02:44:17,741]: Epoch: 184, Loss:0.3299 Train: 0.9167, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0097
[2025-04-01 02:44:17,750]: Epoch: 185, Loss:0.3615 Train: 0.9500, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:17,758]: Epoch: 186, Loss:0.2583 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:17,766]: Epoch: 187, Loss:0.2450 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:17,772]: Epoch: 188, Loss:0.3008 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:17,780]: Epoch: 189, Loss:0.2737 Train: 0.9583, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:17,788]: Epoch: 190, Loss:0.2445 Train: 0.9500, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:17,796]: Epoch: 191, Loss:0.2523 Train: 0.9667, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:17,806]: Epoch: 192, Loss:0.2610 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:17,814]: Epoch: 193, Loss:0.2651 Train: 0.9500, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:17,826]: Epoch: 194, Loss:0.2737 Train: 0.9500, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0113
[2025-04-01 02:44:17,836]: Epoch: 195, Loss:0.2839 Train: 0.9333, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0104
[2025-04-01 02:44:17,846]: Epoch: 196, Loss:0.3404 Train: 0.9500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0098
[2025-04-01 02:44:17,855]: Epoch: 197, Loss:0.2730 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:17,865]: Epoch: 198, Loss:0.2914 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0098
[2025-04-01 02:44:17,875]: Epoch: 199, Loss:0.2806 Train: 0.9417, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0097
[2025-04-01 02:44:17,885]: Epoch: 200, Loss:0.2885 Train: 0.9333, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0102
[2025-04-01 02:44:17,885]: [Run-3 score] {'train': 0.6416666666666667, 'val': 0.4875, 'test': 0.6078431372549019}
[2025-04-01 02:44:17,885]: repeat 1/3
[2025-04-01 02:44:17,885]: Manual random seed:0
[2025-04-01 02:44:17,886]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:17,889]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:17,901]: Epoch: 001, Loss:1.7087 Train: 0.4833, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0097
[2025-04-01 02:44:17,910]: Epoch: 002, Loss:2.1881 Train: 0.5833, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:17,920]: Epoch: 003, Loss:1.2369 Train: 0.5917, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0097
[2025-04-01 02:44:17,929]: Epoch: 004, Loss:1.1853 Train: 0.6750, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:17,940]: Epoch: 005, Loss:1.0345 Train: 0.6750, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0111
[2025-04-01 02:44:17,951]: Epoch: 006, Loss:1.0355 Train: 0.6583, Val:0.5500, Test: 0.5294, Time(s/epoch):0.0105
[2025-04-01 02:44:17,961]: Epoch: 007, Loss:0.9455 Train: 0.6500, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0103
[2025-04-01 02:44:17,971]: Epoch: 008, Loss:0.9221 Train: 0.6583, Val:0.5375, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:17,979]: Epoch: 009, Loss:0.9020 Train: 0.6833, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:17,988]: Epoch: 010, Loss:0.7967 Train: 0.7083, Val:0.5375, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:17,997]: Epoch: 011, Loss:0.9313 Train: 0.7500, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0093
[2025-04-01 02:44:18,007]: Epoch: 012, Loss:0.8553 Train: 0.7417, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0094
[2025-04-01 02:44:18,017]: Epoch: 013, Loss:0.7706 Train: 0.7500, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0095
[2025-04-01 02:44:18,025]: Epoch: 014, Loss:0.6893 Train: 0.7333, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:18,034]: Epoch: 015, Loss:0.7491 Train: 0.7167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:18,044]: Epoch: 016, Loss:0.6608 Train: 0.7583, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:18,054]: Epoch: 017, Loss:0.7264 Train: 0.7500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0100
[2025-04-01 02:44:18,063]: Epoch: 018, Loss:0.7710 Train: 0.7750, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0095
[2025-04-01 02:44:18,073]: Epoch: 019, Loss:0.6894 Train: 0.7833, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0096
[2025-04-01 02:44:18,082]: Epoch: 020, Loss:0.6569 Train: 0.7833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0088
[2025-04-01 02:44:18,092]: Epoch: 021, Loss:0.5917 Train: 0.7583, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0097
[2025-04-01 02:44:18,101]: Epoch: 022, Loss:0.5862 Train: 0.7750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:18,110]: Epoch: 023, Loss:0.6079 Train: 0.7833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:18,119]: Epoch: 024, Loss:0.6151 Train: 0.7917, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:18,127]: Epoch: 025, Loss:0.5841 Train: 0.8167, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:18,137]: Epoch: 026, Loss:0.5372 Train: 0.8417, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0096
[2025-04-01 02:44:18,147]: Epoch: 027, Loss:0.5164 Train: 0.8500, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0096
[2025-04-01 02:44:18,156]: Epoch: 028, Loss:0.5346 Train: 0.8583, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:44:18,164]: Epoch: 029, Loss:0.5317 Train: 0.8583, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:18,171]: Epoch: 030, Loss:0.5643 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:18,181]: Epoch: 031, Loss:0.5440 Train: 0.8583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0093
[2025-04-01 02:44:18,190]: Epoch: 032, Loss:0.5066 Train: 0.8083, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:18,199]: Epoch: 033, Loss:0.6024 Train: 0.8500, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:18,207]: Epoch: 034, Loss:0.4947 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:18,215]: Epoch: 035, Loss:0.4879 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:18,225]: Epoch: 036, Loss:0.4491 Train: 0.8667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0097
[2025-04-01 02:44:18,234]: Epoch: 037, Loss:0.4353 Train: 0.8583, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:18,241]: Epoch: 038, Loss:0.5705 Train: 0.8833, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:18,251]: Epoch: 039, Loss:0.4362 Train: 0.8917, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0095
[2025-04-01 02:44:18,260]: Epoch: 040, Loss:0.5126 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:18,270]: Epoch: 041, Loss:0.3977 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0106
[2025-04-01 02:44:18,282]: Epoch: 042, Loss:0.5516 Train: 0.8417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0114
[2025-04-01 02:44:18,294]: Epoch: 043, Loss:0.4589 Train: 0.8417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0109
[2025-04-01 02:44:18,304]: Epoch: 044, Loss:0.5086 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0107
[2025-04-01 02:44:18,315]: Epoch: 045, Loss:0.4243 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0103
[2025-04-01 02:44:18,324]: Epoch: 046, Loss:0.3940 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:18,332]: Epoch: 047, Loss:0.5459 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:18,342]: Epoch: 048, Loss:0.4352 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0090
[2025-04-01 02:44:18,350]: Epoch: 049, Loss:0.4328 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:18,360]: Epoch: 050, Loss:0.5472 Train: 0.8667, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0094
[2025-04-01 02:44:18,369]: Epoch: 051, Loss:0.4750 Train: 0.8167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:18,379]: Epoch: 052, Loss:0.4684 Train: 0.8417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0098
[2025-04-01 02:44:18,390]: Epoch: 053, Loss:0.4241 Train: 0.8917, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0103
[2025-04-01 02:44:18,401]: Epoch: 054, Loss:0.3731 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0109
[2025-04-01 02:44:18,412]: Epoch: 055, Loss:0.4593 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0106
[2025-04-01 02:44:18,422]: Epoch: 056, Loss:0.4605 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0103
[2025-04-01 02:44:18,432]: Epoch: 057, Loss:0.3730 Train: 0.9083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0099
[2025-04-01 02:44:18,444]: Epoch: 058, Loss:0.4033 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0113
[2025-04-01 02:44:18,454]: Epoch: 059, Loss:0.4215 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0100
[2025-04-01 02:44:18,466]: Epoch: 060, Loss:0.4672 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0120
[2025-04-01 02:44:18,478]: Epoch: 061, Loss:0.4312 Train: 0.9083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0114
[2025-04-01 02:44:18,491]: Epoch: 062, Loss:0.3734 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0132
[2025-04-01 02:44:18,503]: Epoch: 063, Loss:0.3818 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0113
[2025-04-01 02:44:18,519]: Epoch: 064, Loss:0.3761 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0159
[2025-04-01 02:44:18,530]: Epoch: 065, Loss:0.4357 Train: 0.8750, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0109
[2025-04-01 02:44:18,541]: Epoch: 066, Loss:0.4497 Train: 0.8917, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0101
[2025-04-01 02:44:18,551]: Epoch: 067, Loss:0.3768 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0104
[2025-04-01 02:44:18,563]: Epoch: 068, Loss:0.4290 Train: 0.8583, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0117
[2025-04-01 02:44:18,573]: Epoch: 069, Loss:0.4392 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0098
[2025-04-01 02:44:18,583]: Epoch: 070, Loss:0.3729 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0099
[2025-04-01 02:44:18,591]: Epoch: 071, Loss:0.4411 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:18,601]: Epoch: 072, Loss:0.3444 Train: 0.8667, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0095
[2025-04-01 02:44:18,610]: Epoch: 073, Loss:0.4312 Train: 0.8833, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0094
[2025-04-01 02:44:18,619]: Epoch: 074, Loss:0.4252 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:18,628]: Epoch: 075, Loss:0.3409 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:18,638]: Epoch: 076, Loss:0.5354 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0098
[2025-04-01 02:44:18,647]: Epoch: 077, Loss:0.3705 Train: 0.9167, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:18,657]: Epoch: 078, Loss:0.4261 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0102
[2025-04-01 02:44:18,666]: Epoch: 079, Loss:0.3904 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:18,678]: Epoch: 080, Loss:0.4036 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0123
[2025-04-01 02:44:18,688]: Epoch: 081, Loss:0.3592 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:18,696]: Epoch: 082, Loss:0.3935 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:18,705]: Epoch: 083, Loss:0.3647 Train: 0.9000, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0089
[2025-04-01 02:44:18,714]: Epoch: 084, Loss:0.4232 Train: 0.8917, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0091
[2025-04-01 02:44:18,721]: Epoch: 085, Loss:0.3649 Train: 0.8750, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:18,728]: Epoch: 086, Loss:0.3629 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:18,736]: Epoch: 087, Loss:0.4503 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:18,743]: Epoch: 088, Loss:0.3645 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:18,750]: Epoch: 089, Loss:0.3475 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:18,758]: Epoch: 090, Loss:0.3871 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:18,764]: Epoch: 091, Loss:0.4093 Train: 0.9500, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:18,772]: Epoch: 092, Loss:0.3283 Train: 0.9333, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:18,780]: Epoch: 093, Loss:0.3103 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:18,789]: Epoch: 094, Loss:0.4399 Train: 0.9333, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:18,795]: Epoch: 095, Loss:0.4050 Train: 0.8833, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:18,804]: Epoch: 096, Loss:0.3662 Train: 0.8500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:18,812]: Epoch: 097, Loss:0.5148 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:18,820]: Epoch: 098, Loss:0.3900 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:18,828]: Epoch: 099, Loss:0.3386 Train: 0.8750, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:18,836]: Epoch: 100, Loss:0.4727 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:18,842]: Epoch: 101, Loss:0.4679 Train: 0.9417, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:18,850]: Epoch: 102, Loss:0.3517 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:18,857]: Epoch: 103, Loss:0.3822 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:18,865]: Epoch: 104, Loss:0.3836 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:18,873]: Epoch: 105, Loss:0.4177 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:18,880]: Epoch: 106, Loss:0.3856 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:18,887]: Epoch: 107, Loss:0.3519 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:18,896]: Epoch: 108, Loss:0.3518 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:18,903]: Epoch: 109, Loss:0.3929 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:18,911]: Epoch: 110, Loss:0.3532 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:18,920]: Epoch: 111, Loss:0.3068 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:18,928]: Epoch: 112, Loss:0.3713 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:18,936]: Epoch: 113, Loss:0.3322 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:18,943]: Epoch: 114, Loss:0.3154 Train: 0.9500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:18,951]: Epoch: 115, Loss:0.3151 Train: 0.9417, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:18,960]: Epoch: 116, Loss:0.3257 Train: 0.9083, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:18,968]: Epoch: 117, Loss:0.3699 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:18,975]: Epoch: 118, Loss:0.3434 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:18,984]: Epoch: 119, Loss:0.3164 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:18,990]: Epoch: 120, Loss:0.3561 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:18,998]: Epoch: 121, Loss:0.4851 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:19,005]: Epoch: 122, Loss:0.4487 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:19,013]: Epoch: 123, Loss:0.3299 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:19,022]: Epoch: 124, Loss:0.2982 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:19,030]: Epoch: 125, Loss:0.3372 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:19,038]: Epoch: 126, Loss:0.3964 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:19,045]: Epoch: 127, Loss:0.3092 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:19,051]: Epoch: 128, Loss:0.3325 Train: 0.9417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:19,059]: Epoch: 129, Loss:0.3341 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:19,065]: Epoch: 130, Loss:0.3191 Train: 0.9250, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:19,073]: Epoch: 131, Loss:0.4497 Train: 0.9333, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:19,080]: Epoch: 132, Loss:0.2726 Train: 0.9333, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:19,088]: Epoch: 133, Loss:0.3398 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:19,096]: Epoch: 134, Loss:0.3493 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:19,104]: Epoch: 135, Loss:0.3968 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:19,112]: Epoch: 136, Loss:0.3102 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:19,120]: Epoch: 137, Loss:0.4462 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:19,128]: Epoch: 138, Loss:0.2811 Train: 0.9333, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:19,135]: Epoch: 139, Loss:0.3124 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:19,143]: Epoch: 140, Loss:0.3202 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:19,151]: Epoch: 141, Loss:0.2951 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:19,158]: Epoch: 142, Loss:0.3016 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:19,166]: Epoch: 143, Loss:0.2977 Train: 0.9417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:19,174]: Epoch: 144, Loss:0.3247 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:19,181]: Epoch: 145, Loss:0.3309 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:19,189]: Epoch: 146, Loss:0.3022 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:19,196]: Epoch: 147, Loss:0.4806 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:19,204]: Epoch: 148, Loss:0.2903 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:19,211]: Epoch: 149, Loss:0.3087 Train: 0.8917, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:19,219]: Epoch: 150, Loss:0.3598 Train: 0.9333, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:19,225]: Epoch: 151, Loss:0.3098 Train: 0.9417, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:19,232]: Epoch: 152, Loss:0.3251 Train: 0.9250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:19,239]: Epoch: 153, Loss:0.3273 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:19,247]: Epoch: 154, Loss:0.3024 Train: 0.9500, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:19,255]: Epoch: 155, Loss:0.3112 Train: 0.9500, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:19,263]: Epoch: 156, Loss:0.3134 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:19,271]: Epoch: 157, Loss:0.4019 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:19,280]: Epoch: 158, Loss:0.3452 Train: 0.9417, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:19,289]: Epoch: 159, Loss:0.2998 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:19,298]: Epoch: 160, Loss:0.3146 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0096
[2025-04-01 02:44:19,306]: Epoch: 161, Loss:0.3345 Train: 0.9667, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:19,315]: Epoch: 162, Loss:0.2716 Train: 0.9583, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:19,321]: Epoch: 163, Loss:0.4290 Train: 0.9500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:19,329]: Epoch: 164, Loss:0.2992 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:19,335]: Epoch: 165, Loss:0.3005 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:19,341]: Epoch: 166, Loss:0.3714 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:19,350]: Epoch: 167, Loss:0.3291 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:19,357]: Epoch: 168, Loss:0.3219 Train: 0.9333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:19,364]: Epoch: 169, Loss:0.3111 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:19,370]: Epoch: 170, Loss:0.3596 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:19,378]: Epoch: 171, Loss:0.3078 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:19,386]: Epoch: 172, Loss:0.2811 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:19,393]: Epoch: 173, Loss:0.3082 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:19,401]: Epoch: 174, Loss:0.3403 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:19,408]: Epoch: 175, Loss:0.3491 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:19,415]: Epoch: 176, Loss:0.3319 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:19,423]: Epoch: 177, Loss:0.4480 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:19,431]: Epoch: 178, Loss:0.3865 Train: 0.9500, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:19,439]: Epoch: 179, Loss:0.2870 Train: 0.9500, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:19,447]: Epoch: 180, Loss:0.2879 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:19,454]: Epoch: 181, Loss:0.3997 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:19,461]: Epoch: 182, Loss:0.3523 Train: 0.9583, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:19,469]: Epoch: 183, Loss:0.2756 Train: 0.9500, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:19,478]: Epoch: 184, Loss:0.3607 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:19,485]: Epoch: 185, Loss:0.2852 Train: 0.9333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:19,492]: Epoch: 186, Loss:0.3101 Train: 0.9667, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:19,499]: Epoch: 187, Loss:0.2604 Train: 0.9333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:19,506]: Epoch: 188, Loss:0.3356 Train: 0.8750, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:19,514]: Epoch: 189, Loss:0.3549 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:19,522]: Epoch: 190, Loss:0.4311 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:19,530]: Epoch: 191, Loss:0.3099 Train: 0.9417, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:19,538]: Epoch: 192, Loss:0.3118 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:19,544]: Epoch: 193, Loss:0.2578 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:19,552]: Epoch: 194, Loss:0.2810 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:19,559]: Epoch: 195, Loss:0.2965 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:19,567]: Epoch: 196, Loss:0.2572 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:19,573]: Epoch: 197, Loss:0.2823 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:19,582]: Epoch: 198, Loss:0.3055 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:19,590]: Epoch: 199, Loss:0.2837 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:19,597]: Epoch: 200, Loss:0.4058 Train: 0.9500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:19,597]: [Run-1 score] {'train': 0.6583333333333333, 'val': 0.55, 'test': 0.5294117647058824}
[2025-04-01 02:44:19,597]: repeat 2/3
[2025-04-01 02:44:19,597]: Manual random seed:0
[2025-04-01 02:44:19,598]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:19,602]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:19,613]: Epoch: 001, Loss:1.6766 Train: 0.5250, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:19,621]: Epoch: 002, Loss:1.7591 Train: 0.4750, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:19,628]: Epoch: 003, Loss:1.3602 Train: 0.6167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:19,636]: Epoch: 004, Loss:1.1106 Train: 0.6500, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:19,645]: Epoch: 005, Loss:0.9583 Train: 0.6833, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:19,653]: Epoch: 006, Loss:1.1592 Train: 0.6750, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:19,661]: Epoch: 007, Loss:0.9550 Train: 0.6750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:19,668]: Epoch: 008, Loss:0.9350 Train: 0.6917, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:19,677]: Epoch: 009, Loss:0.8550 Train: 0.7333, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:19,685]: Epoch: 010, Loss:0.9106 Train: 0.7500, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:19,693]: Epoch: 011, Loss:0.7721 Train: 0.7833, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:19,701]: Epoch: 012, Loss:0.7246 Train: 0.7417, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:19,708]: Epoch: 013, Loss:0.9499 Train: 0.7417, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:19,716]: Epoch: 014, Loss:0.8735 Train: 0.7667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:19,725]: Epoch: 015, Loss:0.7496 Train: 0.7750, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0091
[2025-04-01 02:44:19,731]: Epoch: 016, Loss:0.6655 Train: 0.7000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:19,740]: Epoch: 017, Loss:0.6927 Train: 0.7000, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:19,748]: Epoch: 018, Loss:0.7953 Train: 0.7083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:19,754]: Epoch: 019, Loss:0.7020 Train: 0.7750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:19,762]: Epoch: 020, Loss:0.6179 Train: 0.8083, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:19,770]: Epoch: 021, Loss:0.6464 Train: 0.8250, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:19,779]: Epoch: 022, Loss:0.5857 Train: 0.7917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:19,788]: Epoch: 023, Loss:0.5804 Train: 0.7833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:19,796]: Epoch: 024, Loss:0.5475 Train: 0.8000, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:19,804]: Epoch: 025, Loss:0.5716 Train: 0.8250, Val:0.4375, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:44:19,813]: Epoch: 026, Loss:0.5348 Train: 0.8333, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:44:19,819]: Epoch: 027, Loss:0.7013 Train: 0.8167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:19,827]: Epoch: 028, Loss:0.5295 Train: 0.8000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:19,835]: Epoch: 029, Loss:0.5210 Train: 0.8000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:19,843]: Epoch: 030, Loss:0.5284 Train: 0.7917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:19,851]: Epoch: 031, Loss:0.5423 Train: 0.8000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:19,859]: Epoch: 032, Loss:0.5096 Train: 0.8333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:19,866]: Epoch: 033, Loss:0.5596 Train: 0.8583, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:19,874]: Epoch: 034, Loss:0.5308 Train: 0.8583, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:19,881]: Epoch: 035, Loss:0.5257 Train: 0.8417, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:19,889]: Epoch: 036, Loss:0.5573 Train: 0.8417, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:19,896]: Epoch: 037, Loss:0.5214 Train: 0.8500, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:19,903]: Epoch: 038, Loss:0.5175 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:19,911]: Epoch: 039, Loss:0.5050 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:19,920]: Epoch: 040, Loss:0.4522 Train: 0.8750, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:19,929]: Epoch: 041, Loss:0.4821 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:19,938]: Epoch: 042, Loss:0.4920 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:19,946]: Epoch: 043, Loss:0.5013 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:19,954]: Epoch: 044, Loss:0.4662 Train: 0.8583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:19,962]: Epoch: 045, Loss:0.4854 Train: 0.8583, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:19,970]: Epoch: 046, Loss:0.4578 Train: 0.8667, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:19,979]: Epoch: 047, Loss:0.4330 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:19,987]: Epoch: 048, Loss:0.4735 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:19,994]: Epoch: 049, Loss:0.4387 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:20,003]: Epoch: 050, Loss:0.4329 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:20,011]: Epoch: 051, Loss:0.4403 Train: 0.8917, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:20,019]: Epoch: 052, Loss:0.4436 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:20,027]: Epoch: 053, Loss:0.4443 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:20,036]: Epoch: 054, Loss:0.4107 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:20,044]: Epoch: 055, Loss:0.4393 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:20,052]: Epoch: 056, Loss:0.4161 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:20,060]: Epoch: 057, Loss:0.4273 Train: 0.8833, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:20,069]: Epoch: 058, Loss:0.4932 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:20,077]: Epoch: 059, Loss:0.4101 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:20,084]: Epoch: 060, Loss:0.4918 Train: 0.8667, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:20,092]: Epoch: 061, Loss:0.4683 Train: 0.8583, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:20,099]: Epoch: 062, Loss:0.4123 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:20,108]: Epoch: 063, Loss:0.4256 Train: 0.8833, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:20,114]: Epoch: 064, Loss:0.4786 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:20,122]: Epoch: 065, Loss:0.3867 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:20,130]: Epoch: 066, Loss:0.4574 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:20,138]: Epoch: 067, Loss:0.4411 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:20,146]: Epoch: 068, Loss:0.3780 Train: 0.9167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:20,154]: Epoch: 069, Loss:0.3672 Train: 0.9250, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:20,162]: Epoch: 070, Loss:0.3637 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:20,169]: Epoch: 071, Loss:0.3564 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:20,175]: Epoch: 072, Loss:0.3187 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:20,182]: Epoch: 073, Loss:0.3595 Train: 0.9250, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:20,189]: Epoch: 074, Loss:0.3214 Train: 0.9417, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:20,197]: Epoch: 075, Loss:0.3344 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:20,204]: Epoch: 076, Loss:0.3576 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:20,212]: Epoch: 077, Loss:0.3672 Train: 0.9250, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:20,220]: Epoch: 078, Loss:0.4000 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:20,228]: Epoch: 079, Loss:0.4074 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:20,235]: Epoch: 080, Loss:0.3941 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:20,242]: Epoch: 081, Loss:0.3496 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:20,249]: Epoch: 082, Loss:0.3442 Train: 0.9250, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:20,257]: Epoch: 083, Loss:0.4079 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:20,265]: Epoch: 084, Loss:0.3628 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:20,272]: Epoch: 085, Loss:0.3377 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:20,281]: Epoch: 086, Loss:0.3340 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0095
[2025-04-01 02:44:20,290]: Epoch: 087, Loss:0.3703 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:20,299]: Epoch: 088, Loss:0.3449 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:20,305]: Epoch: 089, Loss:0.3774 Train: 0.8833, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:20,314]: Epoch: 090, Loss:0.3567 Train: 0.8833, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:20,320]: Epoch: 091, Loss:0.3490 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:20,328]: Epoch: 092, Loss:0.3471 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:20,336]: Epoch: 093, Loss:0.3299 Train: 0.9000, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:20,343]: Epoch: 094, Loss:0.4453 Train: 0.9333, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:20,351]: Epoch: 095, Loss:0.3366 Train: 0.9417, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:20,359]: Epoch: 096, Loss:0.3010 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:20,367]: Epoch: 097, Loss:0.4278 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:20,373]: Epoch: 098, Loss:0.3180 Train: 0.9500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:20,381]: Epoch: 099, Loss:0.3646 Train: 0.9500, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:20,389]: Epoch: 100, Loss:0.3254 Train: 0.9417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:20,397]: Epoch: 101, Loss:0.3030 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:20,404]: Epoch: 102, Loss:0.2531 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:20,412]: Epoch: 103, Loss:0.3276 Train: 0.9417, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:20,419]: Epoch: 104, Loss:0.3466 Train: 0.9333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:20,427]: Epoch: 105, Loss:0.3468 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:20,435]: Epoch: 106, Loss:0.2971 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:20,441]: Epoch: 107, Loss:0.3580 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:20,449]: Epoch: 108, Loss:0.2805 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:20,457]: Epoch: 109, Loss:0.3292 Train: 0.9583, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:20,466]: Epoch: 110, Loss:0.2851 Train: 0.9500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:20,473]: Epoch: 111, Loss:0.2589 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:20,480]: Epoch: 112, Loss:0.2488 Train: 0.9417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:20,488]: Epoch: 113, Loss:0.2971 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:20,497]: Epoch: 114, Loss:0.3412 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:20,505]: Epoch: 115, Loss:0.2988 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:20,513]: Epoch: 116, Loss:0.3389 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:20,522]: Epoch: 117, Loss:0.2949 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:20,531]: Epoch: 118, Loss:0.3227 Train: 0.8750, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:20,540]: Epoch: 119, Loss:0.3289 Train: 0.8833, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:20,549]: Epoch: 120, Loss:0.3017 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0092
[2025-04-01 02:44:20,558]: Epoch: 121, Loss:0.5598 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:20,566]: Epoch: 122, Loss:0.3130 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:20,575]: Epoch: 123, Loss:0.3967 Train: 0.9333, Val:0.4000, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:20,581]: Epoch: 124, Loss:0.4000 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:20,590]: Epoch: 125, Loss:0.3635 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:20,599]: Epoch: 126, Loss:0.3922 Train: 0.9250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:20,608]: Epoch: 127, Loss:0.3199 Train: 0.8667, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:20,616]: Epoch: 128, Loss:0.3598 Train: 0.8500, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:20,624]: Epoch: 129, Loss:0.5057 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:20,633]: Epoch: 130, Loss:0.3426 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0091
[2025-04-01 02:44:20,642]: Epoch: 131, Loss:0.4346 Train: 0.9250, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:20,648]: Epoch: 132, Loss:0.2926 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:20,656]: Epoch: 133, Loss:0.3222 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:20,663]: Epoch: 134, Loss:0.2784 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:20,671]: Epoch: 135, Loss:0.3132 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:20,679]: Epoch: 136, Loss:0.3669 Train: 0.9333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:20,687]: Epoch: 137, Loss:0.3166 Train: 0.9417, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:20,693]: Epoch: 138, Loss:0.3192 Train: 0.9417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:20,703]: Epoch: 139, Loss:0.2926 Train: 0.9417, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:20,711]: Epoch: 140, Loss:0.3238 Train: 0.9417, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:20,719]: Epoch: 141, Loss:0.3526 Train: 0.9583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:20,727]: Epoch: 142, Loss:0.3131 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:20,735]: Epoch: 143, Loss:0.3299 Train: 0.9417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:20,743]: Epoch: 144, Loss:0.2917 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:20,750]: Epoch: 145, Loss:0.3016 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:20,761]: Epoch: 146, Loss:0.3114 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0103
[2025-04-01 02:44:20,774]: Epoch: 147, Loss:0.2938 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0128
[2025-04-01 02:44:20,783]: Epoch: 148, Loss:0.3175 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:20,796]: Epoch: 149, Loss:0.3164 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0129
[2025-04-01 02:44:20,805]: Epoch: 150, Loss:0.2943 Train: 0.9417, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:20,813]: Epoch: 151, Loss:0.3889 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:20,822]: Epoch: 152, Loss:0.3306 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:20,831]: Epoch: 153, Loss:0.3587 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:20,840]: Epoch: 154, Loss:0.3133 Train: 0.9500, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:20,848]: Epoch: 155, Loss:0.2628 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:20,854]: Epoch: 156, Loss:0.3600 Train: 0.8583, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:20,861]: Epoch: 157, Loss:0.3453 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:20,869]: Epoch: 158, Loss:0.2408 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:20,876]: Epoch: 159, Loss:0.3418 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:20,882]: Epoch: 160, Loss:0.4100 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:20,889]: Epoch: 161, Loss:0.3397 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:20,896]: Epoch: 162, Loss:0.3460 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:20,904]: Epoch: 163, Loss:0.2665 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:20,911]: Epoch: 164, Loss:0.2575 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:20,918]: Epoch: 165, Loss:0.3555 Train: 0.9583, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:20,927]: Epoch: 166, Loss:0.3669 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:20,935]: Epoch: 167, Loss:0.3081 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:20,942]: Epoch: 168, Loss:0.3152 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:20,951]: Epoch: 169, Loss:0.3340 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:20,958]: Epoch: 170, Loss:0.3963 Train: 0.9500, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:20,967]: Epoch: 171, Loss:0.3101 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:20,975]: Epoch: 172, Loss:0.3399 Train: 0.9083, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:20,983]: Epoch: 173, Loss:0.3263 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:20,990]: Epoch: 174, Loss:0.3212 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:20,998]: Epoch: 175, Loss:0.4192 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:21,006]: Epoch: 176, Loss:0.4652 Train: 0.9417, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:21,014]: Epoch: 177, Loss:0.2874 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:21,021]: Epoch: 178, Loss:0.3608 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:21,029]: Epoch: 179, Loss:0.3296 Train: 0.9583, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:21,038]: Epoch: 180, Loss:0.3573 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:21,047]: Epoch: 181, Loss:0.3249 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:21,054]: Epoch: 182, Loss:0.5061 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:21,061]: Epoch: 183, Loss:0.3223 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:21,070]: Epoch: 184, Loss:0.3063 Train: 0.8667, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0087
[2025-04-01 02:44:21,079]: Epoch: 185, Loss:0.5714 Train: 0.9000, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:21,086]: Epoch: 186, Loss:0.3196 Train: 0.9333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:21,095]: Epoch: 187, Loss:0.2654 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:21,102]: Epoch: 188, Loss:0.3217 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:21,109]: Epoch: 189, Loss:0.4139 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:21,118]: Epoch: 190, Loss:0.3945 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:21,125]: Epoch: 191, Loss:0.3597 Train: 0.8417, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:21,133]: Epoch: 192, Loss:0.5420 Train: 0.8583, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:21,140]: Epoch: 193, Loss:0.4663 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:21,148]: Epoch: 194, Loss:0.4055 Train: 0.8583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:21,156]: Epoch: 195, Loss:0.4171 Train: 0.7917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:21,163]: Epoch: 196, Loss:0.4709 Train: 0.7917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:21,170]: Epoch: 197, Loss:0.5701 Train: 0.8583, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:21,177]: Epoch: 198, Loss:0.3639 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:21,184]: Epoch: 199, Loss:0.3795 Train: 0.8667, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:21,191]: Epoch: 200, Loss:0.3563 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:21,192]: [Run-2 score] {'train': 0.65, 'val': 0.5375, 'test': 0.49019607843137253}
[2025-04-01 02:44:21,192]: repeat 3/3
[2025-04-01 02:44:21,192]: Manual random seed:0
[2025-04-01 02:44:21,192]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:21,196]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:21,206]: Epoch: 001, Loss:1.7002 Train: 0.5667, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:21,212]: Epoch: 002, Loss:1.3328 Train: 0.6250, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:44:21,220]: Epoch: 003, Loss:1.2210 Train: 0.5917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:21,227]: Epoch: 004, Loss:1.0635 Train: 0.6917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:21,235]: Epoch: 005, Loss:1.1190 Train: 0.6833, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:21,243]: Epoch: 006, Loss:0.8743 Train: 0.6667, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:21,250]: Epoch: 007, Loss:0.8760 Train: 0.6750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:21,259]: Epoch: 008, Loss:0.8388 Train: 0.6917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:21,266]: Epoch: 009, Loss:0.7757 Train: 0.7583, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:21,275]: Epoch: 010, Loss:0.7268 Train: 0.7500, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:21,284]: Epoch: 011, Loss:0.8694 Train: 0.7667, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0090
[2025-04-01 02:44:21,293]: Epoch: 012, Loss:0.7572 Train: 0.8167, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:21,301]: Epoch: 013, Loss:0.6887 Train: 0.8417, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:44:21,310]: Epoch: 014, Loss:0.5724 Train: 0.8167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:21,319]: Epoch: 015, Loss:0.6546 Train: 0.8000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:21,327]: Epoch: 016, Loss:0.6732 Train: 0.7917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:21,333]: Epoch: 017, Loss:0.5615 Train: 0.7917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:21,341]: Epoch: 018, Loss:0.5616 Train: 0.8250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:21,348]: Epoch: 019, Loss:0.5696 Train: 0.7833, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:21,356]: Epoch: 020, Loss:0.5617 Train: 0.8167, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:21,364]: Epoch: 021, Loss:0.5384 Train: 0.8250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:21,370]: Epoch: 022, Loss:0.5819 Train: 0.8417, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:21,377]: Epoch: 023, Loss:0.5131 Train: 0.8250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:21,385]: Epoch: 024, Loss:0.6352 Train: 0.8500, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:21,393]: Epoch: 025, Loss:0.4833 Train: 0.8417, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:21,401]: Epoch: 026, Loss:0.4210 Train: 0.8417, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:21,410]: Epoch: 027, Loss:0.5313 Train: 0.8500, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:21,418]: Epoch: 028, Loss:0.5630 Train: 0.8750, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:21,426]: Epoch: 029, Loss:0.4767 Train: 0.8833, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:21,434]: Epoch: 030, Loss:0.4909 Train: 0.8583, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:21,443]: Epoch: 031, Loss:0.4550 Train: 0.8500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:21,450]: Epoch: 032, Loss:0.4473 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:21,459]: Epoch: 033, Loss:0.4728 Train: 0.9083, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:21,467]: Epoch: 034, Loss:0.5303 Train: 0.8583, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:21,473]: Epoch: 035, Loss:0.5567 Train: 0.8583, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:21,481]: Epoch: 036, Loss:0.4560 Train: 0.8917, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:21,488]: Epoch: 037, Loss:0.3890 Train: 0.9167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:21,494]: Epoch: 038, Loss:0.4200 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:21,502]: Epoch: 039, Loss:0.4310 Train: 0.9083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:21,510]: Epoch: 040, Loss:0.4514 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:21,517]: Epoch: 041, Loss:0.4001 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:21,524]: Epoch: 042, Loss:0.3936 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:21,530]: Epoch: 043, Loss:0.5097 Train: 0.8667, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:21,537]: Epoch: 044, Loss:0.4188 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:21,544]: Epoch: 045, Loss:0.4324 Train: 0.8833, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:21,552]: Epoch: 046, Loss:0.4187 Train: 0.8667, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:21,560]: Epoch: 047, Loss:0.4392 Train: 0.8750, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:21,568]: Epoch: 048, Loss:0.3849 Train: 0.9000, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:21,577]: Epoch: 049, Loss:0.3732 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:21,583]: Epoch: 050, Loss:0.5070 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:21,590]: Epoch: 051, Loss:0.4191 Train: 0.9167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:21,599]: Epoch: 052, Loss:0.3743 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:21,606]: Epoch: 053, Loss:0.3859 Train: 0.9250, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:21,613]: Epoch: 054, Loss:0.3482 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:21,620]: Epoch: 055, Loss:0.4692 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:21,627]: Epoch: 056, Loss:0.3650 Train: 0.9000, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:21,634]: Epoch: 057, Loss:0.3166 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:21,642]: Epoch: 058, Loss:0.3523 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:21,650]: Epoch: 059, Loss:0.3719 Train: 0.9333, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:21,657]: Epoch: 060, Loss:0.3397 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:21,664]: Epoch: 061, Loss:0.4034 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:21,671]: Epoch: 062, Loss:0.4324 Train: 0.9000, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:21,679]: Epoch: 063, Loss:0.3510 Train: 0.9083, Val:0.3875, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:21,687]: Epoch: 064, Loss:0.3984 Train: 0.9083, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:21,695]: Epoch: 065, Loss:0.4120 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:21,703]: Epoch: 066, Loss:0.4150 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:21,710]: Epoch: 067, Loss:0.4212 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:21,717]: Epoch: 068, Loss:0.4256 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:21,725]: Epoch: 069, Loss:0.3976 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:21,732]: Epoch: 070, Loss:0.3147 Train: 0.8833, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:21,740]: Epoch: 071, Loss:0.4013 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:21,748]: Epoch: 072, Loss:0.3417 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:21,758]: Epoch: 073, Loss:0.2946 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:21,766]: Epoch: 074, Loss:0.3426 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:21,773]: Epoch: 075, Loss:0.4194 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:21,782]: Epoch: 076, Loss:0.3511 Train: 0.9250, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0086
[2025-04-01 02:44:21,791]: Epoch: 077, Loss:0.3467 Train: 0.9083, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:21,799]: Epoch: 078, Loss:0.3394 Train: 0.9083, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:21,808]: Epoch: 079, Loss:0.3670 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:21,815]: Epoch: 080, Loss:0.3472 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:21,823]: Epoch: 081, Loss:0.2789 Train: 0.9167, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:21,831]: Epoch: 082, Loss:0.4584 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:21,839]: Epoch: 083, Loss:0.3369 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:21,847]: Epoch: 084, Loss:0.3684 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:21,854]: Epoch: 085, Loss:0.2695 Train: 0.9583, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:21,862]: Epoch: 086, Loss:0.3090 Train: 0.9333, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:21,870]: Epoch: 087, Loss:0.3485 Train: 0.9417, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:21,878]: Epoch: 088, Loss:0.3631 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:21,885]: Epoch: 089, Loss:0.2914 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:21,893]: Epoch: 090, Loss:0.3686 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:21,901]: Epoch: 091, Loss:0.3541 Train: 0.9333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:21,909]: Epoch: 092, Loss:0.3399 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:21,916]: Epoch: 093, Loss:0.3904 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:21,924]: Epoch: 094, Loss:0.3840 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:21,932]: Epoch: 095, Loss:0.2651 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:21,940]: Epoch: 096, Loss:0.2921 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:21,948]: Epoch: 097, Loss:0.2739 Train: 0.9167, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:21,956]: Epoch: 098, Loss:0.4810 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:21,965]: Epoch: 099, Loss:0.3278 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:21,972]: Epoch: 100, Loss:0.3445 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:21,980]: Epoch: 101, Loss:0.3280 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:21,987]: Epoch: 102, Loss:0.3204 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:21,994]: Epoch: 103, Loss:0.3758 Train: 0.9333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:22,002]: Epoch: 104, Loss:0.3408 Train: 0.9250, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:22,010]: Epoch: 105, Loss:0.3276 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:22,018]: Epoch: 106, Loss:0.3578 Train: 0.9250, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:22,025]: Epoch: 107, Loss:0.3244 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:22,033]: Epoch: 108, Loss:0.3938 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:22,041]: Epoch: 109, Loss:0.3334 Train: 0.9083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:22,048]: Epoch: 110, Loss:0.4067 Train: 0.9583, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:22,057]: Epoch: 111, Loss:0.2806 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:22,064]: Epoch: 112, Loss:0.3235 Train: 0.9083, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:22,072]: Epoch: 113, Loss:0.3411 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:22,080]: Epoch: 114, Loss:0.3183 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:22,088]: Epoch: 115, Loss:0.3068 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:22,094]: Epoch: 116, Loss:0.3102 Train: 0.9250, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:22,102]: Epoch: 117, Loss:0.3179 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:22,109]: Epoch: 118, Loss:0.3016 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:22,118]: Epoch: 119, Loss:0.3119 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:22,125]: Epoch: 120, Loss:0.3424 Train: 0.9333, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:22,132]: Epoch: 121, Loss:0.3009 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:22,140]: Epoch: 122, Loss:0.2791 Train: 0.9250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:22,148]: Epoch: 123, Loss:0.2599 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:22,155]: Epoch: 124, Loss:0.2678 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:22,163]: Epoch: 125, Loss:0.3311 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:22,171]: Epoch: 126, Loss:0.3326 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:22,179]: Epoch: 127, Loss:0.3531 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:22,185]: Epoch: 128, Loss:0.4665 Train: 0.9583, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:22,193]: Epoch: 129, Loss:0.3185 Train: 0.9417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:22,201]: Epoch: 130, Loss:0.2791 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:22,208]: Epoch: 131, Loss:0.3252 Train: 0.9417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:22,215]: Epoch: 132, Loss:0.3653 Train: 0.9500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:22,224]: Epoch: 133, Loss:0.2584 Train: 0.9500, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:22,232]: Epoch: 134, Loss:0.3208 Train: 0.9500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:22,239]: Epoch: 135, Loss:0.3030 Train: 0.9500, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:22,246]: Epoch: 136, Loss:0.2932 Train: 0.9500, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:22,254]: Epoch: 137, Loss:0.2997 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:22,262]: Epoch: 138, Loss:0.3036 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:22,270]: Epoch: 139, Loss:0.2948 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:22,280]: Epoch: 140, Loss:0.2825 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0096
[2025-04-01 02:44:22,289]: Epoch: 141, Loss:0.2740 Train: 0.9500, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:22,298]: Epoch: 142, Loss:0.2771 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:22,306]: Epoch: 143, Loss:0.2499 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:22,314]: Epoch: 144, Loss:0.3222 Train: 0.9500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:22,322]: Epoch: 145, Loss:0.2317 Train: 0.9667, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:22,330]: Epoch: 146, Loss:0.2375 Train: 0.9333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:22,338]: Epoch: 147, Loss:0.3084 Train: 0.9583, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:22,345]: Epoch: 148, Loss:0.2835 Train: 0.9583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:22,353]: Epoch: 149, Loss:0.2478 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:22,361]: Epoch: 150, Loss:0.2903 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:22,369]: Epoch: 151, Loss:0.3325 Train: 0.9667, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:22,377]: Epoch: 152, Loss:0.2789 Train: 0.7917, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:22,385]: Epoch: 153, Loss:0.8028 Train: 0.9667, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:22,393]: Epoch: 154, Loss:0.2297 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:22,401]: Epoch: 155, Loss:0.2774 Train: 0.8083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:22,409]: Epoch: 156, Loss:0.4152 Train: 0.8750, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:22,416]: Epoch: 157, Loss:0.5805 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:22,425]: Epoch: 158, Loss:0.3571 Train: 0.8667, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:22,433]: Epoch: 159, Loss:0.5143 Train: 0.8417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:22,441]: Epoch: 160, Loss:0.3713 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:22,449]: Epoch: 161, Loss:0.4373 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:22,457]: Epoch: 162, Loss:0.4533 Train: 0.8583, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:22,465]: Epoch: 163, Loss:0.4204 Train: 0.8333, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:22,475]: Epoch: 164, Loss:0.4990 Train: 0.8750, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:22,481]: Epoch: 165, Loss:0.3915 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:22,489]: Epoch: 166, Loss:0.3338 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:22,497]: Epoch: 167, Loss:0.3391 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:22,504]: Epoch: 168, Loss:0.4164 Train: 0.8833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:22,512]: Epoch: 169, Loss:0.4303 Train: 0.8917, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:22,520]: Epoch: 170, Loss:0.3555 Train: 0.9250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:22,527]: Epoch: 171, Loss:0.3341 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:22,535]: Epoch: 172, Loss:0.3185 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:22,543]: Epoch: 173, Loss:0.3144 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:22,551]: Epoch: 174, Loss:0.3389 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:22,560]: Epoch: 175, Loss:0.3132 Train: 0.9167, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:22,566]: Epoch: 176, Loss:0.3000 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:22,576]: Epoch: 177, Loss:0.3199 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:22,584]: Epoch: 178, Loss:0.4009 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:22,592]: Epoch: 179, Loss:0.2568 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:22,601]: Epoch: 180, Loss:0.3143 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0090
[2025-04-01 02:44:22,609]: Epoch: 181, Loss:0.2938 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:22,618]: Epoch: 182, Loss:0.4730 Train: 0.9500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:22,624]: Epoch: 183, Loss:0.2476 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:22,633]: Epoch: 184, Loss:0.3582 Train: 0.8750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:22,641]: Epoch: 185, Loss:0.3714 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:22,650]: Epoch: 186, Loss:0.4739 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:22,658]: Epoch: 187, Loss:0.2788 Train: 0.9583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:22,665]: Epoch: 188, Loss:0.2463 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:22,673]: Epoch: 189, Loss:0.2881 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:22,681]: Epoch: 190, Loss:0.2978 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:22,690]: Epoch: 191, Loss:0.4077 Train: 0.9583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:22,698]: Epoch: 192, Loss:0.3399 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:22,707]: Epoch: 193, Loss:0.2807 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:22,715]: Epoch: 194, Loss:0.4174 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:22,723]: Epoch: 195, Loss:0.3807 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:22,731]: Epoch: 196, Loss:0.3158 Train: 0.9417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:22,739]: Epoch: 197, Loss:0.2890 Train: 0.9417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:22,747]: Epoch: 198, Loss:0.4281 Train: 0.9500, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:22,754]: Epoch: 199, Loss:0.3583 Train: 0.9333, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:22,760]: Epoch: 200, Loss:0.3033 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:22,760]: [Run-3 score] {'train': 0.5666666666666667, 'val': 0.55, 'test': 0.5098039215686274}
[2025-04-01 02:44:22,760]: repeat 1/3
[2025-04-01 02:44:22,760]: Manual random seed:0
[2025-04-01 02:44:22,761]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:22,764]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:22,775]: Epoch: 001, Loss:1.6675 Train: 0.4750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:22,782]: Epoch: 002, Loss:2.3721 Train: 0.5667, Val:0.5375, Test: 0.5882, Time(s/epoch):0.0075
[2025-04-01 02:44:22,790]: Epoch: 003, Loss:1.2943 Train: 0.5917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:22,798]: Epoch: 004, Loss:1.1588 Train: 0.6250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:22,806]: Epoch: 005, Loss:1.1591 Train: 0.6500, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:44:22,813]: Epoch: 006, Loss:1.0440 Train: 0.6250, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:22,820]: Epoch: 007, Loss:1.0125 Train: 0.6417, Val:0.5750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:22,828]: Epoch: 008, Loss:1.0368 Train: 0.6417, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:22,836]: Epoch: 009, Loss:0.9903 Train: 0.6500, Val:0.5625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:22,842]: Epoch: 010, Loss:0.8945 Train: 0.6667, Val:0.5625, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:22,849]: Epoch: 011, Loss:0.9381 Train: 0.6583, Val:0.5500, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:22,857]: Epoch: 012, Loss:0.8503 Train: 0.6833, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:22,865]: Epoch: 013, Loss:0.8181 Train: 0.7083, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:22,874]: Epoch: 014, Loss:0.8209 Train: 0.7083, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:22,880]: Epoch: 015, Loss:0.7853 Train: 0.6917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:22,888]: Epoch: 016, Loss:0.7660 Train: 0.7167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:22,897]: Epoch: 017, Loss:0.7491 Train: 0.7417, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:22,904]: Epoch: 018, Loss:0.7610 Train: 0.7667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:22,911]: Epoch: 019, Loss:0.6978 Train: 0.7667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:22,920]: Epoch: 020, Loss:0.7304 Train: 0.7417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:22,927]: Epoch: 021, Loss:0.7298 Train: 0.7417, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:22,933]: Epoch: 022, Loss:0.7019 Train: 0.7583, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:22,941]: Epoch: 023, Loss:0.6950 Train: 0.7750, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:22,948]: Epoch: 024, Loss:0.6764 Train: 0.7917, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:22,957]: Epoch: 025, Loss:0.6685 Train: 0.7833, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:22,966]: Epoch: 026, Loss:0.6512 Train: 0.7750, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:22,974]: Epoch: 027, Loss:0.6260 Train: 0.8167, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:22,982]: Epoch: 028, Loss:0.7177 Train: 0.8333, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:22,989]: Epoch: 029, Loss:0.6446 Train: 0.8250, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:22,997]: Epoch: 030, Loss:0.6191 Train: 0.8000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,003]: Epoch: 031, Loss:0.6416 Train: 0.7917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:23,010]: Epoch: 032, Loss:0.6248 Train: 0.7917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:23,018]: Epoch: 033, Loss:0.6214 Train: 0.8083, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:23,026]: Epoch: 034, Loss:0.5828 Train: 0.8250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:23,033]: Epoch: 035, Loss:0.6348 Train: 0.8083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:23,041]: Epoch: 036, Loss:0.5608 Train: 0.8083, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:23,049]: Epoch: 037, Loss:0.6099 Train: 0.8417, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:23,057]: Epoch: 038, Loss:0.5581 Train: 0.8417, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,063]: Epoch: 039, Loss:0.5724 Train: 0.8417, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:23,071]: Epoch: 040, Loss:0.5846 Train: 0.8333, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:23,078]: Epoch: 041, Loss:0.5905 Train: 0.8333, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:23,086]: Epoch: 042, Loss:0.5679 Train: 0.8500, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:23,094]: Epoch: 043, Loss:0.5601 Train: 0.8167, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:23,101]: Epoch: 044, Loss:0.5207 Train: 0.8333, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:23,109]: Epoch: 045, Loss:0.5891 Train: 0.8333, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:23,118]: Epoch: 046, Loss:0.5659 Train: 0.8333, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:23,126]: Epoch: 047, Loss:0.5446 Train: 0.8583, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:23,134]: Epoch: 048, Loss:0.5397 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:23,144]: Epoch: 049, Loss:0.4762 Train: 0.8500, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:23,152]: Epoch: 050, Loss:0.5404 Train: 0.8667, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:23,161]: Epoch: 051, Loss:0.5492 Train: 0.8667, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:23,169]: Epoch: 052, Loss:0.5014 Train: 0.8667, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:23,178]: Epoch: 053, Loss:0.4713 Train: 0.8667, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0093
[2025-04-01 02:44:23,188]: Epoch: 054, Loss:0.4624 Train: 0.8750, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0089
[2025-04-01 02:44:23,197]: Epoch: 055, Loss:0.5451 Train: 0.8500, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:23,206]: Epoch: 056, Loss:0.4922 Train: 0.8667, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0096
[2025-04-01 02:44:23,214]: Epoch: 057, Loss:0.5010 Train: 0.8917, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:23,222]: Epoch: 058, Loss:0.4777 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:23,231]: Epoch: 059, Loss:0.4880 Train: 0.8667, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:23,238]: Epoch: 060, Loss:0.5116 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:23,245]: Epoch: 061, Loss:0.4921 Train: 0.8917, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:23,252]: Epoch: 062, Loss:0.4933 Train: 0.8667, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:23,259]: Epoch: 063, Loss:0.5432 Train: 0.8583, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:23,267]: Epoch: 064, Loss:0.5343 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:23,276]: Epoch: 065, Loss:0.5085 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:23,286]: Epoch: 066, Loss:0.4899 Train: 0.8667, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0102
[2025-04-01 02:44:23,296]: Epoch: 067, Loss:0.4806 Train: 0.8667, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:23,305]: Epoch: 068, Loss:0.5168 Train: 0.8250, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:23,313]: Epoch: 069, Loss:0.5059 Train: 0.8250, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:23,321]: Epoch: 070, Loss:0.6167 Train: 0.8500, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:23,329]: Epoch: 071, Loss:0.5049 Train: 0.8917, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:23,336]: Epoch: 072, Loss:0.4437 Train: 0.8417, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:23,344]: Epoch: 073, Loss:0.4752 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:23,351]: Epoch: 074, Loss:0.4954 Train: 0.8667, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:23,360]: Epoch: 075, Loss:0.5085 Train: 0.8500, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:23,367]: Epoch: 076, Loss:0.5779 Train: 0.8583, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:23,375]: Epoch: 077, Loss:0.4993 Train: 0.8833, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:23,381]: Epoch: 078, Loss:0.4798 Train: 0.8750, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:23,389]: Epoch: 079, Loss:0.4480 Train: 0.8667, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:23,395]: Epoch: 080, Loss:0.4688 Train: 0.8833, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:23,404]: Epoch: 081, Loss:0.4777 Train: 0.8667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:23,410]: Epoch: 082, Loss:0.4537 Train: 0.8500, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:23,417]: Epoch: 083, Loss:0.4556 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:23,425]: Epoch: 084, Loss:0.4656 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:23,431]: Epoch: 085, Loss:0.4125 Train: 0.8917, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:23,439]: Epoch: 086, Loss:0.4042 Train: 0.8667, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:23,447]: Epoch: 087, Loss:0.4078 Train: 0.8750, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:23,455]: Epoch: 088, Loss:0.4795 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,463]: Epoch: 089, Loss:0.3883 Train: 0.8583, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:23,470]: Epoch: 090, Loss:0.4858 Train: 0.8667, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:23,479]: Epoch: 091, Loss:0.4421 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:23,487]: Epoch: 092, Loss:0.4022 Train: 0.8667, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:23,494]: Epoch: 093, Loss:0.3927 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:23,503]: Epoch: 094, Loss:0.4806 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:23,510]: Epoch: 095, Loss:0.4134 Train: 0.8667, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:23,519]: Epoch: 096, Loss:0.4593 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:23,525]: Epoch: 097, Loss:0.3958 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:23,533]: Epoch: 098, Loss:0.4107 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:23,540]: Epoch: 099, Loss:0.4360 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:23,548]: Epoch: 100, Loss:0.4434 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:23,554]: Epoch: 101, Loss:0.4673 Train: 0.8833, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:23,562]: Epoch: 102, Loss:0.3952 Train: 0.8500, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:23,570]: Epoch: 103, Loss:0.4805 Train: 0.8583, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:23,577]: Epoch: 104, Loss:0.4097 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:23,584]: Epoch: 105, Loss:0.4062 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:23,591]: Epoch: 106, Loss:0.4253 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:23,600]: Epoch: 107, Loss:0.3985 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:23,607]: Epoch: 108, Loss:0.3994 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:23,614]: Epoch: 109, Loss:0.4475 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:23,620]: Epoch: 110, Loss:0.4168 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:23,628]: Epoch: 111, Loss:0.4379 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:23,634]: Epoch: 112, Loss:0.4472 Train: 0.8833, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:23,642]: Epoch: 113, Loss:0.3965 Train: 0.8833, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:23,650]: Epoch: 114, Loss:0.4133 Train: 0.9417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:23,656]: Epoch: 115, Loss:0.4012 Train: 0.9000, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:23,663]: Epoch: 116, Loss:0.4055 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:23,670]: Epoch: 117, Loss:0.4113 Train: 0.8833, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:23,678]: Epoch: 118, Loss:0.5135 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:23,686]: Epoch: 119, Loss:0.4835 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,692]: Epoch: 120, Loss:0.4027 Train: 0.8667, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:23,699]: Epoch: 121, Loss:0.4864 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:23,707]: Epoch: 122, Loss:0.3780 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:23,714]: Epoch: 123, Loss:0.4303 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:23,722]: Epoch: 124, Loss:0.4589 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,730]: Epoch: 125, Loss:0.4533 Train: 0.8917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:23,737]: Epoch: 126, Loss:0.4135 Train: 0.8833, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:23,746]: Epoch: 127, Loss:0.4223 Train: 0.9083, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:23,752]: Epoch: 128, Loss:0.4479 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:23,760]: Epoch: 129, Loss:0.3646 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:23,768]: Epoch: 130, Loss:0.4206 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:23,775]: Epoch: 131, Loss:0.3845 Train: 0.8750, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:23,781]: Epoch: 132, Loss:0.4902 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:23,790]: Epoch: 133, Loss:0.3644 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:23,798]: Epoch: 134, Loss:0.4214 Train: 0.8750, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:23,808]: Epoch: 135, Loss:0.4868 Train: 0.9250, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:23,815]: Epoch: 136, Loss:0.4076 Train: 0.9250, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:23,824]: Epoch: 137, Loss:0.3944 Train: 0.9167, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:23,831]: Epoch: 138, Loss:0.4527 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:23,837]: Epoch: 139, Loss:0.4189 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:23,844]: Epoch: 140, Loss:0.4317 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:23,852]: Epoch: 141, Loss:0.3942 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:23,859]: Epoch: 142, Loss:0.4092 Train: 0.9333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:23,866]: Epoch: 143, Loss:0.3783 Train: 0.9167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:23,873]: Epoch: 144, Loss:0.4114 Train: 0.8917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:23,880]: Epoch: 145, Loss:0.4163 Train: 0.8667, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:23,887]: Epoch: 146, Loss:0.3821 Train: 0.8583, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:23,895]: Epoch: 147, Loss:0.5304 Train: 0.9000, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:23,901]: Epoch: 148, Loss:0.4242 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:44:23,908]: Epoch: 149, Loss:0.4004 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:23,915]: Epoch: 150, Loss:0.4479 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:23,921]: Epoch: 151, Loss:0.4607 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:23,928]: Epoch: 152, Loss:0.4347 Train: 0.9167, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:23,935]: Epoch: 153, Loss:0.3781 Train: 0.9250, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:23,943]: Epoch: 154, Loss:0.4693 Train: 0.9083, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:23,950]: Epoch: 155, Loss:0.3918 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:23,957]: Epoch: 156, Loss:0.3959 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:23,964]: Epoch: 157, Loss:0.4185 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:23,971]: Epoch: 158, Loss:0.3780 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:23,979]: Epoch: 159, Loss:0.3876 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:23,986]: Epoch: 160, Loss:0.4052 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:23,994]: Epoch: 161, Loss:0.4123 Train: 0.9000, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:24,001]: Epoch: 162, Loss:0.3886 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:24,007]: Epoch: 163, Loss:0.3979 Train: 0.8917, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:24,015]: Epoch: 164, Loss:0.4469 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:24,022]: Epoch: 165, Loss:0.3711 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:24,030]: Epoch: 166, Loss:0.4427 Train: 0.9417, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:24,038]: Epoch: 167, Loss:0.3620 Train: 0.9333, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:24,046]: Epoch: 168, Loss:0.3782 Train: 0.9167, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:24,054]: Epoch: 169, Loss:0.3922 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:24,062]: Epoch: 170, Loss:0.3402 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:24,068]: Epoch: 171, Loss:0.3732 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:24,075]: Epoch: 172, Loss:0.4384 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:24,083]: Epoch: 173, Loss:0.3689 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:24,089]: Epoch: 174, Loss:0.3779 Train: 0.9250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:24,097]: Epoch: 175, Loss:0.4500 Train: 0.9167, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:24,105]: Epoch: 176, Loss:0.3968 Train: 0.9000, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:24,112]: Epoch: 177, Loss:0.4447 Train: 0.8833, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:24,119]: Epoch: 178, Loss:0.3833 Train: 0.9000, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:24,126]: Epoch: 179, Loss:0.3746 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:24,132]: Epoch: 180, Loss:0.4436 Train: 0.9583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:24,140]: Epoch: 181, Loss:0.3320 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:24,147]: Epoch: 182, Loss:0.4547 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:24,154]: Epoch: 183, Loss:0.4212 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:24,161]: Epoch: 184, Loss:0.5486 Train: 0.8583, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:24,167]: Epoch: 185, Loss:0.4656 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:24,175]: Epoch: 186, Loss:0.3964 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:24,183]: Epoch: 187, Loss:0.3422 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:24,190]: Epoch: 188, Loss:0.4246 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:24,198]: Epoch: 189, Loss:0.3720 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:24,204]: Epoch: 190, Loss:0.4141 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:24,211]: Epoch: 191, Loss:0.4028 Train: 0.9167, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:24,219]: Epoch: 192, Loss:0.4027 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:24,226]: Epoch: 193, Loss:0.4418 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:24,233]: Epoch: 194, Loss:0.3738 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:24,239]: Epoch: 195, Loss:0.3831 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:24,247]: Epoch: 196, Loss:0.3603 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:24,254]: Epoch: 197, Loss:0.4208 Train: 0.9000, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0074
[2025-04-01 02:44:24,261]: Epoch: 198, Loss:0.3444 Train: 0.8750, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:24,267]: Epoch: 199, Loss:0.4737 Train: 0.8833, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:24,276]: Epoch: 200, Loss:0.3525 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:24,276]: [Run-1 score] {'train': 0.6416666666666667, 'val': 0.575, 'test': 0.5098039215686274}
[2025-04-01 02:44:24,276]: repeat 2/3
[2025-04-01 02:44:24,276]: Manual random seed:0
[2025-04-01 02:44:24,277]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:24,280]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:24,292]: Epoch: 001, Loss:1.5688 Train: 0.4750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:24,300]: Epoch: 002, Loss:2.0745 Train: 0.5083, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:24,308]: Epoch: 003, Loss:1.2955 Train: 0.5917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:24,316]: Epoch: 004, Loss:1.2128 Train: 0.4250, Val:0.2500, Test: 0.2549, Time(s/epoch):0.0080
[2025-04-01 02:44:24,324]: Epoch: 005, Loss:1.1374 Train: 0.5917, Val:0.3375, Test: 0.3137, Time(s/epoch):0.0078
[2025-04-01 02:44:24,331]: Epoch: 006, Loss:1.3984 Train: 0.6750, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:24,339]: Epoch: 007, Loss:1.0559 Train: 0.6250, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:24,347]: Epoch: 008, Loss:1.0058 Train: 0.6167, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:24,356]: Epoch: 009, Loss:1.1340 Train: 0.6333, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:24,363]: Epoch: 010, Loss:1.1182 Train: 0.6417, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:24,371]: Epoch: 011, Loss:1.0273 Train: 0.6583, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:24,379]: Epoch: 012, Loss:0.8384 Train: 0.6750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:24,386]: Epoch: 013, Loss:0.8637 Train: 0.7333, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:24,395]: Epoch: 014, Loss:0.8308 Train: 0.7417, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:24,401]: Epoch: 015, Loss:0.8611 Train: 0.7083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:24,407]: Epoch: 016, Loss:0.8056 Train: 0.6917, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:24,415]: Epoch: 017, Loss:0.8090 Train: 0.7167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:24,423]: Epoch: 018, Loss:0.8784 Train: 0.7417, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:24,430]: Epoch: 019, Loss:0.7865 Train: 0.7667, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:24,438]: Epoch: 020, Loss:0.7522 Train: 0.7417, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:24,445]: Epoch: 021, Loss:0.7395 Train: 0.7333, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:24,451]: Epoch: 022, Loss:0.7194 Train: 0.7167, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:24,460]: Epoch: 023, Loss:0.7432 Train: 0.7167, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:24,468]: Epoch: 024, Loss:0.7074 Train: 0.7250, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:24,474]: Epoch: 025, Loss:0.7229 Train: 0.7417, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:24,480]: Epoch: 026, Loss:0.6802 Train: 0.7750, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:24,488]: Epoch: 027, Loss:0.6623 Train: 0.7667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:24,495]: Epoch: 028, Loss:0.6424 Train: 0.7583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:24,503]: Epoch: 029, Loss:0.6896 Train: 0.7750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:24,510]: Epoch: 030, Loss:0.6999 Train: 0.8000, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:24,519]: Epoch: 031, Loss:0.6387 Train: 0.8083, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:24,526]: Epoch: 032, Loss:0.6079 Train: 0.7917, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:24,534]: Epoch: 033, Loss:0.6291 Train: 0.7667, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:24,542]: Epoch: 034, Loss:0.6535 Train: 0.7833, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:24,548]: Epoch: 035, Loss:0.6255 Train: 0.8083, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:24,555]: Epoch: 036, Loss:0.6037 Train: 0.8250, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:24,563]: Epoch: 037, Loss:0.5737 Train: 0.8333, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:24,571]: Epoch: 038, Loss:0.6058 Train: 0.8333, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:24,579]: Epoch: 039, Loss:0.6092 Train: 0.8250, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:24,585]: Epoch: 040, Loss:0.6027 Train: 0.8333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:24,592]: Epoch: 041, Loss:0.5800 Train: 0.8333, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:24,601]: Epoch: 042, Loss:0.6166 Train: 0.8333, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:24,608]: Epoch: 043, Loss:0.5751 Train: 0.8167, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:24,615]: Epoch: 044, Loss:0.5587 Train: 0.8333, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:24,622]: Epoch: 045, Loss:0.5856 Train: 0.8333, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:24,630]: Epoch: 046, Loss:0.5329 Train: 0.8500, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:24,638]: Epoch: 047, Loss:0.5807 Train: 0.8500, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:24,646]: Epoch: 048, Loss:0.5789 Train: 0.8250, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:24,654]: Epoch: 049, Loss:0.5977 Train: 0.8167, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:24,661]: Epoch: 050, Loss:0.5405 Train: 0.8500, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:24,669]: Epoch: 051, Loss:0.5638 Train: 0.8667, Val:0.5125, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:24,678]: Epoch: 052, Loss:0.4971 Train: 0.8667, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:24,686]: Epoch: 053, Loss:0.5240 Train: 0.8667, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:24,695]: Epoch: 054, Loss:0.5588 Train: 0.8667, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:24,702]: Epoch: 055, Loss:0.5450 Train: 0.8583, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:24,710]: Epoch: 056, Loss:0.5225 Train: 0.8417, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:24,718]: Epoch: 057, Loss:0.4953 Train: 0.8500, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:24,727]: Epoch: 058, Loss:0.5193 Train: 0.8500, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:24,735]: Epoch: 059, Loss:0.4601 Train: 0.8583, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:24,744]: Epoch: 060, Loss:0.4915 Train: 0.8583, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:24,752]: Epoch: 061, Loss:0.4812 Train: 0.8500, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:24,760]: Epoch: 062, Loss:0.4986 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:24,768]: Epoch: 063, Loss:0.4943 Train: 0.8583, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:24,776]: Epoch: 064, Loss:0.5084 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:24,785]: Epoch: 065, Loss:0.6056 Train: 0.8583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:24,793]: Epoch: 066, Loss:0.5344 Train: 0.8417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:24,802]: Epoch: 067, Loss:0.5515 Train: 0.8417, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:24,808]: Epoch: 068, Loss:0.4992 Train: 0.8833, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:24,816]: Epoch: 069, Loss:0.5011 Train: 0.8417, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:24,823]: Epoch: 070, Loss:0.5966 Train: 0.8583, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:24,830]: Epoch: 071, Loss:0.5195 Train: 0.8667, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:24,837]: Epoch: 072, Loss:0.5505 Train: 0.8583, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:24,844]: Epoch: 073, Loss:0.5328 Train: 0.8667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:24,851]: Epoch: 074, Loss:0.5078 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:24,859]: Epoch: 075, Loss:0.5828 Train: 0.8500, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:24,866]: Epoch: 076, Loss:0.5231 Train: 0.8417, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:24,874]: Epoch: 077, Loss:0.5504 Train: 0.8417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:24,882]: Epoch: 078, Loss:0.4993 Train: 0.8167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:24,890]: Epoch: 079, Loss:0.5064 Train: 0.8000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:24,898]: Epoch: 080, Loss:0.6009 Train: 0.8333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:24,906]: Epoch: 081, Loss:0.5129 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:24,913]: Epoch: 082, Loss:0.5042 Train: 0.8000, Val:0.3750, Test: 0.3333, Time(s/epoch):0.0074
[2025-04-01 02:44:24,920]: Epoch: 083, Loss:0.5362 Train: 0.8167, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:24,928]: Epoch: 084, Loss:0.6888 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:24,936]: Epoch: 085, Loss:0.5085 Train: 0.7583, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:24,942]: Epoch: 086, Loss:0.5932 Train: 0.7333, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:24,949]: Epoch: 087, Loss:0.6684 Train: 0.7833, Val:0.5250, Test: 0.5686, Time(s/epoch):0.0067
[2025-04-01 02:44:24,957]: Epoch: 088, Loss:0.6392 Train: 0.8583, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:24,964]: Epoch: 089, Loss:0.5151 Train: 0.8417, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:24,971]: Epoch: 090, Loss:0.5232 Train: 0.8000, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:24,979]: Epoch: 091, Loss:0.6157 Train: 0.8250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:24,986]: Epoch: 092, Loss:0.5405 Train: 0.8583, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:24,994]: Epoch: 093, Loss:0.5048 Train: 0.8250, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:25,004]: Epoch: 094, Loss:0.6561 Train: 0.8417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0098
[2025-04-01 02:44:25,015]: Epoch: 095, Loss:0.4752 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0102
[2025-04-01 02:44:25,025]: Epoch: 096, Loss:0.4563 Train: 0.8750, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0096
[2025-04-01 02:44:25,034]: Epoch: 097, Loss:0.5449 Train: 0.8583, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:25,043]: Epoch: 098, Loss:0.5586 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:25,051]: Epoch: 099, Loss:0.5545 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:25,064]: Epoch: 100, Loss:0.5207 Train: 0.8500, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0135
[2025-04-01 02:44:25,072]: Epoch: 101, Loss:0.4891 Train: 0.8250, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:25,081]: Epoch: 102, Loss:0.5019 Train: 0.8250, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:25,088]: Epoch: 103, Loss:0.5381 Train: 0.8250, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:25,097]: Epoch: 104, Loss:0.4715 Train: 0.8083, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:25,105]: Epoch: 105, Loss:0.4787 Train: 0.8250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:25,113]: Epoch: 106, Loss:0.4602 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:25,122]: Epoch: 107, Loss:0.4560 Train: 0.8667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:25,130]: Epoch: 108, Loss:0.4887 Train: 0.8667, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:25,138]: Epoch: 109, Loss:0.4580 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:25,145]: Epoch: 110, Loss:0.4528 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:25,152]: Epoch: 111, Loss:0.5061 Train: 0.8583, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:25,161]: Epoch: 112, Loss:0.4566 Train: 0.8667, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:25,169]: Epoch: 113, Loss:0.4510 Train: 0.8583, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:25,176]: Epoch: 114, Loss:0.4234 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:25,183]: Epoch: 115, Loss:0.4791 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:25,191]: Epoch: 116, Loss:0.4965 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:25,200]: Epoch: 117, Loss:0.4180 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:25,209]: Epoch: 118, Loss:0.4625 Train: 0.8667, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0090
[2025-04-01 02:44:25,220]: Epoch: 119, Loss:0.4624 Train: 0.8750, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0110
[2025-04-01 02:44:25,228]: Epoch: 120, Loss:0.4581 Train: 0.8833, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:25,237]: Epoch: 121, Loss:0.4165 Train: 0.8750, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:25,243]: Epoch: 122, Loss:0.4291 Train: 0.8583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:25,251]: Epoch: 123, Loss:0.4711 Train: 0.8583, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:25,259]: Epoch: 124, Loss:0.4488 Train: 0.8833, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:25,268]: Epoch: 125, Loss:0.4326 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:25,277]: Epoch: 126, Loss:0.4047 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:25,286]: Epoch: 127, Loss:0.5124 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:25,295]: Epoch: 128, Loss:0.4034 Train: 0.8500, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:25,302]: Epoch: 129, Loss:0.4985 Train: 0.8583, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:25,310]: Epoch: 130, Loss:0.4687 Train: 0.8917, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:25,317]: Epoch: 131, Loss:0.4263 Train: 0.8917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:25,325]: Epoch: 132, Loss:0.4737 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:25,332]: Epoch: 133, Loss:0.4145 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:25,340]: Epoch: 134, Loss:0.4266 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:25,348]: Epoch: 135, Loss:0.3707 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:25,356]: Epoch: 136, Loss:0.3924 Train: 0.8917, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:25,364]: Epoch: 137, Loss:0.3864 Train: 0.9000, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:25,370]: Epoch: 138, Loss:0.4478 Train: 0.9333, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:44:25,379]: Epoch: 139, Loss:0.4014 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:25,386]: Epoch: 140, Loss:0.3964 Train: 0.9000, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:25,394]: Epoch: 141, Loss:0.4155 Train: 0.9000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:25,402]: Epoch: 142, Loss:0.4430 Train: 0.8833, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:25,408]: Epoch: 143, Loss:0.4206 Train: 0.8500, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:25,415]: Epoch: 144, Loss:0.4115 Train: 0.8667, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:25,422]: Epoch: 145, Loss:0.3607 Train: 0.8667, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:25,429]: Epoch: 146, Loss:0.4275 Train: 0.8667, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:25,436]: Epoch: 147, Loss:0.4078 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:25,442]: Epoch: 148, Loss:0.3968 Train: 0.8833, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:25,449]: Epoch: 149, Loss:0.4646 Train: 0.9250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:25,457]: Epoch: 150, Loss:0.3848 Train: 0.9083, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:25,465]: Epoch: 151, Loss:0.3919 Train: 0.8917, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:25,471]: Epoch: 152, Loss:0.4575 Train: 0.8917, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0056
[2025-04-01 02:44:25,478]: Epoch: 153, Loss:0.4308 Train: 0.8500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:25,486]: Epoch: 154, Loss:0.4222 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:25,492]: Epoch: 155, Loss:0.3478 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0057
[2025-04-01 02:44:25,500]: Epoch: 156, Loss:0.3849 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:25,508]: Epoch: 157, Loss:0.4011 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:25,515]: Epoch: 158, Loss:0.3549 Train: 0.8750, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:25,521]: Epoch: 159, Loss:0.4594 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:25,529]: Epoch: 160, Loss:0.4295 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:25,536]: Epoch: 161, Loss:0.4214 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:25,544]: Epoch: 162, Loss:0.4510 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:25,550]: Epoch: 163, Loss:0.4620 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:25,557]: Epoch: 164, Loss:0.3903 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:25,564]: Epoch: 165, Loss:0.3846 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:25,570]: Epoch: 166, Loss:0.4123 Train: 0.8750, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:25,578]: Epoch: 167, Loss:0.3840 Train: 0.8750, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:25,585]: Epoch: 168, Loss:0.3917 Train: 0.8917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:25,592]: Epoch: 169, Loss:0.4366 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:25,599]: Epoch: 170, Loss:0.3689 Train: 0.8583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:25,607]: Epoch: 171, Loss:0.3965 Train: 0.8417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:25,615]: Epoch: 172, Loss:0.5291 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:25,623]: Epoch: 173, Loss:0.3664 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:25,629]: Epoch: 174, Loss:0.3741 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:25,636]: Epoch: 175, Loss:0.4778 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:25,644]: Epoch: 176, Loss:0.4854 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:25,651]: Epoch: 177, Loss:0.4789 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:25,658]: Epoch: 178, Loss:0.4311 Train: 0.8583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:25,665]: Epoch: 179, Loss:0.4584 Train: 0.8667, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:25,671]: Epoch: 180, Loss:0.5044 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:25,678]: Epoch: 181, Loss:0.4802 Train: 0.8750, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:25,685]: Epoch: 182, Loss:0.4270 Train: 0.8833, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:25,692]: Epoch: 183, Loss:0.4687 Train: 0.8833, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:25,699]: Epoch: 184, Loss:0.4039 Train: 0.8333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:25,706]: Epoch: 185, Loss:0.4048 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:25,713]: Epoch: 186, Loss:0.4390 Train: 0.8833, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:25,719]: Epoch: 187, Loss:0.4096 Train: 0.8583, Val:0.3625, Test: 0.3333, Time(s/epoch):0.0065
[2025-04-01 02:44:25,726]: Epoch: 188, Loss:0.5990 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:25,732]: Epoch: 189, Loss:0.4637 Train: 0.8750, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:44:25,739]: Epoch: 190, Loss:0.3697 Train: 0.8583, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:25,746]: Epoch: 191, Loss:0.5514 Train: 0.8500, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:25,753]: Epoch: 192, Loss:0.3600 Train: 0.8667, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:25,760]: Epoch: 193, Loss:0.4472 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:25,769]: Epoch: 194, Loss:0.4025 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:25,777]: Epoch: 195, Loss:0.4600 Train: 0.8833, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:25,784]: Epoch: 196, Loss:0.4061 Train: 0.9000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:25,790]: Epoch: 197, Loss:0.3949 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0058
[2025-04-01 02:44:25,797]: Epoch: 198, Loss:0.3915 Train: 0.8917, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:25,804]: Epoch: 199, Loss:0.3799 Train: 0.8917, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:25,811]: Epoch: 200, Loss:0.4144 Train: 0.9000, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:25,811]: [Run-2 score] {'train': 0.675, 'val': 0.5625, 'test': 0.43137254901960786}
[2025-04-01 02:44:25,811]: repeat 3/3
[2025-04-01 02:44:25,811]: Manual random seed:0
[2025-04-01 02:44:25,812]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:25,815]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:25,825]: Epoch: 001, Loss:1.7001 Train: 0.5500, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:25,834]: Epoch: 002, Loss:1.2594 Train: 0.5083, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:25,842]: Epoch: 003, Loss:1.3566 Train: 0.6750, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:25,848]: Epoch: 004, Loss:1.1523 Train: 0.5417, Val:0.3125, Test: 0.3333, Time(s/epoch):0.0059
[2025-04-01 02:44:25,855]: Epoch: 005, Loss:1.2276 Train: 0.7333, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:25,863]: Epoch: 006, Loss:0.9270 Train: 0.6667, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:25,870]: Epoch: 007, Loss:1.0347 Train: 0.6750, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:25,877]: Epoch: 008, Loss:0.9288 Train: 0.7167, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:25,885]: Epoch: 009, Loss:0.8681 Train: 0.7583, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:25,891]: Epoch: 010, Loss:0.8331 Train: 0.7417, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:25,898]: Epoch: 011, Loss:0.7970 Train: 0.6417, Val:0.3375, Test: 0.3333, Time(s/epoch):0.0066
[2025-04-01 02:44:25,905]: Epoch: 012, Loss:0.9117 Train: 0.7167, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:25,913]: Epoch: 013, Loss:0.8440 Train: 0.7583, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:25,919]: Epoch: 014, Loss:0.7545 Train: 0.7583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:25,926]: Epoch: 015, Loss:0.7311 Train: 0.7333, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:25,934]: Epoch: 016, Loss:0.7334 Train: 0.7750, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:25,940]: Epoch: 017, Loss:0.7425 Train: 0.8000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:25,949]: Epoch: 018, Loss:0.6571 Train: 0.8167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:25,955]: Epoch: 019, Loss:0.6394 Train: 0.8250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:25,963]: Epoch: 020, Loss:0.6874 Train: 0.8167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:25,971]: Epoch: 021, Loss:0.6456 Train: 0.8167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:25,979]: Epoch: 022, Loss:0.6309 Train: 0.8083, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:25,987]: Epoch: 023, Loss:0.6008 Train: 0.8250, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:25,995]: Epoch: 024, Loss:0.5969 Train: 0.8000, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:26,003]: Epoch: 025, Loss:0.6093 Train: 0.8083, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:26,010]: Epoch: 026, Loss:0.5924 Train: 0.8000, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:26,018]: Epoch: 027, Loss:0.5805 Train: 0.8167, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:26,025]: Epoch: 028, Loss:0.5652 Train: 0.8083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:26,034]: Epoch: 029, Loss:0.5635 Train: 0.8250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:26,041]: Epoch: 030, Loss:0.5897 Train: 0.8250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:26,049]: Epoch: 031, Loss:0.5254 Train: 0.8417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:26,057]: Epoch: 032, Loss:0.6185 Train: 0.8083, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:26,065]: Epoch: 033, Loss:0.5337 Train: 0.8250, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:26,071]: Epoch: 034, Loss:0.5490 Train: 0.8583, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:26,079]: Epoch: 035, Loss:0.5522 Train: 0.8500, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:26,086]: Epoch: 036, Loss:0.5058 Train: 0.8500, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:26,094]: Epoch: 037, Loss:0.5038 Train: 0.8500, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:26,102]: Epoch: 038, Loss:0.5545 Train: 0.8500, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:26,108]: Epoch: 039, Loss:0.5432 Train: 0.8333, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:26,115]: Epoch: 040, Loss:0.5127 Train: 0.8250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:26,122]: Epoch: 041, Loss:0.5266 Train: 0.8250, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:26,130]: Epoch: 042, Loss:0.4889 Train: 0.8750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:26,138]: Epoch: 043, Loss:0.4702 Train: 0.8583, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:26,145]: Epoch: 044, Loss:0.5004 Train: 0.8583, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:26,153]: Epoch: 045, Loss:0.4709 Train: 0.8333, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:26,160]: Epoch: 046, Loss:0.5348 Train: 0.8417, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:26,170]: Epoch: 047, Loss:0.4547 Train: 0.8750, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0094
[2025-04-01 02:44:26,184]: Epoch: 048, Loss:0.5267 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0144
[2025-04-01 02:44:26,201]: Epoch: 049, Loss:0.4791 Train: 0.8750, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0166
[2025-04-01 02:44:26,212]: Epoch: 050, Loss:0.5515 Train: 0.8833, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0109
[2025-04-01 02:44:26,219]: Epoch: 051, Loss:0.4787 Train: 0.8583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:26,230]: Epoch: 052, Loss:0.4451 Train: 0.8333, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0108
[2025-04-01 02:44:26,237]: Epoch: 053, Loss:0.4623 Train: 0.8750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:26,243]: Epoch: 054, Loss:0.4632 Train: 0.8667, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:26,249]: Epoch: 055, Loss:0.5346 Train: 0.8667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0057
[2025-04-01 02:44:26,256]: Epoch: 056, Loss:0.4851 Train: 0.8500, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:26,262]: Epoch: 057, Loss:0.4719 Train: 0.8500, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:26,270]: Epoch: 058, Loss:0.4710 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:26,279]: Epoch: 059, Loss:0.4486 Train: 0.8667, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:26,288]: Epoch: 060, Loss:0.4820 Train: 0.8583, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:26,296]: Epoch: 061, Loss:0.4196 Train: 0.8667, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:26,305]: Epoch: 062, Loss:0.4406 Train: 0.8583, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:26,311]: Epoch: 063, Loss:0.4647 Train: 0.8417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:26,319]: Epoch: 064, Loss:0.4790 Train: 0.8750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:26,326]: Epoch: 065, Loss:0.4843 Train: 0.8417, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:26,332]: Epoch: 066, Loss:0.4516 Train: 0.8750, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:26,340]: Epoch: 067, Loss:0.4713 Train: 0.8750, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:26,350]: Epoch: 068, Loss:0.4442 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0090
[2025-04-01 02:44:26,357]: Epoch: 069, Loss:0.4798 Train: 0.9083, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:26,364]: Epoch: 070, Loss:0.4744 Train: 0.8750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:26,372]: Epoch: 071, Loss:0.4465 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:26,378]: Epoch: 072, Loss:0.4990 Train: 0.8667, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:26,386]: Epoch: 073, Loss:0.4326 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:26,393]: Epoch: 074, Loss:0.4023 Train: 0.8750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:26,400]: Epoch: 075, Loss:0.4695 Train: 0.8750, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:26,407]: Epoch: 076, Loss:0.4052 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:26,414]: Epoch: 077, Loss:0.5357 Train: 0.9000, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:26,422]: Epoch: 078, Loss:0.4443 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:26,430]: Epoch: 079, Loss:0.4542 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:26,437]: Epoch: 080, Loss:0.4279 Train: 0.8750, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:26,444]: Epoch: 081, Loss:0.5038 Train: 0.8583, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:26,451]: Epoch: 082, Loss:0.4034 Train: 0.8500, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:26,459]: Epoch: 083, Loss:0.4576 Train: 0.8333, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:26,467]: Epoch: 084, Loss:0.4824 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:26,475]: Epoch: 085, Loss:0.4359 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:26,483]: Epoch: 086, Loss:0.3783 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:26,490]: Epoch: 087, Loss:0.4278 Train: 0.9000, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:26,497]: Epoch: 088, Loss:0.5278 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:26,503]: Epoch: 089, Loss:0.4958 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:26,510]: Epoch: 090, Loss:0.4298 Train: 0.8667, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:26,516]: Epoch: 091, Loss:0.4482 Train: 0.8917, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:26,523]: Epoch: 092, Loss:0.4246 Train: 0.8833, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:26,531]: Epoch: 093, Loss:0.3939 Train: 0.8667, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:26,539]: Epoch: 094, Loss:0.5094 Train: 0.8583, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:26,547]: Epoch: 095, Loss:0.3832 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:26,553]: Epoch: 096, Loss:0.4402 Train: 0.8833, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:26,561]: Epoch: 097, Loss:0.3824 Train: 0.8667, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:26,569]: Epoch: 098, Loss:0.4440 Train: 0.8417, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:26,577]: Epoch: 099, Loss:0.4308 Train: 0.8583, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:26,585]: Epoch: 100, Loss:0.4452 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:26,591]: Epoch: 101, Loss:0.4280 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0056
[2025-04-01 02:44:26,599]: Epoch: 102, Loss:0.4534 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:26,605]: Epoch: 103, Loss:0.3959 Train: 0.8417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:26,612]: Epoch: 104, Loss:0.4685 Train: 0.8500, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:26,619]: Epoch: 105, Loss:0.4305 Train: 0.8917, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:26,627]: Epoch: 106, Loss:0.4382 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:26,634]: Epoch: 107, Loss:0.4405 Train: 0.9083, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:26,640]: Epoch: 108, Loss:0.4399 Train: 0.9000, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:26,648]: Epoch: 109, Loss:0.4323 Train: 0.9000, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:26,656]: Epoch: 110, Loss:0.3802 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:26,664]: Epoch: 111, Loss:0.3812 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:26,672]: Epoch: 112, Loss:0.4168 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:26,680]: Epoch: 113, Loss:0.4571 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:26,688]: Epoch: 114, Loss:0.4665 Train: 0.8833, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:26,695]: Epoch: 115, Loss:0.4438 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:26,703]: Epoch: 116, Loss:0.5207 Train: 0.8833, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:26,711]: Epoch: 117, Loss:0.3759 Train: 0.8583, Val:0.5500, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:26,718]: Epoch: 118, Loss:0.4727 Train: 0.8417, Val:0.5500, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:44:26,725]: Epoch: 119, Loss:0.4915 Train: 0.8833, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:26,732]: Epoch: 120, Loss:0.4295 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:26,740]: Epoch: 121, Loss:0.4021 Train: 0.8667, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:26,748]: Epoch: 122, Loss:0.3903 Train: 0.8583, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:26,756]: Epoch: 123, Loss:0.3907 Train: 0.8417, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:26,764]: Epoch: 124, Loss:0.4214 Train: 0.8583, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:26,772]: Epoch: 125, Loss:0.3890 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:26,780]: Epoch: 126, Loss:0.3876 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:26,788]: Epoch: 127, Loss:0.4245 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:26,797]: Epoch: 128, Loss:0.5084 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:26,805]: Epoch: 129, Loss:0.3868 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:26,815]: Epoch: 130, Loss:0.4337 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0093
[2025-04-01 02:44:26,823]: Epoch: 131, Loss:0.3730 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:26,831]: Epoch: 132, Loss:0.3560 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:26,840]: Epoch: 133, Loss:0.4320 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:26,849]: Epoch: 134, Loss:0.4083 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:26,856]: Epoch: 135, Loss:0.4299 Train: 0.8667, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:26,864]: Epoch: 136, Loss:0.4083 Train: 0.8583, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:26,873]: Epoch: 137, Loss:0.4736 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:26,880]: Epoch: 138, Loss:0.3787 Train: 0.9250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:26,889]: Epoch: 139, Loss:0.4142 Train: 0.9500, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:26,896]: Epoch: 140, Loss:0.4035 Train: 0.9083, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:26,902]: Epoch: 141, Loss:0.4036 Train: 0.8750, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:26,911]: Epoch: 142, Loss:0.3694 Train: 0.8917, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:26,919]: Epoch: 143, Loss:0.3969 Train: 0.9250, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:26,926]: Epoch: 144, Loss:0.4094 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:26,934]: Epoch: 145, Loss:0.4257 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:26,942]: Epoch: 146, Loss:0.4103 Train: 0.8917, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:26,950]: Epoch: 147, Loss:0.3584 Train: 0.8917, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:26,958]: Epoch: 148, Loss:0.4137 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:26,965]: Epoch: 149, Loss:0.3954 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:26,973]: Epoch: 150, Loss:0.3866 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:26,981]: Epoch: 151, Loss:0.3851 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:26,987]: Epoch: 152, Loss:0.4148 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:26,995]: Epoch: 153, Loss:0.3772 Train: 0.9167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:27,001]: Epoch: 154, Loss:0.4206 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:27,009]: Epoch: 155, Loss:0.4737 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:27,017]: Epoch: 156, Loss:0.3548 Train: 0.8750, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:27,024]: Epoch: 157, Loss:0.4472 Train: 0.9000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:27,031]: Epoch: 158, Loss:0.3654 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:27,039]: Epoch: 159, Loss:0.3598 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:27,047]: Epoch: 160, Loss:0.3366 Train: 0.9417, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,055]: Epoch: 161, Loss:0.3461 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:27,063]: Epoch: 162, Loss:0.3423 Train: 0.9000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:27,069]: Epoch: 163, Loss:0.3529 Train: 0.8833, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:27,076]: Epoch: 164, Loss:0.3915 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:27,084]: Epoch: 165, Loss:0.3643 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:27,093]: Epoch: 166, Loss:0.4467 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:27,100]: Epoch: 167, Loss:0.3769 Train: 0.9583, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:27,108]: Epoch: 168, Loss:0.3706 Train: 0.9583, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:27,115]: Epoch: 169, Loss:0.3339 Train: 0.9583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:27,121]: Epoch: 170, Loss:0.3853 Train: 0.9167, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:27,129]: Epoch: 171, Loss:0.3628 Train: 0.8833, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:27,137]: Epoch: 172, Loss:0.3671 Train: 0.9083, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:27,144]: Epoch: 173, Loss:0.4011 Train: 0.9167, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:27,151]: Epoch: 174, Loss:0.3216 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:27,159]: Epoch: 175, Loss:0.3767 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,166]: Epoch: 176, Loss:0.3400 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:27,174]: Epoch: 177, Loss:0.3377 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:27,181]: Epoch: 178, Loss:0.3721 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:27,189]: Epoch: 179, Loss:0.3239 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:27,198]: Epoch: 180, Loss:0.3774 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:27,205]: Epoch: 181, Loss:0.3237 Train: 0.9250, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:27,211]: Epoch: 182, Loss:0.3741 Train: 0.9167, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:27,219]: Epoch: 183, Loss:0.3566 Train: 0.8833, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:27,226]: Epoch: 184, Loss:0.3169 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:27,234]: Epoch: 185, Loss:0.3910 Train: 0.9167, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,240]: Epoch: 186, Loss:0.3370 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:27,248]: Epoch: 187, Loss:0.3905 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:27,256]: Epoch: 188, Loss:0.3626 Train: 0.9250, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,262]: Epoch: 189, Loss:0.3230 Train: 0.8667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:27,270]: Epoch: 190, Loss:0.4371 Train: 0.9417, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:27,278]: Epoch: 191, Loss:0.3406 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:27,288]: Epoch: 192, Loss:0.4370 Train: 0.9333, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0096
[2025-04-01 02:44:27,299]: Epoch: 193, Loss:0.3772 Train: 0.8583, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0111
[2025-04-01 02:44:27,308]: Epoch: 194, Loss:0.3354 Train: 0.8667, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:27,317]: Epoch: 195, Loss:0.4441 Train: 0.8917, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:27,326]: Epoch: 196, Loss:0.3707 Train: 0.9250, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0092
[2025-04-01 02:44:27,335]: Epoch: 197, Loss:0.3894 Train: 0.9333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:27,343]: Epoch: 198, Loss:0.3339 Train: 0.9667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:27,351]: Epoch: 199, Loss:0.3617 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:27,360]: Epoch: 200, Loss:0.3641 Train: 0.9167, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:27,360]: [Run-3 score] {'train': 0.8583333333333333, 'val': 0.55, 'test': 0.5686274509803921}
[2025-04-01 02:44:27,360]: repeat 1/3
[2025-04-01 02:44:27,360]: Manual random seed:0
[2025-04-01 02:44:27,361]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:27,364]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:27,375]: Epoch: 001, Loss:1.6812 Train: 0.4750, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:27,382]: Epoch: 002, Loss:2.1482 Train: 0.5917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:27,388]: Epoch: 003, Loss:1.1307 Train: 0.6333, Val:0.4125, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:44:27,395]: Epoch: 004, Loss:1.0909 Train: 0.6167, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0065
[2025-04-01 02:44:27,401]: Epoch: 005, Loss:0.9999 Train: 0.6833, Val:0.4500, Test: 0.3725, Time(s/epoch):0.0058
[2025-04-01 02:44:27,409]: Epoch: 006, Loss:0.9488 Train: 0.6917, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:27,415]: Epoch: 007, Loss:0.9579 Train: 0.6917, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:27,423]: Epoch: 008, Loss:0.9996 Train: 0.7000, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:27,429]: Epoch: 009, Loss:0.8176 Train: 0.6917, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:27,436]: Epoch: 010, Loss:0.7304 Train: 0.7500, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:27,444]: Epoch: 011, Loss:0.8659 Train: 0.7333, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:27,452]: Epoch: 012, Loss:0.7549 Train: 0.7250, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:27,460]: Epoch: 013, Loss:0.7184 Train: 0.7333, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:27,467]: Epoch: 014, Loss:0.6754 Train: 0.7750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:27,475]: Epoch: 015, Loss:0.7533 Train: 0.7833, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:27,482]: Epoch: 016, Loss:0.6454 Train: 0.8000, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:27,489]: Epoch: 017, Loss:0.5934 Train: 0.8167, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:27,496]: Epoch: 018, Loss:0.6027 Train: 0.8250, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:27,504]: Epoch: 019, Loss:0.5566 Train: 0.8167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:27,510]: Epoch: 020, Loss:0.5800 Train: 0.8000, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:27,517]: Epoch: 021, Loss:0.5666 Train: 0.8000, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:27,524]: Epoch: 022, Loss:0.5751 Train: 0.8000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:27,530]: Epoch: 023, Loss:0.5520 Train: 0.8167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:27,538]: Epoch: 024, Loss:0.5191 Train: 0.8167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:27,545]: Epoch: 025, Loss:0.5404 Train: 0.8250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:27,552]: Epoch: 026, Loss:0.5124 Train: 0.8250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:27,558]: Epoch: 027, Loss:0.5123 Train: 0.8250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:27,566]: Epoch: 028, Loss:0.4432 Train: 0.8333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:27,574]: Epoch: 029, Loss:0.4993 Train: 0.8333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:27,580]: Epoch: 030, Loss:0.4977 Train: 0.8500, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0054
[2025-04-01 02:44:27,587]: Epoch: 031, Loss:0.5207 Train: 0.8667, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:27,595]: Epoch: 032, Loss:0.5064 Train: 0.8500, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:27,602]: Epoch: 033, Loss:0.4350 Train: 0.8500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:27,608]: Epoch: 034, Loss:0.4224 Train: 0.8667, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:27,616]: Epoch: 035, Loss:0.4570 Train: 0.8750, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:27,622]: Epoch: 036, Loss:0.4418 Train: 0.8833, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:27,630]: Epoch: 037, Loss:0.4376 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:27,638]: Epoch: 038, Loss:0.4851 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:27,646]: Epoch: 039, Loss:0.4267 Train: 0.8833, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:27,654]: Epoch: 040, Loss:0.4739 Train: 0.8833, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:27,660]: Epoch: 041, Loss:0.4910 Train: 0.8583, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:27,668]: Epoch: 042, Loss:0.4849 Train: 0.8250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:27,676]: Epoch: 043, Loss:0.4634 Train: 0.8250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:27,683]: Epoch: 044, Loss:0.5092 Train: 0.8583, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:27,691]: Epoch: 045, Loss:0.4458 Train: 0.8417, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:27,698]: Epoch: 046, Loss:0.4707 Train: 0.8333, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:27,704]: Epoch: 047, Loss:0.5242 Train: 0.8500, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:27,711]: Epoch: 048, Loss:0.4332 Train: 0.8667, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:27,719]: Epoch: 049, Loss:0.4041 Train: 0.8583, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:27,726]: Epoch: 050, Loss:0.4827 Train: 0.8333, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:27,733]: Epoch: 051, Loss:0.4569 Train: 0.8250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:27,741]: Epoch: 052, Loss:0.5252 Train: 0.8417, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:27,749]: Epoch: 053, Loss:0.4258 Train: 0.8333, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:27,756]: Epoch: 054, Loss:0.4016 Train: 0.8667, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,764]: Epoch: 055, Loss:0.4264 Train: 0.8667, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:27,772]: Epoch: 056, Loss:0.4342 Train: 0.8750, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:27,779]: Epoch: 057, Loss:0.4198 Train: 0.8667, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:27,788]: Epoch: 058, Loss:0.3794 Train: 0.8667, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:27,796]: Epoch: 059, Loss:0.3774 Train: 0.8583, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:27,803]: Epoch: 060, Loss:0.3744 Train: 0.8500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:27,811]: Epoch: 061, Loss:0.3705 Train: 0.8583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:27,818]: Epoch: 062, Loss:0.3950 Train: 0.8833, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:27,826]: Epoch: 063, Loss:0.3678 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:27,834]: Epoch: 064, Loss:0.3683 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:27,842]: Epoch: 065, Loss:0.3970 Train: 0.8833, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:27,850]: Epoch: 066, Loss:0.4334 Train: 0.8750, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:27,857]: Epoch: 067, Loss:0.3664 Train: 0.8667, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:27,864]: Epoch: 068, Loss:0.3996 Train: 0.8417, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:27,872]: Epoch: 069, Loss:0.4637 Train: 0.8500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:27,879]: Epoch: 070, Loss:0.3326 Train: 0.8917, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:27,885]: Epoch: 071, Loss:0.3994 Train: 0.8917, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:27,892]: Epoch: 072, Loss:0.4097 Train: 0.8750, Val:0.3250, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:27,900]: Epoch: 073, Loss:0.3941 Train: 0.9000, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:27,908]: Epoch: 074, Loss:0.6830 Train: 0.8667, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:27,916]: Epoch: 075, Loss:0.4335 Train: 0.7917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:27,922]: Epoch: 076, Loss:0.7855 Train: 0.8667, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:27,929]: Epoch: 077, Loss:0.4382 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:27,936]: Epoch: 078, Loss:0.4027 Train: 0.9000, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:27,943]: Epoch: 079, Loss:0.4273 Train: 0.8833, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:27,951]: Epoch: 080, Loss:0.3898 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:27,958]: Epoch: 081, Loss:0.4214 Train: 0.9000, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:27,965]: Epoch: 082, Loss:0.4415 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:27,971]: Epoch: 083, Loss:0.3964 Train: 0.8750, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:27,981]: Epoch: 084, Loss:0.4743 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0101
[2025-04-01 02:44:27,987]: Epoch: 085, Loss:0.3828 Train: 0.8667, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:27,994]: Epoch: 086, Loss:0.3876 Train: 0.8917, Val:0.3375, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:28,001]: Epoch: 087, Loss:0.3694 Train: 0.9083, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:28,008]: Epoch: 088, Loss:0.4051 Train: 0.9083, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:28,016]: Epoch: 089, Loss:0.3866 Train: 0.9083, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:28,024]: Epoch: 090, Loss:0.4068 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:28,031]: Epoch: 091, Loss:0.3689 Train: 0.9417, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:28,039]: Epoch: 092, Loss:0.3402 Train: 0.9250, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:28,047]: Epoch: 093, Loss:0.3638 Train: 0.9250, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:28,054]: Epoch: 094, Loss:0.3630 Train: 0.9250, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:28,062]: Epoch: 095, Loss:0.3692 Train: 0.8750, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:28,070]: Epoch: 096, Loss:0.3774 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:28,078]: Epoch: 097, Loss:0.3846 Train: 0.8833, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:28,086]: Epoch: 098, Loss:0.3400 Train: 0.9000, Val:0.3375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:28,094]: Epoch: 099, Loss:0.3606 Train: 0.9083, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:28,102]: Epoch: 100, Loss:0.3624 Train: 0.9083, Val:0.3500, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:28,110]: Epoch: 101, Loss:0.3474 Train: 0.8917, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:28,118]: Epoch: 102, Loss:0.3155 Train: 0.8917, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:28,124]: Epoch: 103, Loss:0.3360 Train: 0.8917, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:28,132]: Epoch: 104, Loss:0.3785 Train: 0.8917, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:28,140]: Epoch: 105, Loss:0.3182 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:28,147]: Epoch: 106, Loss:0.3537 Train: 0.9000, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:28,154]: Epoch: 107, Loss:0.3606 Train: 0.9250, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:28,161]: Epoch: 108, Loss:0.2986 Train: 0.8667, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:28,169]: Epoch: 109, Loss:0.3201 Train: 0.8667, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:28,176]: Epoch: 110, Loss:0.3863 Train: 0.8833, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:28,183]: Epoch: 111, Loss:0.3539 Train: 0.9333, Val:0.3375, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:28,190]: Epoch: 112, Loss:0.3529 Train: 0.8667, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:28,197]: Epoch: 113, Loss:0.4238 Train: 0.9333, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:28,205]: Epoch: 114, Loss:0.3262 Train: 0.9333, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:28,213]: Epoch: 115, Loss:0.3242 Train: 0.9250, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:28,219]: Epoch: 116, Loss:0.3552 Train: 0.9417, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:28,226]: Epoch: 117, Loss:0.3388 Train: 0.9417, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:28,233]: Epoch: 118, Loss:0.3333 Train: 0.9250, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:28,239]: Epoch: 119, Loss:0.3503 Train: 0.9500, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:28,247]: Epoch: 120, Loss:0.3122 Train: 0.9083, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:28,255]: Epoch: 121, Loss:0.3714 Train: 0.9167, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:28,261]: Epoch: 122, Loss:0.2713 Train: 0.9000, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:28,269]: Epoch: 123, Loss:0.3834 Train: 0.9083, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:28,279]: Epoch: 124, Loss:0.3802 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0096
[2025-04-01 02:44:28,287]: Epoch: 125, Loss:0.3545 Train: 0.9000, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:28,296]: Epoch: 126, Loss:0.3208 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0092
[2025-04-01 02:44:28,305]: Epoch: 127, Loss:0.3057 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:28,311]: Epoch: 128, Loss:0.3265 Train: 0.9417, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:28,320]: Epoch: 129, Loss:0.3061 Train: 0.9583, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:28,328]: Epoch: 130, Loss:0.3213 Train: 0.9333, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:28,336]: Epoch: 131, Loss:0.3394 Train: 0.9083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:28,342]: Epoch: 132, Loss:0.3464 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:28,351]: Epoch: 133, Loss:0.3470 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:28,358]: Epoch: 134, Loss:0.3271 Train: 0.9083, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:28,365]: Epoch: 135, Loss:0.3699 Train: 0.9250, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:28,373]: Epoch: 136, Loss:0.3280 Train: 0.9167, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:28,381]: Epoch: 137, Loss:0.3155 Train: 0.9167, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:28,389]: Epoch: 138, Loss:0.3449 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:28,397]: Epoch: 139, Loss:0.2962 Train: 0.9083, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:28,405]: Epoch: 140, Loss:0.3448 Train: 0.9167, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:28,413]: Epoch: 141, Loss:0.3200 Train: 0.9250, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:28,420]: Epoch: 142, Loss:0.2869 Train: 0.9250, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:28,427]: Epoch: 143, Loss:0.3109 Train: 0.9333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:28,435]: Epoch: 144, Loss:0.3230 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:28,443]: Epoch: 145, Loss:0.3486 Train: 0.8917, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:28,450]: Epoch: 146, Loss:0.3391 Train: 0.8750, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:28,458]: Epoch: 147, Loss:0.4183 Train: 0.9333, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:28,465]: Epoch: 148, Loss:0.2871 Train: 0.9500, Val:0.3375, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:28,471]: Epoch: 149, Loss:0.3007 Train: 0.9167, Val:0.3250, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:28,478]: Epoch: 150, Loss:0.4092 Train: 0.9417, Val:0.3375, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:28,486]: Epoch: 151, Loss:0.3663 Train: 0.9083, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:28,492]: Epoch: 152, Loss:0.3437 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:28,500]: Epoch: 153, Loss:0.3104 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:28,507]: Epoch: 154, Loss:0.3772 Train: 0.9167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:28,515]: Epoch: 155, Loss:0.3484 Train: 0.9500, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:28,521]: Epoch: 156, Loss:0.3007 Train: 0.9417, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0058
[2025-04-01 02:44:28,528]: Epoch: 157, Loss:0.3220 Train: 0.9250, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:28,535]: Epoch: 158, Loss:0.3586 Train: 0.9583, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:28,542]: Epoch: 159, Loss:0.3205 Train: 0.9583, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:28,549]: Epoch: 160, Loss:0.3435 Train: 0.9167, Val:0.3375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:28,556]: Epoch: 161, Loss:0.3124 Train: 0.9083, Val:0.3375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:28,564]: Epoch: 162, Loss:0.3439 Train: 0.9083, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:28,570]: Epoch: 163, Loss:0.3396 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:44:28,578]: Epoch: 164, Loss:0.4311 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:28,585]: Epoch: 165, Loss:0.2647 Train: 0.9167, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:28,597]: Epoch: 166, Loss:0.3094 Train: 0.9250, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0114
[2025-04-01 02:44:28,604]: Epoch: 167, Loss:0.3305 Train: 0.9417, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:28,612]: Epoch: 168, Loss:0.2852 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:28,618]: Epoch: 169, Loss:0.2953 Train: 0.9000, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:28,625]: Epoch: 170, Loss:0.3287 Train: 0.9417, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:28,632]: Epoch: 171, Loss:0.3046 Train: 0.9417, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:28,641]: Epoch: 172, Loss:0.3366 Train: 0.9250, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:28,647]: Epoch: 173, Loss:0.2855 Train: 0.9250, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:28,655]: Epoch: 174, Loss:0.2897 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:28,662]: Epoch: 175, Loss:0.2875 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:28,669]: Epoch: 176, Loss:0.2852 Train: 0.9333, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:28,678]: Epoch: 177, Loss:0.3837 Train: 0.9417, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:28,685]: Epoch: 178, Loss:0.3350 Train: 0.9167, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:28,692]: Epoch: 179, Loss:0.3023 Train: 0.8833, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:28,699]: Epoch: 180, Loss:0.3371 Train: 0.8833, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:28,706]: Epoch: 181, Loss:0.3440 Train: 0.9417, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:28,715]: Epoch: 182, Loss:0.2902 Train: 0.9083, Val:0.3500, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:28,723]: Epoch: 183, Loss:0.4306 Train: 0.9250, Val:0.3375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:28,730]: Epoch: 184, Loss:0.3787 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:28,738]: Epoch: 185, Loss:0.3448 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:28,745]: Epoch: 186, Loss:0.3261 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:28,753]: Epoch: 187, Loss:0.2826 Train: 0.9083, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:28,760]: Epoch: 188, Loss:0.3283 Train: 0.9250, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:28,767]: Epoch: 189, Loss:0.3184 Train: 0.9250, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:28,776]: Epoch: 190, Loss:0.3014 Train: 0.9167, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:28,783]: Epoch: 191, Loss:0.3348 Train: 0.9333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:28,790]: Epoch: 192, Loss:0.3137 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:28,796]: Epoch: 193, Loss:0.3031 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:28,803]: Epoch: 194, Loss:0.3122 Train: 0.9417, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:28,810]: Epoch: 195, Loss:0.3245 Train: 0.8833, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:28,818]: Epoch: 196, Loss:0.3290 Train: 0.8917, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:28,826]: Epoch: 197, Loss:0.3553 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:28,834]: Epoch: 198, Loss:0.3115 Train: 0.8917, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:28,841]: Epoch: 199, Loss:0.3547 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:28,850]: Epoch: 200, Loss:0.3757 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0089
[2025-04-01 02:44:28,850]: [Run-1 score] {'train': 0.7, 'val': 0.525, 'test': 0.47058823529411764}
[2025-04-01 02:44:28,850]: repeat 2/3
[2025-04-01 02:44:28,851]: Manual random seed:0
[2025-04-01 02:44:28,851]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:28,854]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:28,866]: Epoch: 001, Loss:1.6183 Train: 0.4750, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0087
[2025-04-01 02:44:28,874]: Epoch: 002, Loss:1.7842 Train: 0.4250, Val:0.2750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:28,881]: Epoch: 003, Loss:1.4100 Train: 0.6333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:28,889]: Epoch: 004, Loss:1.0593 Train: 0.6750, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:28,899]: Epoch: 005, Loss:0.9557 Train: 0.6583, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0092
[2025-04-01 02:44:28,906]: Epoch: 006, Loss:1.1204 Train: 0.6167, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:28,913]: Epoch: 007, Loss:1.0259 Train: 0.6333, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:28,922]: Epoch: 008, Loss:0.9705 Train: 0.6583, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:28,931]: Epoch: 009, Loss:0.9184 Train: 0.7250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0092
[2025-04-01 02:44:28,940]: Epoch: 010, Loss:0.9160 Train: 0.7417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:28,948]: Epoch: 011, Loss:0.8197 Train: 0.7167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:28,956]: Epoch: 012, Loss:0.8032 Train: 0.7333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:28,964]: Epoch: 013, Loss:0.8510 Train: 0.7500, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:28,972]: Epoch: 014, Loss:0.7498 Train: 0.7500, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:28,978]: Epoch: 015, Loss:0.7378 Train: 0.7500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:28,985]: Epoch: 016, Loss:0.7242 Train: 0.7667, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:28,992]: Epoch: 017, Loss:0.7743 Train: 0.7667, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:28,999]: Epoch: 018, Loss:0.7124 Train: 0.7750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:29,006]: Epoch: 019, Loss:0.6572 Train: 0.7833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:29,013]: Epoch: 020, Loss:0.7048 Train: 0.8000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:29,019]: Epoch: 021, Loss:0.6006 Train: 0.8167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:29,026]: Epoch: 022, Loss:0.5539 Train: 0.8083, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:29,034]: Epoch: 023, Loss:0.6231 Train: 0.8000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:29,041]: Epoch: 024, Loss:0.5158 Train: 0.8333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:29,048]: Epoch: 025, Loss:0.6141 Train: 0.8250, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:29,056]: Epoch: 026, Loss:0.5272 Train: 0.8250, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:29,064]: Epoch: 027, Loss:0.5120 Train: 0.8417, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:29,070]: Epoch: 028, Loss:0.5028 Train: 0.8333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:29,077]: Epoch: 029, Loss:0.4974 Train: 0.8417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:29,084]: Epoch: 030, Loss:0.4820 Train: 0.8500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:29,091]: Epoch: 031, Loss:0.4732 Train: 0.8667, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:29,103]: Epoch: 032, Loss:0.4995 Train: 0.8750, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0112
[2025-04-01 02:44:29,111]: Epoch: 033, Loss:0.4997 Train: 0.8500, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:29,119]: Epoch: 034, Loss:0.4394 Train: 0.8583, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:29,127]: Epoch: 035, Loss:0.4892 Train: 0.8583, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:29,135]: Epoch: 036, Loss:0.4473 Train: 0.8500, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:29,141]: Epoch: 037, Loss:0.4950 Train: 0.8583, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:29,148]: Epoch: 038, Loss:0.4652 Train: 0.8667, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:29,155]: Epoch: 039, Loss:0.4878 Train: 0.8583, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:29,162]: Epoch: 040, Loss:0.4357 Train: 0.8583, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:29,170]: Epoch: 041, Loss:0.4247 Train: 0.8500, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:29,178]: Epoch: 042, Loss:0.4278 Train: 0.8500, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:29,186]: Epoch: 043, Loss:0.4482 Train: 0.8500, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:29,194]: Epoch: 044, Loss:0.4647 Train: 0.8500, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:29,202]: Epoch: 045, Loss:0.4338 Train: 0.8500, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:29,209]: Epoch: 046, Loss:0.4162 Train: 0.8667, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:29,216]: Epoch: 047, Loss:0.4359 Train: 0.8667, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:29,223]: Epoch: 048, Loss:0.4310 Train: 0.8667, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:29,230]: Epoch: 049, Loss:0.3968 Train: 0.8583, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:29,238]: Epoch: 050, Loss:0.4261 Train: 0.8583, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:29,245]: Epoch: 051, Loss:0.4448 Train: 0.8667, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:29,251]: Epoch: 052, Loss:0.3910 Train: 0.9167, Val:0.3500, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:29,260]: Epoch: 053, Loss:0.4343 Train: 0.9000, Val:0.3375, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:29,267]: Epoch: 054, Loss:0.4034 Train: 0.8833, Val:0.3250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:29,276]: Epoch: 055, Loss:0.4052 Train: 0.8583, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:29,284]: Epoch: 056, Loss:0.4132 Train: 0.8667, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:29,291]: Epoch: 057, Loss:0.4350 Train: 0.8833, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:29,299]: Epoch: 058, Loss:0.3583 Train: 0.9000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:29,307]: Epoch: 059, Loss:0.3654 Train: 0.8917, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:29,315]: Epoch: 060, Loss:0.3936 Train: 0.9000, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:29,323]: Epoch: 061, Loss:0.3810 Train: 0.8917, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:29,329]: Epoch: 062, Loss:0.4138 Train: 0.8833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:44:29,336]: Epoch: 063, Loss:0.3525 Train: 0.8917, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:29,343]: Epoch: 064, Loss:0.3583 Train: 0.8750, Val:0.3375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:29,351]: Epoch: 065, Loss:0.3721 Train: 0.8667, Val:0.3125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:29,359]: Epoch: 066, Loss:0.4850 Train: 0.9000, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:29,368]: Epoch: 067, Loss:0.4095 Train: 0.8500, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:29,376]: Epoch: 068, Loss:0.4344 Train: 0.8583, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:29,384]: Epoch: 069, Loss:0.3930 Train: 0.8833, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:29,392]: Epoch: 070, Loss:0.3705 Train: 0.8750, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:29,400]: Epoch: 071, Loss:0.4238 Train: 0.8750, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:29,408]: Epoch: 072, Loss:0.4147 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:29,415]: Epoch: 073, Loss:0.3663 Train: 0.8667, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:29,424]: Epoch: 074, Loss:0.3722 Train: 0.8583, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:29,431]: Epoch: 075, Loss:0.4410 Train: 0.8667, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:29,440]: Epoch: 076, Loss:0.4263 Train: 0.9000, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:29,447]: Epoch: 077, Loss:0.3836 Train: 0.8917, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:29,455]: Epoch: 078, Loss:0.4465 Train: 0.8917, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:29,463]: Epoch: 079, Loss:0.4619 Train: 0.9083, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:29,470]: Epoch: 080, Loss:0.4210 Train: 0.8583, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:29,477]: Epoch: 081, Loss:0.3664 Train: 0.8500, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:29,484]: Epoch: 082, Loss:0.4577 Train: 0.8583, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:29,491]: Epoch: 083, Loss:0.4453 Train: 0.8833, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:29,499]: Epoch: 084, Loss:0.3728 Train: 0.9083, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:29,506]: Epoch: 085, Loss:0.3490 Train: 0.8833, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:29,514]: Epoch: 086, Loss:0.4598 Train: 0.8917, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:29,521]: Epoch: 087, Loss:0.3580 Train: 0.9000, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:29,528]: Epoch: 088, Loss:0.3158 Train: 0.8583, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:29,535]: Epoch: 089, Loss:0.4241 Train: 0.8833, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:29,541]: Epoch: 090, Loss:0.4103 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:29,550]: Epoch: 091, Loss:0.3263 Train: 0.8833, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:29,556]: Epoch: 092, Loss:0.4333 Train: 0.8917, Val:0.3250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:29,564]: Epoch: 093, Loss:0.3593 Train: 0.9333, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:29,571]: Epoch: 094, Loss:0.3653 Train: 0.9250, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:29,578]: Epoch: 095, Loss:0.3785 Train: 0.9000, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:29,584]: Epoch: 096, Loss:0.3627 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:29,592]: Epoch: 097, Loss:0.3630 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:29,599]: Epoch: 098, Loss:0.3684 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:29,608]: Epoch: 099, Loss:0.3760 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:29,614]: Epoch: 100, Loss:0.3910 Train: 0.9000, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:29,621]: Epoch: 101, Loss:0.4092 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:29,630]: Epoch: 102, Loss:0.3458 Train: 0.8583, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0090
[2025-04-01 02:44:29,639]: Epoch: 103, Loss:0.4180 Train: 0.8833, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:29,646]: Epoch: 104, Loss:0.3787 Train: 0.9250, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:29,654]: Epoch: 105, Loss:0.3411 Train: 0.9083, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:29,661]: Epoch: 106, Loss:0.3421 Train: 0.8917, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:29,667]: Epoch: 107, Loss:0.3894 Train: 0.8833, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:29,675]: Epoch: 108, Loss:0.3682 Train: 0.8750, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:29,683]: Epoch: 109, Loss:0.3547 Train: 0.8917, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:29,690]: Epoch: 110, Loss:0.3399 Train: 0.9083, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:29,698]: Epoch: 111, Loss:0.3903 Train: 0.9000, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:29,705]: Epoch: 112, Loss:0.3600 Train: 0.9167, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:29,711]: Epoch: 113, Loss:0.2960 Train: 0.9000, Val:0.3500, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:29,719]: Epoch: 114, Loss:0.3027 Train: 0.9000, Val:0.3500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:29,726]: Epoch: 115, Loss:0.3762 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:29,733]: Epoch: 116, Loss:0.3213 Train: 0.9333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:29,739]: Epoch: 117, Loss:0.2919 Train: 0.9083, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:29,747]: Epoch: 118, Loss:0.3482 Train: 0.8583, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:29,753]: Epoch: 119, Loss:0.4138 Train: 0.8833, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:29,761]: Epoch: 120, Loss:0.4511 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:29,768]: Epoch: 121, Loss:0.3303 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:29,775]: Epoch: 122, Loss:0.3668 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:29,782]: Epoch: 123, Loss:0.3801 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:29,791]: Epoch: 124, Loss:0.3833 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:29,798]: Epoch: 125, Loss:0.3866 Train: 0.9333, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:29,806]: Epoch: 126, Loss:0.3794 Train: 0.9167, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:29,815]: Epoch: 127, Loss:0.4110 Train: 0.9250, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:29,822]: Epoch: 128, Loss:0.3577 Train: 0.9167, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:29,830]: Epoch: 129, Loss:0.3916 Train: 0.9250, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:29,838]: Epoch: 130, Loss:0.3790 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:29,846]: Epoch: 131, Loss:0.3497 Train: 0.8667, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:29,854]: Epoch: 132, Loss:0.4012 Train: 0.8833, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:29,861]: Epoch: 133, Loss:0.2926 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:29,870]: Epoch: 134, Loss:0.3316 Train: 0.9083, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:29,878]: Epoch: 135, Loss:0.3304 Train: 0.8833, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:29,884]: Epoch: 136, Loss:0.4386 Train: 0.9083, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:29,892]: Epoch: 137, Loss:0.3175 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:29,899]: Epoch: 138, Loss:0.3449 Train: 0.9167, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:29,907]: Epoch: 139, Loss:0.3182 Train: 0.8917, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:29,915]: Epoch: 140, Loss:0.3215 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:29,923]: Epoch: 141, Loss:0.3428 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:29,932]: Epoch: 142, Loss:0.3354 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:29,940]: Epoch: 143, Loss:0.3523 Train: 0.8917, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:29,949]: Epoch: 144, Loss:0.3295 Train: 0.9000, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:29,957]: Epoch: 145, Loss:0.3234 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:29,964]: Epoch: 146, Loss:0.3644 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:29,980]: Epoch: 147, Loss:0.3209 Train: 0.8500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0164
[2025-04-01 02:44:29,988]: Epoch: 148, Loss:0.4468 Train: 0.8750, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:29,995]: Epoch: 149, Loss:0.3876 Train: 0.9250, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:30,002]: Epoch: 150, Loss:0.3415 Train: 0.8917, Val:0.3000, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:30,009]: Epoch: 151, Loss:0.4874 Train: 0.8917, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:30,016]: Epoch: 152, Loss:0.3463 Train: 0.9000, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:30,023]: Epoch: 153, Loss:0.3327 Train: 0.8833, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:30,030]: Epoch: 154, Loss:0.3818 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:30,038]: Epoch: 155, Loss:0.3144 Train: 0.8833, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:30,046]: Epoch: 156, Loss:0.3101 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:30,054]: Epoch: 157, Loss:0.3478 Train: 0.9333, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:30,062]: Epoch: 158, Loss:0.3127 Train: 0.9333, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:30,070]: Epoch: 159, Loss:0.3425 Train: 0.9417, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:30,076]: Epoch: 160, Loss:0.4263 Train: 0.9417, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:30,084]: Epoch: 161, Loss:0.3615 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:30,090]: Epoch: 162, Loss:0.2992 Train: 0.9000, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:30,099]: Epoch: 163, Loss:0.3262 Train: 0.9250, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:30,105]: Epoch: 164, Loss:0.3448 Train: 0.9167, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:30,112]: Epoch: 165, Loss:0.4280 Train: 0.9083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:30,120]: Epoch: 166, Loss:0.3406 Train: 0.8917, Val:0.3250, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:30,126]: Epoch: 167, Loss:0.3675 Train: 0.9167, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:30,135]: Epoch: 168, Loss:0.3042 Train: 0.9250, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:30,142]: Epoch: 169, Loss:0.2548 Train: 0.8917, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:30,149]: Epoch: 170, Loss:0.3150 Train: 0.9000, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:30,157]: Epoch: 171, Loss:0.3510 Train: 0.9250, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:30,164]: Epoch: 172, Loss:0.3086 Train: 0.8833, Val:0.3000, Test: 0.3725, Time(s/epoch):0.0064
[2025-04-01 02:44:30,171]: Epoch: 173, Loss:0.3612 Train: 0.9000, Val:0.3000, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:30,178]: Epoch: 174, Loss:0.3227 Train: 0.9333, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:30,184]: Epoch: 175, Loss:0.3720 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:30,193]: Epoch: 176, Loss:0.3117 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:30,200]: Epoch: 177, Loss:0.3545 Train: 0.9000, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:30,208]: Epoch: 178, Loss:0.3547 Train: 0.9000, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:30,216]: Epoch: 179, Loss:0.4048 Train: 0.9250, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:30,224]: Epoch: 180, Loss:0.3020 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:30,231]: Epoch: 181, Loss:0.3942 Train: 0.9000, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:30,238]: Epoch: 182, Loss:0.3549 Train: 0.9000, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:30,246]: Epoch: 183, Loss:0.3870 Train: 0.9333, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:30,256]: Epoch: 184, Loss:0.3320 Train: 0.9333, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:30,262]: Epoch: 185, Loss:0.3277 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:30,271]: Epoch: 186, Loss:0.2913 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:30,279]: Epoch: 187, Loss:0.3403 Train: 0.9167, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:30,288]: Epoch: 188, Loss:0.2749 Train: 0.9333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:30,297]: Epoch: 189, Loss:0.3084 Train: 0.9250, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0087
[2025-04-01 02:44:30,306]: Epoch: 190, Loss:0.3286 Train: 0.9167, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:30,314]: Epoch: 191, Loss:0.3472 Train: 0.9250, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:30,320]: Epoch: 192, Loss:0.3200 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:30,328]: Epoch: 193, Loss:0.3163 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:30,336]: Epoch: 194, Loss:0.3126 Train: 0.9583, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:30,344]: Epoch: 195, Loss:0.3712 Train: 0.9250, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:30,351]: Epoch: 196, Loss:0.3248 Train: 0.9083, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:30,359]: Epoch: 197, Loss:0.3504 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:30,367]: Epoch: 198, Loss:0.2810 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:30,376]: Epoch: 199, Loss:0.3169 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:30,384]: Epoch: 200, Loss:0.3877 Train: 0.9000, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:30,385]: [Run-2 score] {'train': 0.475, 'val': 0.5125, 'test': 0.39215686274509803}
[2025-04-01 02:44:30,385]: repeat 3/3
[2025-04-01 02:44:30,385]: Manual random seed:0
[2025-04-01 02:44:30,385]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:30,388]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:30,399]: Epoch: 001, Loss:1.7091 Train: 0.5250, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:30,407]: Epoch: 002, Loss:1.2973 Train: 0.6583, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:30,415]: Epoch: 003, Loss:1.1217 Train: 0.6500, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:30,422]: Epoch: 004, Loss:1.0587 Train: 0.5750, Val:0.2750, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:30,430]: Epoch: 005, Loss:1.1507 Train: 0.7500, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:30,437]: Epoch: 006, Loss:0.7918 Train: 0.7167, Val:0.4875, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:30,444]: Epoch: 007, Loss:0.8493 Train: 0.7000, Val:0.4875, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:30,452]: Epoch: 008, Loss:0.8781 Train: 0.7417, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:30,466]: Epoch: 009, Loss:0.7850 Train: 0.7417, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0138
[2025-04-01 02:44:30,476]: Epoch: 010, Loss:0.7535 Train: 0.7583, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0100
[2025-04-01 02:44:30,486]: Epoch: 011, Loss:0.7775 Train: 0.7250, Val:0.3250, Test: 0.4118, Time(s/epoch):0.0095
[2025-04-01 02:44:30,493]: Epoch: 012, Loss:0.8469 Train: 0.7833, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:30,500]: Epoch: 013, Loss:0.8964 Train: 0.7917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:30,507]: Epoch: 014, Loss:0.6928 Train: 0.8000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:30,514]: Epoch: 015, Loss:0.6682 Train: 0.7833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:30,521]: Epoch: 016, Loss:0.6669 Train: 0.7667, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:30,529]: Epoch: 017, Loss:0.6963 Train: 0.7833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:30,537]: Epoch: 018, Loss:0.7249 Train: 0.7917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:30,543]: Epoch: 019, Loss:0.5734 Train: 0.8167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0057
[2025-04-01 02:44:30,549]: Epoch: 020, Loss:0.5849 Train: 0.8333, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:30,556]: Epoch: 021, Loss:0.6106 Train: 0.8333, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:30,564]: Epoch: 022, Loss:0.5892 Train: 0.8333, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:30,571]: Epoch: 023, Loss:0.5633 Train: 0.8083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:30,577]: Epoch: 024, Loss:0.5211 Train: 0.8417, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:30,585]: Epoch: 025, Loss:0.5502 Train: 0.8333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:30,592]: Epoch: 026, Loss:0.4842 Train: 0.8167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:30,600]: Epoch: 027, Loss:0.5442 Train: 0.8167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:30,606]: Epoch: 028, Loss:0.5339 Train: 0.8167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:30,614]: Epoch: 029, Loss:0.4653 Train: 0.8250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:30,622]: Epoch: 030, Loss:0.4864 Train: 0.8333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:30,630]: Epoch: 031, Loss:0.4673 Train: 0.8500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:30,637]: Epoch: 032, Loss:0.4456 Train: 0.8250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:30,644]: Epoch: 033, Loss:0.4707 Train: 0.8500, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:30,650]: Epoch: 034, Loss:0.5537 Train: 0.8500, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:30,658]: Epoch: 035, Loss:0.5099 Train: 0.8667, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:30,665]: Epoch: 036, Loss:0.4413 Train: 0.8667, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:30,672]: Epoch: 037, Loss:0.4330 Train: 0.8667, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:30,681]: Epoch: 038, Loss:0.4661 Train: 0.8750, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:30,688]: Epoch: 039, Loss:0.4610 Train: 0.8750, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:30,696]: Epoch: 040, Loss:0.4483 Train: 0.8417, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:30,703]: Epoch: 041, Loss:0.4968 Train: 0.8583, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:30,710]: Epoch: 042, Loss:0.4007 Train: 0.8500, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:30,718]: Epoch: 043, Loss:0.4160 Train: 0.8583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:30,726]: Epoch: 044, Loss:0.4236 Train: 0.8583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:30,734]: Epoch: 045, Loss:0.4168 Train: 0.8750, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:30,741]: Epoch: 046, Loss:0.4304 Train: 0.8833, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:30,748]: Epoch: 047, Loss:0.3931 Train: 0.8917, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:30,755]: Epoch: 048, Loss:0.4523 Train: 0.8750, Val:0.3500, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:44:30,763]: Epoch: 049, Loss:0.4369 Train: 0.8833, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:30,771]: Epoch: 050, Loss:0.4457 Train: 0.8833, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:30,779]: Epoch: 051, Loss:0.4324 Train: 0.8583, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:30,787]: Epoch: 052, Loss:0.3990 Train: 0.8750, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:30,793]: Epoch: 053, Loss:0.4515 Train: 0.8750, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:30,799]: Epoch: 054, Loss:0.3734 Train: 0.8833, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:30,807]: Epoch: 055, Loss:0.4303 Train: 0.8917, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:30,814]: Epoch: 056, Loss:0.4044 Train: 0.8917, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:30,821]: Epoch: 057, Loss:0.4339 Train: 0.8750, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:30,828]: Epoch: 058, Loss:0.3702 Train: 0.8750, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:30,835]: Epoch: 059, Loss:0.4332 Train: 0.8583, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:30,841]: Epoch: 060, Loss:0.4110 Train: 0.8667, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:30,849]: Epoch: 061, Loss:0.3854 Train: 0.8750, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:30,856]: Epoch: 062, Loss:0.3689 Train: 0.8833, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:30,863]: Epoch: 063, Loss:0.4404 Train: 0.8917, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:30,870]: Epoch: 064, Loss:0.4097 Train: 0.9167, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:30,879]: Epoch: 065, Loss:0.3927 Train: 0.8750, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:30,887]: Epoch: 066, Loss:0.3845 Train: 0.8333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:30,895]: Epoch: 067, Loss:0.4622 Train: 0.8417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:30,904]: Epoch: 068, Loss:0.4757 Train: 0.8750, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:30,911]: Epoch: 069, Loss:0.4009 Train: 0.8667, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:30,919]: Epoch: 070, Loss:0.4210 Train: 0.8333, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:30,927]: Epoch: 071, Loss:0.5224 Train: 0.8917, Val:0.3375, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:30,936]: Epoch: 072, Loss:0.4274 Train: 0.8583, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0085
[2025-04-01 02:44:30,945]: Epoch: 073, Loss:0.3690 Train: 0.8667, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:30,953]: Epoch: 074, Loss:0.3831 Train: 0.8667, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:30,963]: Epoch: 075, Loss:0.4646 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0091
[2025-04-01 02:44:30,971]: Epoch: 076, Loss:0.3983 Train: 0.9000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:30,981]: Epoch: 077, Loss:0.3752 Train: 0.9083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:30,989]: Epoch: 078, Loss:0.3967 Train: 0.8917, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:30,998]: Epoch: 079, Loss:0.4399 Train: 0.8583, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:31,006]: Epoch: 080, Loss:0.4220 Train: 0.8667, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:31,015]: Epoch: 081, Loss:0.4167 Train: 0.8833, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:31,023]: Epoch: 082, Loss:0.3257 Train: 0.9083, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:31,031]: Epoch: 083, Loss:0.3463 Train: 0.9000, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:31,040]: Epoch: 084, Loss:0.3306 Train: 0.9000, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:31,048]: Epoch: 085, Loss:0.3417 Train: 0.9083, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:31,055]: Epoch: 086, Loss:0.3505 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:31,062]: Epoch: 087, Loss:0.3222 Train: 0.8833, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:31,069]: Epoch: 088, Loss:0.3446 Train: 0.8917, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:31,076]: Epoch: 089, Loss:0.3710 Train: 0.8917, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:31,082]: Epoch: 090, Loss:0.3271 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:31,089]: Epoch: 091, Loss:0.3490 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:31,097]: Epoch: 092, Loss:0.3412 Train: 0.9250, Val:0.3500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:31,106]: Epoch: 093, Loss:0.3429 Train: 0.9167, Val:0.3625, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:31,113]: Epoch: 094, Loss:0.3179 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:31,120]: Epoch: 095, Loss:0.3161 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:31,128]: Epoch: 096, Loss:0.3959 Train: 0.9000, Val:0.3375, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:31,136]: Epoch: 097, Loss:0.3417 Train: 0.9083, Val:0.3500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:31,144]: Epoch: 098, Loss:0.3542 Train: 0.9250, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:31,151]: Epoch: 099, Loss:0.3912 Train: 0.9250, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:31,158]: Epoch: 100, Loss:0.3333 Train: 0.9083, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:31,165]: Epoch: 101, Loss:0.3358 Train: 0.9000, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:31,172]: Epoch: 102, Loss:0.3508 Train: 0.8833, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:31,180]: Epoch: 103, Loss:0.3642 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:31,188]: Epoch: 104, Loss:0.3918 Train: 0.8833, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:31,195]: Epoch: 105, Loss:0.3356 Train: 0.8917, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:31,204]: Epoch: 106, Loss:0.3560 Train: 0.9167, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:31,211]: Epoch: 107, Loss:0.3627 Train: 0.9500, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:31,219]: Epoch: 108, Loss:0.3500 Train: 0.9083, Val:0.3125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:31,227]: Epoch: 109, Loss:0.3143 Train: 0.8750, Val:0.3250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:31,235]: Epoch: 110, Loss:0.3696 Train: 0.9000, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:31,243]: Epoch: 111, Loss:0.4019 Train: 0.8833, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:31,251]: Epoch: 112, Loss:0.3834 Train: 0.8750, Val:0.3750, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:31,258]: Epoch: 113, Loss:0.4588 Train: 0.9167, Val:0.3500, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:31,267]: Epoch: 114, Loss:0.3812 Train: 0.8750, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:31,274]: Epoch: 115, Loss:0.3186 Train: 0.8833, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:31,282]: Epoch: 116, Loss:0.5373 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:31,291]: Epoch: 117, Loss:0.3385 Train: 0.8833, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:31,301]: Epoch: 118, Loss:0.4449 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0097
[2025-04-01 02:44:31,308]: Epoch: 119, Loss:0.3417 Train: 0.8667, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:31,316]: Epoch: 120, Loss:0.3954 Train: 0.8833, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:31,323]: Epoch: 121, Loss:0.4794 Train: 0.8833, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:31,330]: Epoch: 122, Loss:0.4227 Train: 0.8917, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:31,337]: Epoch: 123, Loss:0.4068 Train: 0.8750, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:31,344]: Epoch: 124, Loss:0.3511 Train: 0.8583, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:31,351]: Epoch: 125, Loss:0.3513 Train: 0.8667, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:31,359]: Epoch: 126, Loss:0.4167 Train: 0.9250, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:31,366]: Epoch: 127, Loss:0.3463 Train: 0.8417, Val:0.3000, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:31,374]: Epoch: 128, Loss:0.6790 Train: 0.9333, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:31,382]: Epoch: 129, Loss:0.3448 Train: 0.9167, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:31,390]: Epoch: 130, Loss:0.3186 Train: 0.8750, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:31,398]: Epoch: 131, Loss:0.3981 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:31,406]: Epoch: 132, Loss:0.3470 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:31,412]: Epoch: 133, Loss:0.3366 Train: 0.9333, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:31,419]: Epoch: 134, Loss:0.3491 Train: 0.9333, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:31,427]: Epoch: 135, Loss:0.3121 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:31,435]: Epoch: 136, Loss:0.3646 Train: 0.9333, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:31,442]: Epoch: 137, Loss:0.3217 Train: 0.9333, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:31,449]: Epoch: 138, Loss:0.3369 Train: 0.9333, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:31,456]: Epoch: 139, Loss:0.3544 Train: 0.9000, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:31,465]: Epoch: 140, Loss:0.3200 Train: 0.9000, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:31,473]: Epoch: 141, Loss:0.3584 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:31,481]: Epoch: 142, Loss:0.2854 Train: 0.9333, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:31,488]: Epoch: 143, Loss:0.3622 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:31,497]: Epoch: 144, Loss:0.3819 Train: 0.9000, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:31,506]: Epoch: 145, Loss:0.3343 Train: 0.9250, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:31,514]: Epoch: 146, Loss:0.3408 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:31,522]: Epoch: 147, Loss:0.3375 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:31,530]: Epoch: 148, Loss:0.4262 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:31,539]: Epoch: 149, Loss:0.3611 Train: 0.9167, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:31,547]: Epoch: 150, Loss:0.3269 Train: 0.9333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:31,555]: Epoch: 151, Loss:0.3147 Train: 0.9250, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:31,562]: Epoch: 152, Loss:0.3196 Train: 0.9167, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:31,570]: Epoch: 153, Loss:0.3615 Train: 0.9333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:31,576]: Epoch: 154, Loss:0.3501 Train: 0.9000, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:31,583]: Epoch: 155, Loss:0.3920 Train: 0.9000, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:31,591]: Epoch: 156, Loss:0.3246 Train: 0.9333, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:31,599]: Epoch: 157, Loss:0.3114 Train: 0.9417, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:31,606]: Epoch: 158, Loss:0.3067 Train: 0.9250, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:31,613]: Epoch: 159, Loss:0.2958 Train: 0.9083, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:31,620]: Epoch: 160, Loss:0.3556 Train: 0.9333, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:31,627]: Epoch: 161, Loss:0.3629 Train: 0.9500, Val:0.3625, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:31,636]: Epoch: 162, Loss:0.3177 Train: 0.9250, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:31,643]: Epoch: 163, Loss:0.3558 Train: 0.9000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:31,650]: Epoch: 164, Loss:0.3364 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:31,657]: Epoch: 165, Loss:0.3516 Train: 0.8833, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:31,664]: Epoch: 166, Loss:0.3935 Train: 0.9250, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:31,671]: Epoch: 167, Loss:0.4356 Train: 0.8833, Val:0.3250, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:31,678]: Epoch: 168, Loss:0.3842 Train: 0.8667, Val:0.3000, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:31,685]: Epoch: 169, Loss:0.4269 Train: 0.9250, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:31,693]: Epoch: 170, Loss:0.3371 Train: 0.9000, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:31,700]: Epoch: 171, Loss:0.3131 Train: 0.8750, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:31,707]: Epoch: 172, Loss:0.4596 Train: 0.8667, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:31,715]: Epoch: 173, Loss:0.4212 Train: 0.9000, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:31,723]: Epoch: 174, Loss:0.3129 Train: 0.9250, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:31,729]: Epoch: 175, Loss:0.3826 Train: 0.9083, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:31,736]: Epoch: 176, Loss:0.3441 Train: 0.9083, Val:0.3500, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:31,744]: Epoch: 177, Loss:0.4388 Train: 0.8917, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:31,750]: Epoch: 178, Loss:0.4878 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:31,758]: Epoch: 179, Loss:0.3307 Train: 0.9083, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:31,764]: Epoch: 180, Loss:0.3512 Train: 0.9167, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:31,771]: Epoch: 181, Loss:0.3531 Train: 0.9333, Val:0.3375, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:31,778]: Epoch: 182, Loss:0.4220 Train: 0.9333, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:31,786]: Epoch: 183, Loss:0.3396 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:31,793]: Epoch: 184, Loss:0.3234 Train: 0.8750, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:31,801]: Epoch: 185, Loss:0.3316 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:31,809]: Epoch: 186, Loss:0.3029 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:31,816]: Epoch: 187, Loss:0.3584 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:31,823]: Epoch: 188, Loss:0.2880 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:31,830]: Epoch: 189, Loss:0.2883 Train: 0.8917, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:31,838]: Epoch: 190, Loss:0.3622 Train: 0.9250, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:31,845]: Epoch: 191, Loss:0.3607 Train: 0.9167, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:31,852]: Epoch: 192, Loss:0.3661 Train: 0.8833, Val:0.3375, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:31,859]: Epoch: 193, Loss:0.4143 Train: 0.9167, Val:0.3000, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:31,866]: Epoch: 194, Loss:0.4258 Train: 0.9333, Val:0.3625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:31,874]: Epoch: 195, Loss:0.3237 Train: 0.8917, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:31,883]: Epoch: 196, Loss:0.3235 Train: 0.8917, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:31,891]: Epoch: 197, Loss:0.3831 Train: 0.9083, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:31,898]: Epoch: 198, Loss:0.3595 Train: 0.9083, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:31,906]: Epoch: 199, Loss:0.4454 Train: 0.9417, Val:0.3500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:31,914]: Epoch: 200, Loss:0.2875 Train: 0.9167, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:31,915]: [Run-3 score] {'train': 0.525, 'val': 0.5, 'test': 0.39215686274509803}
[2025-04-01 02:44:31,915]: repeat 1/3
[2025-04-01 02:44:31,915]: Manual random seed:0
[2025-04-01 02:44:31,915]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:31,917]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:31,926]: Epoch: 001, Loss:1.6837 Train: 0.4333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:31,932]: Epoch: 002, Loss:2.2147 Train: 0.5917, Val:0.4750, Test: 0.5882, Time(s/epoch):0.0055
[2025-04-01 02:44:31,940]: Epoch: 003, Loss:1.2395 Train: 0.5333, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:31,948]: Epoch: 004, Loss:1.1312 Train: 0.6000, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:31,955]: Epoch: 005, Loss:1.0348 Train: 0.6333, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:31,963]: Epoch: 006, Loss:0.9832 Train: 0.6583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:31,969]: Epoch: 007, Loss:0.9248 Train: 0.6750, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0058
[2025-04-01 02:44:31,976]: Epoch: 008, Loss:0.8549 Train: 0.6833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:31,984]: Epoch: 009, Loss:0.8149 Train: 0.7167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:31,991]: Epoch: 010, Loss:0.7829 Train: 0.7000, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:31,999]: Epoch: 011, Loss:0.7634 Train: 0.7167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:32,007]: Epoch: 012, Loss:0.7303 Train: 0.7417, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:32,014]: Epoch: 013, Loss:0.7182 Train: 0.7583, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:32,022]: Epoch: 014, Loss:0.6920 Train: 0.7833, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:32,028]: Epoch: 015, Loss:0.6901 Train: 0.7750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:32,036]: Epoch: 016, Loss:0.6644 Train: 0.7750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:32,043]: Epoch: 017, Loss:0.6270 Train: 0.7750, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:32,051]: Epoch: 018, Loss:0.6478 Train: 0.7917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:32,057]: Epoch: 019, Loss:0.6611 Train: 0.8167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:32,065]: Epoch: 020, Loss:0.5866 Train: 0.8000, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:32,072]: Epoch: 021, Loss:0.5442 Train: 0.8000, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:32,079]: Epoch: 022, Loss:0.5865 Train: 0.8000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:32,086]: Epoch: 023, Loss:0.6050 Train: 0.8250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:32,093]: Epoch: 024, Loss:0.5927 Train: 0.8167, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:32,100]: Epoch: 025, Loss:0.5769 Train: 0.8583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:32,108]: Epoch: 026, Loss:0.5297 Train: 0.8500, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:32,115]: Epoch: 027, Loss:0.5677 Train: 0.8583, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:32,122]: Epoch: 028, Loss:0.5366 Train: 0.8583, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:32,129]: Epoch: 029, Loss:0.5872 Train: 0.8500, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:32,136]: Epoch: 030, Loss:0.4749 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:32,144]: Epoch: 031, Loss:0.5534 Train: 0.8667, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:32,151]: Epoch: 032, Loss:0.4768 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:32,159]: Epoch: 033, Loss:0.4966 Train: 0.8500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:32,167]: Epoch: 034, Loss:0.4733 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:32,174]: Epoch: 035, Loss:0.4985 Train: 0.8417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:32,182]: Epoch: 036, Loss:0.4442 Train: 0.8417, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:32,190]: Epoch: 037, Loss:0.4410 Train: 0.8583, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:32,199]: Epoch: 038, Loss:0.4226 Train: 0.8667, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:32,206]: Epoch: 039, Loss:0.4879 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:32,214]: Epoch: 040, Loss:0.4793 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:32,220]: Epoch: 041, Loss:0.5086 Train: 0.9083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:32,229]: Epoch: 042, Loss:0.4780 Train: 0.9083, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:32,237]: Epoch: 043, Loss:0.5312 Train: 0.8750, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:32,244]: Epoch: 044, Loss:0.4133 Train: 0.8583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:32,252]: Epoch: 045, Loss:0.5039 Train: 0.8750, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:32,260]: Epoch: 046, Loss:0.4553 Train: 0.8917, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:32,268]: Epoch: 047, Loss:0.4360 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:32,275]: Epoch: 048, Loss:0.4318 Train: 0.8750, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:32,282]: Epoch: 049, Loss:0.4096 Train: 0.9083, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:32,291]: Epoch: 050, Loss:0.4182 Train: 0.8917, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:32,300]: Epoch: 051, Loss:0.5287 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0089
[2025-04-01 02:44:32,309]: Epoch: 052, Loss:0.3989 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:32,317]: Epoch: 053, Loss:0.4314 Train: 0.8583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:32,325]: Epoch: 054, Loss:0.4357 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:32,331]: Epoch: 055, Loss:0.4252 Train: 0.9000, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:32,339]: Epoch: 056, Loss:0.4238 Train: 0.9083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:32,345]: Epoch: 057, Loss:0.3657 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:32,352]: Epoch: 058, Loss:0.3981 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:32,360]: Epoch: 059, Loss:0.4432 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:32,366]: Epoch: 060, Loss:0.3796 Train: 0.9000, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:32,374]: Epoch: 061, Loss:0.4222 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:32,382]: Epoch: 062, Loss:0.3845 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:32,390]: Epoch: 063, Loss:0.3713 Train: 0.9167, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:32,398]: Epoch: 064, Loss:0.4019 Train: 0.9167, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:32,406]: Epoch: 065, Loss:0.3830 Train: 0.9083, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0084
[2025-04-01 02:44:32,414]: Epoch: 066, Loss:0.3729 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:32,423]: Epoch: 067, Loss:0.3603 Train: 0.9167, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:32,431]: Epoch: 068, Loss:0.4227 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:32,437]: Epoch: 069, Loss:0.3640 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:32,444]: Epoch: 070, Loss:0.3818 Train: 0.9250, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:32,451]: Epoch: 071, Loss:0.3970 Train: 0.8833, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:32,458]: Epoch: 072, Loss:0.3564 Train: 0.9167, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:32,467]: Epoch: 073, Loss:0.4158 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:32,474]: Epoch: 074, Loss:0.3611 Train: 0.8667, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:32,480]: Epoch: 075, Loss:0.3327 Train: 0.8500, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:32,487]: Epoch: 076, Loss:0.4510 Train: 0.8750, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:32,495]: Epoch: 077, Loss:0.3746 Train: 0.8833, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:32,501]: Epoch: 078, Loss:0.4178 Train: 0.8917, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:32,508]: Epoch: 079, Loss:0.4246 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:32,515]: Epoch: 080, Loss:0.3790 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:32,522]: Epoch: 081, Loss:0.4167 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:32,529]: Epoch: 082, Loss:0.4417 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:32,536]: Epoch: 083, Loss:0.4370 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:32,543]: Epoch: 084, Loss:0.3650 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:32,551]: Epoch: 085, Loss:0.3846 Train: 0.9167, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:32,558]: Epoch: 086, Loss:0.3994 Train: 0.9083, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:32,564]: Epoch: 087, Loss:0.3999 Train: 0.9000, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:32,573]: Epoch: 088, Loss:0.3734 Train: 0.8833, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0081
[2025-04-01 02:44:32,580]: Epoch: 089, Loss:0.3672 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:32,587]: Epoch: 090, Loss:0.3971 Train: 0.9250, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:32,595]: Epoch: 091, Loss:0.3535 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:32,602]: Epoch: 092, Loss:0.4549 Train: 0.9000, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:32,609]: Epoch: 093, Loss:0.3501 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:32,616]: Epoch: 094, Loss:0.3492 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:32,624]: Epoch: 095, Loss:0.4366 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:32,631]: Epoch: 096, Loss:0.3963 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:32,639]: Epoch: 097, Loss:0.4330 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:32,648]: Epoch: 098, Loss:0.3890 Train: 0.8750, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:32,656]: Epoch: 099, Loss:0.4424 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:32,664]: Epoch: 100, Loss:0.3541 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:32,671]: Epoch: 101, Loss:0.4743 Train: 0.8917, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:32,679]: Epoch: 102, Loss:0.3860 Train: 0.8833, Val:0.3625, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:32,687]: Epoch: 103, Loss:0.4837 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:32,695]: Epoch: 104, Loss:0.5009 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:32,701]: Epoch: 105, Loss:0.4047 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:32,710]: Epoch: 106, Loss:0.3673 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:32,718]: Epoch: 107, Loss:0.3351 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:32,724]: Epoch: 108, Loss:0.3536 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:32,731]: Epoch: 109, Loss:0.3379 Train: 0.9083, Val:0.3375, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:32,738]: Epoch: 110, Loss:0.4374 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:32,745]: Epoch: 111, Loss:0.3183 Train: 0.8917, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:32,752]: Epoch: 112, Loss:0.4225 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:32,759]: Epoch: 113, Loss:0.3691 Train: 0.9000, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:32,765]: Epoch: 114, Loss:0.3331 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:32,772]: Epoch: 115, Loss:0.3619 Train: 0.8750, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:32,779]: Epoch: 116, Loss:0.3920 Train: 0.8833, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:32,787]: Epoch: 117, Loss:0.4013 Train: 0.9083, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:32,795]: Epoch: 118, Loss:0.3368 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:32,802]: Epoch: 119, Loss:0.3932 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:32,810]: Epoch: 120, Loss:0.3930 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:32,818]: Epoch: 121, Loss:0.3578 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:32,824]: Epoch: 122, Loss:0.3161 Train: 0.9417, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:32,832]: Epoch: 123, Loss:0.3098 Train: 0.9083, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:32,839]: Epoch: 124, Loss:0.3712 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:32,845]: Epoch: 125, Loss:0.3499 Train: 0.9333, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:32,852]: Epoch: 126, Loss:0.3108 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:32,860]: Epoch: 127, Loss:0.3148 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:32,868]: Epoch: 128, Loss:0.3104 Train: 0.8917, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:32,876]: Epoch: 129, Loss:0.3111 Train: 0.9333, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:32,884]: Epoch: 130, Loss:0.3866 Train: 0.8500, Val:0.3125, Test: 0.2941, Time(s/epoch):0.0081
[2025-04-01 02:44:32,890]: Epoch: 131, Loss:0.5505 Train: 0.9417, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:32,898]: Epoch: 132, Loss:0.3941 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:32,904]: Epoch: 133, Loss:0.3281 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:32,913]: Epoch: 134, Loss:0.3653 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:32,919]: Epoch: 135, Loss:0.4901 Train: 0.8917, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:32,926]: Epoch: 136, Loss:0.3841 Train: 0.8667, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:32,933]: Epoch: 137, Loss:0.3483 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:32,941]: Epoch: 138, Loss:0.4727 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:32,948]: Epoch: 139, Loss:0.3225 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:32,954]: Epoch: 140, Loss:0.3905 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:32,962]: Epoch: 141, Loss:0.3592 Train: 0.8750, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:32,969]: Epoch: 142, Loss:0.3343 Train: 0.9000, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0068
[2025-04-01 02:44:32,977]: Epoch: 143, Loss:0.3433 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:32,984]: Epoch: 144, Loss:0.3112 Train: 0.8750, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:32,991]: Epoch: 145, Loss:0.3682 Train: 0.8667, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:32,999]: Epoch: 146, Loss:0.3735 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:33,008]: Epoch: 147, Loss:0.3341 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:33,014]: Epoch: 148, Loss:0.3372 Train: 0.8750, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:33,022]: Epoch: 149, Loss:0.4050 Train: 0.8917, Val:0.3625, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:33,031]: Epoch: 150, Loss:0.3536 Train: 0.8750, Val:0.3500, Test: 0.3529, Time(s/epoch):0.0090
[2025-04-01 02:44:33,040]: Epoch: 151, Loss:0.3902 Train: 0.9000, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0083
[2025-04-01 02:44:33,049]: Epoch: 152, Loss:0.3509 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0090
[2025-04-01 02:44:33,058]: Epoch: 153, Loss:0.3453 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0090
[2025-04-01 02:44:33,068]: Epoch: 154, Loss:0.4015 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0100
[2025-04-01 02:44:33,077]: Epoch: 155, Loss:0.3162 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:33,087]: Epoch: 156, Loss:0.3957 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0098
[2025-04-01 02:44:33,098]: Epoch: 157, Loss:0.3175 Train: 0.9000, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0105
[2025-04-01 02:44:33,107]: Epoch: 158, Loss:0.3587 Train: 0.8667, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0087
[2025-04-01 02:44:33,116]: Epoch: 159, Loss:0.4225 Train: 0.9083, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0089
[2025-04-01 02:44:33,124]: Epoch: 160, Loss:0.3554 Train: 0.8917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:33,133]: Epoch: 161, Loss:0.3228 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0095
[2025-04-01 02:44:33,142]: Epoch: 162, Loss:0.4235 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:33,150]: Epoch: 163, Loss:0.3615 Train: 0.9333, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:33,158]: Epoch: 164, Loss:0.3196 Train: 0.9250, Val:0.3750, Test: 0.3137, Time(s/epoch):0.0080
[2025-04-01 02:44:33,166]: Epoch: 165, Loss:0.3394 Train: 0.9083, Val:0.3250, Test: 0.3333, Time(s/epoch):0.0078
[2025-04-01 02:44:33,173]: Epoch: 166, Loss:0.3689 Train: 0.9250, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0064
[2025-04-01 02:44:33,182]: Epoch: 167, Loss:0.4130 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:33,189]: Epoch: 168, Loss:0.3573 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:33,197]: Epoch: 169, Loss:0.3638 Train: 0.9333, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:33,206]: Epoch: 170, Loss:0.3949 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:33,212]: Epoch: 171, Loss:0.3801 Train: 0.9250, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:33,220]: Epoch: 172, Loss:0.3237 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:33,227]: Epoch: 173, Loss:0.3471 Train: 0.9083, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:33,236]: Epoch: 174, Loss:0.3272 Train: 0.9250, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0084
[2025-04-01 02:44:33,244]: Epoch: 175, Loss:0.3937 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:33,251]: Epoch: 176, Loss:0.3955 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:33,259]: Epoch: 177, Loss:0.3845 Train: 0.9167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0084
[2025-04-01 02:44:33,267]: Epoch: 178, Loss:0.4042 Train: 0.9167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0071
[2025-04-01 02:44:33,275]: Epoch: 179, Loss:0.3063 Train: 0.9417, Val:0.4000, Test: 0.3333, Time(s/epoch):0.0080
[2025-04-01 02:44:33,283]: Epoch: 180, Loss:0.3644 Train: 0.8917, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0077
[2025-04-01 02:44:33,292]: Epoch: 181, Loss:0.4015 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:33,301]: Epoch: 182, Loss:0.4037 Train: 0.9000, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0087
[2025-04-01 02:44:33,310]: Epoch: 183, Loss:0.3223 Train: 0.9000, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0086
[2025-04-01 02:44:33,318]: Epoch: 184, Loss:0.4318 Train: 0.9083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:33,326]: Epoch: 185, Loss:0.3280 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:33,334]: Epoch: 186, Loss:0.3063 Train: 0.9000, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:33,342]: Epoch: 187, Loss:0.3228 Train: 0.9167, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0077
[2025-04-01 02:44:33,349]: Epoch: 188, Loss:0.3581 Train: 0.9167, Val:0.4000, Test: 0.3333, Time(s/epoch):0.0069
[2025-04-01 02:44:33,356]: Epoch: 189, Loss:0.3116 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:33,365]: Epoch: 190, Loss:0.3318 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:33,381]: Epoch: 191, Loss:0.3429 Train: 0.9250, Val:0.4375, Test: 0.3529, Time(s/epoch):0.0165
[2025-04-01 02:44:33,393]: Epoch: 192, Loss:0.3190 Train: 0.9250, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0120
[2025-04-01 02:44:33,404]: Epoch: 193, Loss:0.2971 Train: 0.9333, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0106
[2025-04-01 02:44:33,412]: Epoch: 194, Loss:0.3204 Train: 0.9500, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:33,419]: Epoch: 195, Loss:0.2709 Train: 0.9250, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:33,428]: Epoch: 196, Loss:0.3139 Train: 0.9250, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0088
[2025-04-01 02:44:33,436]: Epoch: 197, Loss:0.3348 Train: 0.9333, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:33,445]: Epoch: 198, Loss:0.3122 Train: 0.9167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:33,454]: Epoch: 199, Loss:0.2784 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:33,462]: Epoch: 200, Loss:0.2912 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:33,462]: [Run-1 score] {'train': 0.6583333333333333, 'val': 0.5, 'test': 0.49019607843137253}
[2025-04-01 02:44:33,462]: repeat 2/3
[2025-04-01 02:44:33,462]: Manual random seed:0
[2025-04-01 02:44:33,463]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:33,466]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:33,478]: Epoch: 001, Loss:1.6088 Train: 0.4333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:44:33,486]: Epoch: 002, Loss:1.8578 Train: 0.5500, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:33,494]: Epoch: 003, Loss:1.0540 Train: 0.4000, Val:0.2375, Test: 0.1961, Time(s/epoch):0.0080
[2025-04-01 02:44:33,501]: Epoch: 004, Loss:1.1721 Train: 0.5833, Val:0.3875, Test: 0.2549, Time(s/epoch):0.0071
[2025-04-01 02:44:33,509]: Epoch: 005, Loss:1.0667 Train: 0.6333, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:33,516]: Epoch: 006, Loss:1.1806 Train: 0.6833, Val:0.4625, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:33,522]: Epoch: 007, Loss:0.8965 Train: 0.7000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:33,529]: Epoch: 008, Loss:0.8408 Train: 0.7083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:33,537]: Epoch: 009, Loss:1.1692 Train: 0.7083, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:33,546]: Epoch: 010, Loss:1.0844 Train: 0.7167, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:44:33,554]: Epoch: 011, Loss:1.1798 Train: 0.7167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:33,562]: Epoch: 012, Loss:0.7367 Train: 0.7417, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:33,571]: Epoch: 013, Loss:0.7122 Train: 0.7417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:33,578]: Epoch: 014, Loss:0.7743 Train: 0.7583, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:33,586]: Epoch: 015, Loss:0.7294 Train: 0.7667, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0080
[2025-04-01 02:44:33,594]: Epoch: 016, Loss:0.7392 Train: 0.6833, Val:0.2625, Test: 0.2549, Time(s/epoch):0.0078
[2025-04-01 02:44:33,603]: Epoch: 017, Loss:0.8755 Train: 0.6833, Val:0.2750, Test: 0.2549, Time(s/epoch):0.0090
[2025-04-01 02:44:33,611]: Epoch: 018, Loss:0.7949 Train: 0.7833, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:33,618]: Epoch: 019, Loss:0.6416 Train: 0.7333, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:33,626]: Epoch: 020, Loss:0.6907 Train: 0.7750, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:33,633]: Epoch: 021, Loss:0.6438 Train: 0.7583, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:33,640]: Epoch: 022, Loss:0.6239 Train: 0.7833, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:33,648]: Epoch: 023, Loss:0.5938 Train: 0.8000, Val:0.4875, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:33,656]: Epoch: 024, Loss:0.5868 Train: 0.8167, Val:0.4750, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:33,664]: Epoch: 025, Loss:0.5721 Train: 0.8000, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:33,670]: Epoch: 026, Loss:0.5982 Train: 0.8000, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:33,679]: Epoch: 027, Loss:0.5769 Train: 0.8000, Val:0.4500, Test: 0.3725, Time(s/epoch):0.0083
[2025-04-01 02:44:33,685]: Epoch: 028, Loss:0.5962 Train: 0.8250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:33,691]: Epoch: 029, Loss:0.5252 Train: 0.8500, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:33,699]: Epoch: 030, Loss:0.5794 Train: 0.8583, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:33,707]: Epoch: 031, Loss:0.5108 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:33,715]: Epoch: 032, Loss:0.5354 Train: 0.8000, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:33,722]: Epoch: 033, Loss:0.5432 Train: 0.7833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:33,730]: Epoch: 034, Loss:0.5394 Train: 0.7833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:33,736]: Epoch: 035, Loss:0.5301 Train: 0.8167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:33,745]: Epoch: 036, Loss:0.5431 Train: 0.8417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0091
[2025-04-01 02:44:33,751]: Epoch: 037, Loss:0.5115 Train: 0.8583, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:33,759]: Epoch: 038, Loss:0.4520 Train: 0.8667, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:33,766]: Epoch: 039, Loss:0.4797 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:33,774]: Epoch: 040, Loss:0.4491 Train: 0.8833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:33,781]: Epoch: 041, Loss:0.4336 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:33,789]: Epoch: 042, Loss:0.4878 Train: 0.8500, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:33,796]: Epoch: 043, Loss:0.4390 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:33,805]: Epoch: 044, Loss:0.4659 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:33,811]: Epoch: 045, Loss:0.4913 Train: 0.8583, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:33,819]: Epoch: 046, Loss:0.4894 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:33,825]: Epoch: 047, Loss:0.4707 Train: 0.8583, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:33,832]: Epoch: 048, Loss:0.5131 Train: 0.8583, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:33,840]: Epoch: 049, Loss:0.5278 Train: 0.8667, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:33,848]: Epoch: 050, Loss:0.4538 Train: 0.8500, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:33,854]: Epoch: 051, Loss:0.4744 Train: 0.8667, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:33,862]: Epoch: 052, Loss:0.5145 Train: 0.8583, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:33,869]: Epoch: 053, Loss:0.4299 Train: 0.8667, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:33,875]: Epoch: 054, Loss:0.4564 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:33,883]: Epoch: 055, Loss:0.4974 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:33,889]: Epoch: 056, Loss:0.5008 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:33,897]: Epoch: 057, Loss:0.4670 Train: 0.8750, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:33,906]: Epoch: 058, Loss:0.4080 Train: 0.8667, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:33,912]: Epoch: 059, Loss:0.4809 Train: 0.8583, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:33,920]: Epoch: 060, Loss:0.4414 Train: 0.8333, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:33,928]: Epoch: 061, Loss:0.4300 Train: 0.8500, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:33,935]: Epoch: 062, Loss:0.4065 Train: 0.8500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:33,942]: Epoch: 063, Loss:0.4858 Train: 0.8667, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:33,949]: Epoch: 064, Loss:0.4437 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:33,956]: Epoch: 065, Loss:0.4054 Train: 0.8917, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:33,965]: Epoch: 066, Loss:0.5229 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:33,972]: Epoch: 067, Loss:0.5035 Train: 0.8667, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:33,979]: Epoch: 068, Loss:0.4586 Train: 0.8667, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:33,985]: Epoch: 069, Loss:0.4943 Train: 0.8833, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:33,992]: Epoch: 070, Loss:0.4403 Train: 0.8667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:33,999]: Epoch: 071, Loss:0.4379 Train: 0.8667, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:34,006]: Epoch: 072, Loss:0.4076 Train: 0.8833, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:34,013]: Epoch: 073, Loss:0.4450 Train: 0.8833, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:34,020]: Epoch: 074, Loss:0.4362 Train: 0.8917, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:34,027]: Epoch: 075, Loss:0.4426 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:34,034]: Epoch: 076, Loss:0.4301 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,042]: Epoch: 077, Loss:0.4147 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:34,048]: Epoch: 078, Loss:0.4305 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:34,055]: Epoch: 079, Loss:0.4410 Train: 0.8667, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:34,061]: Epoch: 080, Loss:0.4365 Train: 0.8917, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:34,069]: Epoch: 081, Loss:0.4278 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:34,075]: Epoch: 082, Loss:0.4758 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:34,082]: Epoch: 083, Loss:0.4541 Train: 0.9000, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:34,089]: Epoch: 084, Loss:0.4274 Train: 0.8833, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:34,096]: Epoch: 085, Loss:0.4082 Train: 0.8583, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:34,104]: Epoch: 086, Loss:0.4319 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:34,111]: Epoch: 087, Loss:0.4840 Train: 0.9000, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:34,120]: Epoch: 088, Loss:0.4509 Train: 0.8833, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:34,126]: Epoch: 089, Loss:0.4854 Train: 0.9000, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:34,135]: Epoch: 090, Loss:0.4240 Train: 0.8917, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:34,143]: Epoch: 091, Loss:0.3688 Train: 0.8500, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:34,151]: Epoch: 092, Loss:0.4173 Train: 0.8750, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:34,158]: Epoch: 093, Loss:0.4154 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:34,165]: Epoch: 094, Loss:0.3668 Train: 0.8833, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:34,173]: Epoch: 095, Loss:0.4453 Train: 0.8583, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:34,179]: Epoch: 096, Loss:0.4261 Train: 0.8750, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0059
[2025-04-01 02:44:34,187]: Epoch: 097, Loss:0.3788 Train: 0.8833, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:34,193]: Epoch: 098, Loss:0.4215 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:34,200]: Epoch: 099, Loss:0.3627 Train: 0.8917, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:34,208]: Epoch: 100, Loss:0.3979 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:34,216]: Epoch: 101, Loss:0.3922 Train: 0.9000, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:34,224]: Epoch: 102, Loss:0.3363 Train: 0.8917, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:34,231]: Epoch: 103, Loss:0.3851 Train: 0.8917, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:34,238]: Epoch: 104, Loss:0.3353 Train: 0.8667, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:34,245]: Epoch: 105, Loss:0.4435 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,254]: Epoch: 106, Loss:0.3853 Train: 0.8750, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:34,260]: Epoch: 107, Loss:0.4077 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:34,268]: Epoch: 108, Loss:0.3663 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:34,276]: Epoch: 109, Loss:0.3969 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:34,282]: Epoch: 110, Loss:0.3916 Train: 0.8750, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0060
[2025-04-01 02:44:34,290]: Epoch: 111, Loss:0.3993 Train: 0.9167, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0080
[2025-04-01 02:44:34,297]: Epoch: 112, Loss:0.3671 Train: 0.9000, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:34,306]: Epoch: 113, Loss:0.3804 Train: 0.9167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:34,314]: Epoch: 114, Loss:0.3932 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:34,321]: Epoch: 115, Loss:0.3381 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:34,328]: Epoch: 116, Loss:0.4909 Train: 0.8833, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:34,336]: Epoch: 117, Loss:0.3159 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:34,342]: Epoch: 118, Loss:0.4021 Train: 0.9000, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:34,350]: Epoch: 119, Loss:0.4083 Train: 0.9167, Val:0.3625, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:34,358]: Epoch: 120, Loss:0.4369 Train: 0.9167, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:34,366]: Epoch: 121, Loss:0.4116 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:34,374]: Epoch: 122, Loss:0.3396 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:34,381]: Epoch: 123, Loss:0.3978 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:34,389]: Epoch: 124, Loss:0.3002 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:34,396]: Epoch: 125, Loss:0.3521 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:34,405]: Epoch: 126, Loss:0.3326 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:34,412]: Epoch: 127, Loss:0.4248 Train: 0.8833, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:34,418]: Epoch: 128, Loss:0.3564 Train: 0.9000, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0064
[2025-04-01 02:44:34,425]: Epoch: 129, Loss:0.3913 Train: 0.8667, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:34,434]: Epoch: 130, Loss:0.3849 Train: 0.8917, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:34,442]: Epoch: 131, Loss:0.3937 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:34,450]: Epoch: 132, Loss:0.3454 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:34,458]: Epoch: 133, Loss:0.3440 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:34,464]: Epoch: 134, Loss:0.3730 Train: 0.9000, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:34,472]: Epoch: 135, Loss:0.3019 Train: 0.8750, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:34,480]: Epoch: 136, Loss:0.4673 Train: 0.9083, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0081
[2025-04-01 02:44:34,488]: Epoch: 137, Loss:0.3763 Train: 0.9083, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:34,494]: Epoch: 138, Loss:0.3922 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:34,502]: Epoch: 139, Loss:0.3921 Train: 0.9167, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:34,510]: Epoch: 140, Loss:0.3158 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:34,517]: Epoch: 141, Loss:0.3800 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,525]: Epoch: 142, Loss:0.3367 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:34,533]: Epoch: 143, Loss:0.3961 Train: 0.8917, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:34,541]: Epoch: 144, Loss:0.3702 Train: 0.9167, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:34,548]: Epoch: 145, Loss:0.3126 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:34,554]: Epoch: 146, Loss:0.4404 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:34,561]: Epoch: 147, Loss:0.3636 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:34,568]: Epoch: 148, Loss:0.2984 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:34,575]: Epoch: 149, Loss:0.3299 Train: 0.9167, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:34,583]: Epoch: 150, Loss:0.3769 Train: 0.9000, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,590]: Epoch: 151, Loss:0.3924 Train: 0.9000, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:34,597]: Epoch: 152, Loss:0.3109 Train: 0.8833, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:34,606]: Epoch: 153, Loss:0.3430 Train: 0.8833, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0088
[2025-04-01 02:44:34,612]: Epoch: 154, Loss:0.4246 Train: 0.9167, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0059
[2025-04-01 02:44:34,619]: Epoch: 155, Loss:0.3579 Train: 0.9167, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:34,626]: Epoch: 156, Loss:0.2836 Train: 0.9333, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:34,633]: Epoch: 157, Loss:0.3769 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:34,640]: Epoch: 158, Loss:0.3467 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:34,648]: Epoch: 159, Loss:0.5601 Train: 0.8917, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:34,656]: Epoch: 160, Loss:0.3275 Train: 0.9167, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:34,663]: Epoch: 161, Loss:0.3713 Train: 0.8750, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:34,669]: Epoch: 162, Loss:0.4999 Train: 0.9250, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0061
[2025-04-01 02:44:34,676]: Epoch: 163, Loss:0.3500 Train: 0.9167, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0064
[2025-04-01 02:44:34,685]: Epoch: 164, Loss:0.3721 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0091
[2025-04-01 02:44:34,693]: Epoch: 165, Loss:0.4430 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,699]: Epoch: 166, Loss:0.3203 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:34,706]: Epoch: 167, Loss:0.3566 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:34,712]: Epoch: 168, Loss:0.3521 Train: 0.8750, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:34,719]: Epoch: 169, Loss:0.3237 Train: 0.8833, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:34,726]: Epoch: 170, Loss:0.4010 Train: 0.9167, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:34,734]: Epoch: 171, Loss:0.3513 Train: 0.9083, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:34,742]: Epoch: 172, Loss:0.3641 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:34,750]: Epoch: 173, Loss:0.3283 Train: 0.9083, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:34,757]: Epoch: 174, Loss:0.3583 Train: 0.9167, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:34,763]: Epoch: 175, Loss:0.4038 Train: 0.9333, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:34,770]: Epoch: 176, Loss:0.3584 Train: 0.9250, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:34,777]: Epoch: 177, Loss:0.3217 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:34,783]: Epoch: 178, Loss:0.3700 Train: 0.9000, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0061
[2025-04-01 02:44:34,790]: Epoch: 179, Loss:0.3355 Train: 0.8833, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:34,797]: Epoch: 180, Loss:0.3585 Train: 0.9167, Val:0.4000, Test: 0.3333, Time(s/epoch):0.0070
[2025-04-01 02:44:34,805]: Epoch: 181, Loss:0.3280 Train: 0.9083, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:34,813]: Epoch: 182, Loss:0.3431 Train: 0.9250, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0076
[2025-04-01 02:44:34,819]: Epoch: 183, Loss:0.3200 Train: 0.9250, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:34,827]: Epoch: 184, Loss:0.2809 Train: 0.9250, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:34,835]: Epoch: 185, Loss:0.4851 Train: 0.9250, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:34,841]: Epoch: 186, Loss:0.3316 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0054
[2025-04-01 02:44:34,849]: Epoch: 187, Loss:0.3609 Train: 0.9000, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:34,856]: Epoch: 188, Loss:0.4061 Train: 0.9333, Val:0.3625, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:34,864]: Epoch: 189, Loss:0.3895 Train: 0.9083, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:34,871]: Epoch: 190, Loss:0.3403 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:34,879]: Epoch: 191, Loss:0.3933 Train: 0.9000, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:34,887]: Epoch: 192, Loss:0.4053 Train: 0.8833, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:34,895]: Epoch: 193, Loss:0.3711 Train: 0.9000, Val:0.3500, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:34,902]: Epoch: 194, Loss:0.3738 Train: 0.8833, Val:0.3500, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:34,910]: Epoch: 195, Loss:0.4032 Train: 0.8917, Val:0.3375, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:34,918]: Epoch: 196, Loss:0.3418 Train: 0.9000, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:34,926]: Epoch: 197, Loss:0.3866 Train: 0.8833, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:34,935]: Epoch: 198, Loss:0.3900 Train: 0.8583, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:34,942]: Epoch: 199, Loss:0.3705 Train: 0.8750, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:34,950]: Epoch: 200, Loss:0.4292 Train: 0.9083, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:34,951]: [Run-2 score] {'train': 0.55, 'val': 0.5125, 'test': 0.43137254901960786}
[2025-04-01 02:44:34,951]: repeat 3/3
[2025-04-01 02:44:34,951]: Manual random seed:0
[2025-04-01 02:44:34,951]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:34,954]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:34,965]: Epoch: 001, Loss:1.7000 Train: 0.5000, Val:0.5250, Test: 0.6078, Time(s/epoch):0.0078
[2025-04-01 02:44:34,971]: Epoch: 002, Loss:1.2601 Train: 0.5750, Val:0.4125, Test: 0.3333, Time(s/epoch):0.0059
[2025-04-01 02:44:34,979]: Epoch: 003, Loss:1.2118 Train: 0.6917, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:34,987]: Epoch: 004, Loss:1.0110 Train: 0.7000, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:34,993]: Epoch: 005, Loss:0.9210 Train: 0.7167, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:35,001]: Epoch: 006, Loss:0.8845 Train: 0.7000, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:35,007]: Epoch: 007, Loss:0.8323 Train: 0.7500, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:35,014]: Epoch: 008, Loss:0.7435 Train: 0.7667, Val:0.4375, Test: 0.3529, Time(s/epoch):0.0063
[2025-04-01 02:44:35,021]: Epoch: 009, Loss:0.7385 Train: 0.7583, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:35,030]: Epoch: 010, Loss:0.6439 Train: 0.7750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:35,038]: Epoch: 011, Loss:0.7425 Train: 0.8083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:35,045]: Epoch: 012, Loss:0.6786 Train: 0.8083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:35,053]: Epoch: 013, Loss:0.6884 Train: 0.7250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:35,059]: Epoch: 014, Loss:0.7522 Train: 0.7667, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:35,066]: Epoch: 015, Loss:0.6213 Train: 0.8250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:35,075]: Epoch: 016, Loss:0.6099 Train: 0.8417, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:35,082]: Epoch: 017, Loss:0.6278 Train: 0.8000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:35,089]: Epoch: 018, Loss:0.5356 Train: 0.7833, Val:0.3875, Test: 0.3333, Time(s/epoch):0.0062
[2025-04-01 02:44:35,096]: Epoch: 019, Loss:0.5640 Train: 0.7167, Val:0.3000, Test: 0.2549, Time(s/epoch):0.0074
[2025-04-01 02:44:35,105]: Epoch: 020, Loss:0.6554 Train: 0.7833, Val:0.3500, Test: 0.2745, Time(s/epoch):0.0088
[2025-04-01 02:44:35,113]: Epoch: 021, Loss:0.5413 Train: 0.8250, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:35,122]: Epoch: 022, Loss:0.4818 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:35,130]: Epoch: 023, Loss:0.5396 Train: 0.8333, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:35,138]: Epoch: 024, Loss:0.5760 Train: 0.8333, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:35,148]: Epoch: 025, Loss:0.5273 Train: 0.8583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0099
[2025-04-01 02:44:35,158]: Epoch: 026, Loss:0.4607 Train: 0.8417, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0103
[2025-04-01 02:44:35,167]: Epoch: 027, Loss:0.5316 Train: 0.8583, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0089
[2025-04-01 02:44:35,176]: Epoch: 028, Loss:0.5019 Train: 0.8750, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0088
[2025-04-01 02:44:35,186]: Epoch: 029, Loss:0.4576 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0092
[2025-04-01 02:44:35,194]: Epoch: 030, Loss:0.4532 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:35,202]: Epoch: 031, Loss:0.4658 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:35,211]: Epoch: 032, Loss:0.5063 Train: 0.8667, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:35,220]: Epoch: 033, Loss:0.5573 Train: 0.8667, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0087
[2025-04-01 02:44:35,227]: Epoch: 034, Loss:0.5089 Train: 0.8417, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:35,234]: Epoch: 035, Loss:0.5298 Train: 0.8333, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:35,241]: Epoch: 036, Loss:0.4762 Train: 0.8583, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:35,248]: Epoch: 037, Loss:0.4521 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:35,255]: Epoch: 038, Loss:0.4422 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:35,261]: Epoch: 039, Loss:0.4013 Train: 0.8667, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:35,270]: Epoch: 040, Loss:0.4553 Train: 0.8750, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:35,276]: Epoch: 041, Loss:0.4190 Train: 0.8667, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:35,284]: Epoch: 042, Loss:0.4265 Train: 0.8667, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:35,295]: Epoch: 043, Loss:0.4268 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0099
[2025-04-01 02:44:35,303]: Epoch: 044, Loss:0.4255 Train: 0.8667, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:35,309]: Epoch: 045, Loss:0.4068 Train: 0.8667, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:35,317]: Epoch: 046, Loss:0.4383 Train: 0.8833, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:35,325]: Epoch: 047, Loss:0.4386 Train: 0.8750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:35,333]: Epoch: 048, Loss:0.3934 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:35,340]: Epoch: 049, Loss:0.3760 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:35,349]: Epoch: 050, Loss:0.4221 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:35,356]: Epoch: 051, Loss:0.4117 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:35,364]: Epoch: 052, Loss:0.3957 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:35,370]: Epoch: 053, Loss:0.4247 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:35,377]: Epoch: 054, Loss:0.4264 Train: 0.8833, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:35,385]: Epoch: 055, Loss:0.4131 Train: 0.8833, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0079
[2025-04-01 02:44:35,391]: Epoch: 056, Loss:0.3710 Train: 0.8833, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0057
[2025-04-01 02:44:35,399]: Epoch: 057, Loss:0.3918 Train: 0.8917, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:35,406]: Epoch: 058, Loss:0.3602 Train: 0.9000, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:35,415]: Epoch: 059, Loss:0.4024 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:35,421]: Epoch: 060, Loss:0.3977 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0057
[2025-04-01 02:44:35,428]: Epoch: 061, Loss:0.4043 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:35,435]: Epoch: 062, Loss:0.3775 Train: 0.8833, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:35,443]: Epoch: 063, Loss:0.3647 Train: 0.8833, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:35,449]: Epoch: 064, Loss:0.3968 Train: 0.8917, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0060
[2025-04-01 02:44:35,456]: Epoch: 065, Loss:0.4258 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:35,465]: Epoch: 066, Loss:0.3754 Train: 0.9083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:35,472]: Epoch: 067, Loss:0.4264 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:35,480]: Epoch: 068, Loss:0.3780 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:35,488]: Epoch: 069, Loss:0.4161 Train: 0.8917, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:35,496]: Epoch: 070, Loss:0.4117 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:35,503]: Epoch: 071, Loss:0.4224 Train: 0.8667, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0072
[2025-04-01 02:44:35,511]: Epoch: 072, Loss:0.3648 Train: 0.9000, Val:0.3625, Test: 0.3529, Time(s/epoch):0.0076
[2025-04-01 02:44:35,517]: Epoch: 073, Loss:0.3668 Train: 0.9000, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0063
[2025-04-01 02:44:35,524]: Epoch: 074, Loss:0.4030 Train: 0.8750, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:35,530]: Epoch: 075, Loss:0.4281 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:35,539]: Epoch: 076, Loss:0.3512 Train: 0.8917, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:35,546]: Epoch: 077, Loss:0.3592 Train: 0.9000, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:35,554]: Epoch: 078, Loss:0.3469 Train: 0.8917, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0076
[2025-04-01 02:44:35,560]: Epoch: 079, Loss:0.3719 Train: 0.8917, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0059
[2025-04-01 02:44:35,568]: Epoch: 080, Loss:0.3580 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:35,576]: Epoch: 081, Loss:0.3519 Train: 0.9167, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:35,582]: Epoch: 082, Loss:0.3891 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0056
[2025-04-01 02:44:35,588]: Epoch: 083, Loss:0.3551 Train: 0.8917, Val:0.4125, Test: 0.3333, Time(s/epoch):0.0066
[2025-04-01 02:44:35,596]: Epoch: 084, Loss:0.3968 Train: 0.9333, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:35,602]: Epoch: 085, Loss:0.3429 Train: 0.9583, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:35,609]: Epoch: 086, Loss:0.3470 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:35,617]: Epoch: 087, Loss:0.3662 Train: 0.9500, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0081
[2025-04-01 02:44:35,624]: Epoch: 088, Loss:0.4099 Train: 0.9500, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:35,633]: Epoch: 089, Loss:0.3421 Train: 0.9000, Val:0.4375, Test: 0.3529, Time(s/epoch):0.0084
[2025-04-01 02:44:35,641]: Epoch: 090, Loss:0.3632 Train: 0.8833, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:35,648]: Epoch: 091, Loss:0.3642 Train: 0.9000, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:35,657]: Epoch: 092, Loss:0.3476 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:35,664]: Epoch: 093, Loss:0.3690 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:35,672]: Epoch: 094, Loss:0.3849 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:35,679]: Epoch: 095, Loss:0.3253 Train: 0.9167, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:35,687]: Epoch: 096, Loss:0.3417 Train: 0.9000, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:35,696]: Epoch: 097, Loss:0.3318 Train: 0.8833, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0086
[2025-04-01 02:44:35,704]: Epoch: 098, Loss:0.3719 Train: 0.8750, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:35,711]: Epoch: 099, Loss:0.3451 Train: 0.8833, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:35,719]: Epoch: 100, Loss:0.4009 Train: 0.9000, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0072
[2025-04-01 02:44:35,727]: Epoch: 101, Loss:0.3187 Train: 0.9000, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0082
[2025-04-01 02:44:35,735]: Epoch: 102, Loss:0.4307 Train: 0.9083, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:35,742]: Epoch: 103, Loss:0.3534 Train: 0.9000, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:35,750]: Epoch: 104, Loss:0.3583 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:35,758]: Epoch: 105, Loss:0.4566 Train: 0.8833, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:35,766]: Epoch: 106, Loss:0.3097 Train: 0.8833, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:35,774]: Epoch: 107, Loss:0.3307 Train: 0.9333, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:35,780]: Epoch: 108, Loss:0.4386 Train: 0.9167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0057
[2025-04-01 02:44:35,788]: Epoch: 109, Loss:0.3689 Train: 0.9333, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:35,795]: Epoch: 110, Loss:0.3323 Train: 0.9250, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:35,801]: Epoch: 111, Loss:0.4247 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:35,809]: Epoch: 112, Loss:0.3296 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:35,815]: Epoch: 113, Loss:0.4336 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:35,824]: Epoch: 114, Loss:0.3832 Train: 0.9000, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:35,830]: Epoch: 115, Loss:0.3744 Train: 0.8667, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:35,838]: Epoch: 116, Loss:0.4510 Train: 0.8833, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:35,846]: Epoch: 117, Loss:0.3721 Train: 0.9083, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:35,854]: Epoch: 118, Loss:0.3081 Train: 0.8750, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:35,861]: Epoch: 119, Loss:0.3576 Train: 0.8750, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:35,870]: Epoch: 120, Loss:0.3786 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0087
[2025-04-01 02:44:35,877]: Epoch: 121, Loss:0.3243 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:35,884]: Epoch: 122, Loss:0.4246 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:35,890]: Epoch: 123, Loss:0.3349 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:35,898]: Epoch: 124, Loss:0.3557 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:35,906]: Epoch: 125, Loss:0.3942 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:35,915]: Epoch: 126, Loss:0.3515 Train: 0.9000, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0087
[2025-04-01 02:44:35,923]: Epoch: 127, Loss:0.3570 Train: 0.8917, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:35,929]: Epoch: 128, Loss:0.3950 Train: 0.9000, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:35,936]: Epoch: 129, Loss:0.3638 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:35,945]: Epoch: 130, Loss:0.3266 Train: 0.9167, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0087
[2025-04-01 02:44:35,952]: Epoch: 131, Loss:0.3733 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:35,960]: Epoch: 132, Loss:0.3462 Train: 0.9000, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:35,969]: Epoch: 133, Loss:0.3278 Train: 0.9000, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0090
[2025-04-01 02:44:35,976]: Epoch: 134, Loss:0.3538 Train: 0.9333, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:35,985]: Epoch: 135, Loss:0.3833 Train: 0.9083, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0086
[2025-04-01 02:44:35,992]: Epoch: 136, Loss:0.3857 Train: 0.9167, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0071
[2025-04-01 02:44:36,012]: Epoch: 137, Loss:0.3424 Train: 0.9167, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0188
[2025-04-01 02:44:36,028]: Epoch: 138, Loss:0.2851 Train: 0.9083, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0158
[2025-04-01 02:44:36,040]: Epoch: 139, Loss:0.3368 Train: 0.9167, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0119
[2025-04-01 02:44:36,047]: Epoch: 140, Loss:0.3426 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:36,059]: Epoch: 141, Loss:0.3796 Train: 0.9250, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0117
[2025-04-01 02:44:36,067]: Epoch: 142, Loss:0.3231 Train: 0.9167, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:36,074]: Epoch: 143, Loss:0.3226 Train: 0.8917, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0071
[2025-04-01 02:44:36,082]: Epoch: 144, Loss:0.3396 Train: 0.9167, Val:0.3750, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:36,089]: Epoch: 145, Loss:0.3775 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:36,096]: Epoch: 146, Loss:0.3017 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:36,105]: Epoch: 147, Loss:0.3144 Train: 0.8667, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:36,112]: Epoch: 148, Loss:0.4508 Train: 0.8833, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:36,118]: Epoch: 149, Loss:0.2767 Train: 0.9333, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0059
[2025-04-01 02:44:36,125]: Epoch: 150, Loss:0.2437 Train: 0.9417, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:36,131]: Epoch: 151, Loss:0.3173 Train: 0.9250, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:36,138]: Epoch: 152, Loss:0.3461 Train: 0.9250, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0064
[2025-04-01 02:44:36,145]: Epoch: 153, Loss:0.3165 Train: 0.9417, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:36,152]: Epoch: 154, Loss:0.2928 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:36,159]: Epoch: 155, Loss:0.3465 Train: 0.8917, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:36,167]: Epoch: 156, Loss:0.3447 Train: 0.9000, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:36,176]: Epoch: 157, Loss:0.4289 Train: 0.9333, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0083
[2025-04-01 02:44:36,182]: Epoch: 158, Loss:0.3922 Train: 0.8833, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:36,190]: Epoch: 159, Loss:0.5124 Train: 0.9000, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:36,199]: Epoch: 160, Loss:0.4501 Train: 0.9250, Val:0.3875, Test: 0.3529, Time(s/epoch):0.0083
[2025-04-01 02:44:36,206]: Epoch: 161, Loss:0.3627 Train: 0.8750, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:36,213]: Epoch: 162, Loss:0.3553 Train: 0.8250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:36,220]: Epoch: 163, Loss:0.4066 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:36,228]: Epoch: 164, Loss:0.3852 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:36,236]: Epoch: 165, Loss:0.3174 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:36,244]: Epoch: 166, Loss:0.3960 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:36,251]: Epoch: 167, Loss:0.3230 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:36,258]: Epoch: 168, Loss:0.3290 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:36,265]: Epoch: 169, Loss:0.2797 Train: 0.8917, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:36,273]: Epoch: 170, Loss:0.3604 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:36,279]: Epoch: 171, Loss:0.3128 Train: 0.9083, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0057
[2025-04-01 02:44:36,287]: Epoch: 172, Loss:0.3538 Train: 0.9000, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:36,295]: Epoch: 173, Loss:0.3926 Train: 0.9333, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:36,305]: Epoch: 174, Loss:0.3532 Train: 0.9500, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0097
[2025-04-01 02:44:36,313]: Epoch: 175, Loss:0.3205 Train: 0.9250, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0080
[2025-04-01 02:44:36,320]: Epoch: 176, Loss:0.3360 Train: 0.9250, Val:0.3750, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:36,328]: Epoch: 177, Loss:0.3022 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0072
[2025-04-01 02:44:36,334]: Epoch: 178, Loss:0.3948 Train: 0.9333, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:36,342]: Epoch: 179, Loss:0.3448 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:36,350]: Epoch: 180, Loss:0.3197 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:36,358]: Epoch: 181, Loss:0.3965 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:36,366]: Epoch: 182, Loss:0.4992 Train: 0.9500, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:36,374]: Epoch: 183, Loss:0.3118 Train: 0.9333, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:36,380]: Epoch: 184, Loss:0.3205 Train: 0.9167, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:36,388]: Epoch: 185, Loss:0.3831 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:36,395]: Epoch: 186, Loss:0.3519 Train: 0.9167, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:36,404]: Epoch: 187, Loss:0.2764 Train: 0.9417, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0086
[2025-04-01 02:44:36,411]: Epoch: 188, Loss:0.2586 Train: 0.9333, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:36,420]: Epoch: 189, Loss:0.2880 Train: 0.9250, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:36,427]: Epoch: 190, Loss:0.3055 Train: 0.9250, Val:0.3625, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:36,435]: Epoch: 191, Loss:0.3592 Train: 0.9000, Val:0.3625, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:36,442]: Epoch: 192, Loss:0.3531 Train: 0.9417, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:36,451]: Epoch: 193, Loss:0.3821 Train: 0.9417, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0082
[2025-04-01 02:44:36,460]: Epoch: 194, Loss:0.3942 Train: 0.9417, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:36,466]: Epoch: 195, Loss:0.3509 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:36,475]: Epoch: 196, Loss:0.3783 Train: 0.9250, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:36,481]: Epoch: 197, Loss:0.3245 Train: 0.9250, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:36,489]: Epoch: 198, Loss:0.3501 Train: 0.8917, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:36,497]: Epoch: 199, Loss:0.3483 Train: 0.9167, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:36,505]: Epoch: 200, Loss:0.3203 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:36,505]: [Run-3 score] {'train': 0.5, 'val': 0.525, 'test': 0.6078431372549019}
[2025-04-01 02:44:36,505]: repeat 1/3
[2025-04-01 02:44:36,505]: Manual random seed:0
[2025-04-01 02:44:36,505]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:36,508]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:36,518]: Epoch: 001, Loss:1.7289 Train: 0.4500, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:36,524]: Epoch: 002, Loss:2.1126 Train: 0.5833, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0056
[2025-04-01 02:44:36,531]: Epoch: 003, Loss:1.3509 Train: 0.5667, Val:0.4250, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:44:36,538]: Epoch: 004, Loss:1.1575 Train: 0.6250, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:36,546]: Epoch: 005, Loss:1.0424 Train: 0.6083, Val:0.6125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:36,552]: Epoch: 006, Loss:1.1247 Train: 0.6250, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:36,560]: Epoch: 007, Loss:1.1322 Train: 0.6667, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:36,568]: Epoch: 008, Loss:1.0117 Train: 0.6917, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:36,574]: Epoch: 009, Loss:0.9752 Train: 0.7167, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:36,581]: Epoch: 010, Loss:0.8936 Train: 0.7250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:36,589]: Epoch: 011, Loss:0.9470 Train: 0.7250, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:36,597]: Epoch: 012, Loss:0.8276 Train: 0.7000, Val:0.5625, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:36,604]: Epoch: 013, Loss:0.8670 Train: 0.7083, Val:0.5625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:36,612]: Epoch: 014, Loss:0.7814 Train: 0.7167, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:36,618]: Epoch: 015, Loss:0.8103 Train: 0.7250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:36,625]: Epoch: 016, Loss:0.6934 Train: 0.7333, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:36,633]: Epoch: 017, Loss:0.7496 Train: 0.6917, Val:0.2875, Test: 0.3333, Time(s/epoch):0.0077
[2025-04-01 02:44:36,641]: Epoch: 018, Loss:0.8278 Train: 0.7750, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:36,648]: Epoch: 019, Loss:0.7405 Train: 0.8083, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:36,656]: Epoch: 020, Loss:0.8566 Train: 0.7583, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:36,662]: Epoch: 021, Loss:0.6930 Train: 0.7417, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:36,669]: Epoch: 022, Loss:0.6638 Train: 0.7083, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:36,677]: Epoch: 023, Loss:0.7001 Train: 0.7750, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:36,685]: Epoch: 024, Loss:0.6486 Train: 0.8083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:36,691]: Epoch: 025, Loss:0.6579 Train: 0.8333, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:36,699]: Epoch: 026, Loss:0.6524 Train: 0.8167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:36,707]: Epoch: 027, Loss:0.6412 Train: 0.8083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:36,714]: Epoch: 028, Loss:0.5953 Train: 0.8167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:36,722]: Epoch: 029, Loss:0.6297 Train: 0.8167, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:36,728]: Epoch: 030, Loss:0.5576 Train: 0.8333, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:36,735]: Epoch: 031, Loss:0.5790 Train: 0.8500, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:36,742]: Epoch: 032, Loss:0.6884 Train: 0.8583, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:36,749]: Epoch: 033, Loss:0.5478 Train: 0.8500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:36,757]: Epoch: 034, Loss:0.5311 Train: 0.8250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:36,763]: Epoch: 035, Loss:0.5430 Train: 0.8167, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:36,771]: Epoch: 036, Loss:0.5195 Train: 0.8083, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:36,779]: Epoch: 037, Loss:0.6562 Train: 0.8083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:36,787]: Epoch: 038, Loss:0.5136 Train: 0.8250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:36,795]: Epoch: 039, Loss:0.5955 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:36,801]: Epoch: 040, Loss:0.5215 Train: 0.8750, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:36,809]: Epoch: 041, Loss:0.5381 Train: 0.8750, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:36,817]: Epoch: 042, Loss:0.5442 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:36,824]: Epoch: 043, Loss:0.5247 Train: 0.8750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:36,832]: Epoch: 044, Loss:0.5407 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:36,840]: Epoch: 045, Loss:0.4828 Train: 0.8500, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:36,848]: Epoch: 046, Loss:0.5472 Train: 0.8583, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:36,856]: Epoch: 047, Loss:0.4754 Train: 0.8833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:36,862]: Epoch: 048, Loss:0.4646 Train: 0.8917, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:36,870]: Epoch: 049, Loss:0.4213 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:36,877]: Epoch: 050, Loss:0.4805 Train: 0.8917, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:36,884]: Epoch: 051, Loss:0.5229 Train: 0.8500, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:36,891]: Epoch: 052, Loss:0.4208 Train: 0.8417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:36,899]: Epoch: 053, Loss:0.4631 Train: 0.8750, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:36,907]: Epoch: 054, Loss:0.4242 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:36,913]: Epoch: 055, Loss:0.4422 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:36,921]: Epoch: 056, Loss:0.4180 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:36,930]: Epoch: 057, Loss:0.4729 Train: 0.9167, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:36,938]: Epoch: 058, Loss:0.4033 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:36,947]: Epoch: 059, Loss:0.4426 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:36,953]: Epoch: 060, Loss:0.4283 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:36,960]: Epoch: 061, Loss:0.4462 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:36,968]: Epoch: 062, Loss:0.4139 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:36,975]: Epoch: 063, Loss:0.4136 Train: 0.8750, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:36,981]: Epoch: 064, Loss:0.4106 Train: 0.8833, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:36,988]: Epoch: 065, Loss:0.4546 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:36,996]: Epoch: 066, Loss:0.4306 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:37,003]: Epoch: 067, Loss:0.4080 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:37,009]: Epoch: 068, Loss:0.4687 Train: 0.8667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:37,017]: Epoch: 069, Loss:0.4384 Train: 0.8583, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:37,023]: Epoch: 070, Loss:0.4382 Train: 0.8750, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:37,031]: Epoch: 071, Loss:0.4036 Train: 0.9083, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:37,039]: Epoch: 072, Loss:0.4055 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:37,048]: Epoch: 073, Loss:0.4300 Train: 0.9250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:37,054]: Epoch: 074, Loss:0.3620 Train: 0.8833, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:37,062]: Epoch: 075, Loss:0.3858 Train: 0.8667, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:37,070]: Epoch: 076, Loss:0.4829 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:37,077]: Epoch: 077, Loss:0.4075 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:37,085]: Epoch: 078, Loss:0.4186 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:37,094]: Epoch: 079, Loss:0.3776 Train: 0.9250, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:37,100]: Epoch: 080, Loss:0.4123 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:37,107]: Epoch: 081, Loss:0.3660 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:37,114]: Epoch: 082, Loss:0.4110 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:37,121]: Epoch: 083, Loss:0.3854 Train: 0.8667, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:37,128]: Epoch: 084, Loss:0.4114 Train: 0.8250, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:37,135]: Epoch: 085, Loss:0.4495 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:37,142]: Epoch: 086, Loss:0.4039 Train: 0.9250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:37,149]: Epoch: 087, Loss:0.3645 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:37,157]: Epoch: 088, Loss:0.4134 Train: 0.9250, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:37,165]: Epoch: 089, Loss:0.4130 Train: 0.9250, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:37,174]: Epoch: 090, Loss:0.3738 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0087
[2025-04-01 02:44:37,182]: Epoch: 091, Loss:0.4782 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:37,191]: Epoch: 092, Loss:0.3755 Train: 0.9167, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:37,199]: Epoch: 093, Loss:0.3442 Train: 0.9000, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:37,208]: Epoch: 094, Loss:0.3940 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:37,214]: Epoch: 095, Loss:0.3428 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:37,223]: Epoch: 096, Loss:0.3885 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0089
[2025-04-01 02:44:37,232]: Epoch: 097, Loss:0.4254 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:37,241]: Epoch: 098, Loss:0.3745 Train: 0.9250, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:37,249]: Epoch: 099, Loss:0.3703 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:37,258]: Epoch: 100, Loss:0.3853 Train: 0.9167, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:37,267]: Epoch: 101, Loss:0.3383 Train: 0.9167, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0092
[2025-04-01 02:44:37,276]: Epoch: 102, Loss:0.3954 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:37,284]: Epoch: 103, Loss:0.3507 Train: 0.9083, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:37,295]: Epoch: 104, Loss:0.3819 Train: 0.9250, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0108
[2025-04-01 02:44:37,305]: Epoch: 105, Loss:0.3714 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0094
[2025-04-01 02:44:37,314]: Epoch: 106, Loss:0.3426 Train: 0.9333, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0088
[2025-04-01 02:44:37,322]: Epoch: 107, Loss:0.3231 Train: 0.9250, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:37,329]: Epoch: 108, Loss:0.3415 Train: 0.9250, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:37,336]: Epoch: 109, Loss:0.3380 Train: 0.9167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:37,344]: Epoch: 110, Loss:0.3610 Train: 0.8833, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:37,350]: Epoch: 111, Loss:0.3975 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:37,359]: Epoch: 112, Loss:0.3862 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:37,366]: Epoch: 113, Loss:0.3316 Train: 0.9333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:37,374]: Epoch: 114, Loss:0.3550 Train: 0.9083, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:37,380]: Epoch: 115, Loss:0.3724 Train: 0.9167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:37,386]: Epoch: 116, Loss:0.3115 Train: 0.9167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:37,394]: Epoch: 117, Loss:0.3775 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:37,402]: Epoch: 118, Loss:0.3363 Train: 0.9167, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:37,409]: Epoch: 119, Loss:0.3918 Train: 0.9333, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:37,415]: Epoch: 120, Loss:0.3395 Train: 0.9333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:37,423]: Epoch: 121, Loss:0.3827 Train: 0.8833, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:37,431]: Epoch: 122, Loss:0.3902 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:37,438]: Epoch: 123, Loss:0.4140 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:37,446]: Epoch: 124, Loss:0.3213 Train: 0.8750, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:37,454]: Epoch: 125, Loss:0.4588 Train: 0.9167, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:37,462]: Epoch: 126, Loss:0.3676 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:37,470]: Epoch: 127, Loss:0.3484 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:37,478]: Epoch: 128, Loss:0.3862 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:37,485]: Epoch: 129, Loss:0.4151 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:37,493]: Epoch: 130, Loss:0.3672 Train: 0.9417, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:37,501]: Epoch: 131, Loss:0.3417 Train: 0.8750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:37,509]: Epoch: 132, Loss:0.3863 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:37,517]: Epoch: 133, Loss:0.3796 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:37,525]: Epoch: 134, Loss:0.4177 Train: 0.8917, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:37,532]: Epoch: 135, Loss:0.3552 Train: 0.8667, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:37,540]: Epoch: 136, Loss:0.4424 Train: 0.9000, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:37,547]: Epoch: 137, Loss:0.3244 Train: 0.9083, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:37,555]: Epoch: 138, Loss:0.4481 Train: 0.9083, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:37,562]: Epoch: 139, Loss:0.3398 Train: 0.9250, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:37,570]: Epoch: 140, Loss:0.4342 Train: 0.9417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:37,579]: Epoch: 141, Loss:0.3506 Train: 0.9167, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:37,585]: Epoch: 142, Loss:0.3300 Train: 0.9000, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:37,593]: Epoch: 143, Loss:0.3604 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:37,601]: Epoch: 144, Loss:0.3734 Train: 0.9250, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:37,608]: Epoch: 145, Loss:0.3570 Train: 0.9083, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:37,616]: Epoch: 146, Loss:0.3123 Train: 0.9250, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:37,624]: Epoch: 147, Loss:0.3913 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:37,630]: Epoch: 148, Loss:0.5025 Train: 0.9083, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:37,636]: Epoch: 149, Loss:0.3356 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:37,641]: Epoch: 150, Loss:0.4157 Train: 0.9333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0053
[2025-04-01 02:44:37,649]: Epoch: 151, Loss:0.3450 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:37,657]: Epoch: 152, Loss:0.3554 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:37,665]: Epoch: 153, Loss:0.3881 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:37,674]: Epoch: 154, Loss:0.3141 Train: 0.9417, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:37,681]: Epoch: 155, Loss:0.3358 Train: 0.9167, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:37,688]: Epoch: 156, Loss:0.3365 Train: 0.9417, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:37,695]: Epoch: 157, Loss:0.3677 Train: 0.9417, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:37,703]: Epoch: 158, Loss:0.3178 Train: 0.9167, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:37,710]: Epoch: 159, Loss:0.3609 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:37,718]: Epoch: 160, Loss:0.3388 Train: 0.9250, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:37,725]: Epoch: 161, Loss:0.3399 Train: 0.9250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:37,735]: Epoch: 162, Loss:0.3373 Train: 0.8917, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0091
[2025-04-01 02:44:37,741]: Epoch: 163, Loss:0.3587 Train: 0.8917, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:37,749]: Epoch: 164, Loss:0.4768 Train: 0.8917, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:37,758]: Epoch: 165, Loss:0.3060 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:37,766]: Epoch: 166, Loss:0.4023 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:37,773]: Epoch: 167, Loss:0.3414 Train: 0.9083, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:37,780]: Epoch: 168, Loss:0.3356 Train: 0.9417, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:37,790]: Epoch: 169, Loss:0.3010 Train: 0.9250, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0093
[2025-04-01 02:44:37,799]: Epoch: 170, Loss:0.3526 Train: 0.9083, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0095
[2025-04-01 02:44:37,808]: Epoch: 171, Loss:0.3858 Train: 0.9500, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:37,818]: Epoch: 172, Loss:0.3258 Train: 0.9500, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0092
[2025-04-01 02:44:37,826]: Epoch: 173, Loss:0.3019 Train: 0.9333, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:37,832]: Epoch: 174, Loss:0.3767 Train: 0.9417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:37,839]: Epoch: 175, Loss:0.3551 Train: 0.9417, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:37,847]: Epoch: 176, Loss:0.3538 Train: 0.9417, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:37,857]: Epoch: 177, Loss:0.3327 Train: 0.9500, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:37,864]: Epoch: 178, Loss:0.3182 Train: 0.9417, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:37,870]: Epoch: 179, Loss:0.3387 Train: 0.9250, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:44:37,879]: Epoch: 180, Loss:0.3636 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:37,885]: Epoch: 181, Loss:0.3331 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:37,894]: Epoch: 182, Loss:0.3522 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:37,901]: Epoch: 183, Loss:0.3707 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:37,908]: Epoch: 184, Loss:0.3354 Train: 0.8750, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:37,916]: Epoch: 185, Loss:0.3305 Train: 0.9167, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:37,924]: Epoch: 186, Loss:0.2761 Train: 0.9417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:37,930]: Epoch: 187, Loss:0.3094 Train: 0.9250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:37,938]: Epoch: 188, Loss:0.4017 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:37,945]: Epoch: 189, Loss:0.2929 Train: 0.9167, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:37,951]: Epoch: 190, Loss:0.2973 Train: 0.9250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:37,958]: Epoch: 191, Loss:0.3453 Train: 0.9333, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:37,966]: Epoch: 192, Loss:0.3065 Train: 0.8917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:37,972]: Epoch: 193, Loss:0.4672 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:37,979]: Epoch: 194, Loss:0.3636 Train: 0.8500, Val:0.4125, Test: 0.3333, Time(s/epoch):0.0064
[2025-04-01 02:44:37,987]: Epoch: 195, Loss:0.4159 Train: 0.8583, Val:0.4000, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:37,993]: Epoch: 196, Loss:0.3836 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:37,999]: Epoch: 197, Loss:0.3627 Train: 0.9417, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:38,008]: Epoch: 198, Loss:0.3044 Train: 0.8583, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:38,016]: Epoch: 199, Loss:0.4532 Train: 0.9333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:38,022]: Epoch: 200, Loss:0.3631 Train: 0.9333, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:38,022]: [Run-1 score] {'train': 0.6083333333333333, 'val': 0.6125, 'test': 0.47058823529411764}
[2025-04-01 02:44:38,022]: repeat 2/3
[2025-04-01 02:44:38,022]: Manual random seed:0
[2025-04-01 02:44:38,022]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:38,025]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:38,036]: Epoch: 001, Loss:1.6261 Train: 0.5583, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:44:38,043]: Epoch: 002, Loss:1.6662 Train: 0.3167, Val:0.2125, Test: 0.2941, Time(s/epoch):0.0069
[2025-04-01 02:44:38,049]: Epoch: 003, Loss:1.6209 Train: 0.5750, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:38,056]: Epoch: 004, Loss:1.2259 Train: 0.6250, Val:0.5500, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:38,064]: Epoch: 005, Loss:1.0100 Train: 0.6250, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:38,070]: Epoch: 006, Loss:1.1082 Train: 0.6167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:44:38,078]: Epoch: 007, Loss:1.0476 Train: 0.6417, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:38,086]: Epoch: 008, Loss:1.0570 Train: 0.6750, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:38,093]: Epoch: 009, Loss:0.8733 Train: 0.7000, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:38,101]: Epoch: 010, Loss:0.9256 Train: 0.7000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:38,107]: Epoch: 011, Loss:0.8229 Train: 0.7000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:38,114]: Epoch: 012, Loss:0.7781 Train: 0.7167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:38,120]: Epoch: 013, Loss:0.9276 Train: 0.7083, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:38,128]: Epoch: 014, Loss:0.8868 Train: 0.7000, Val:0.5750, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:44:38,134]: Epoch: 015, Loss:0.7240 Train: 0.6500, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:38,142]: Epoch: 016, Loss:0.7326 Train: 0.6500, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:38,150]: Epoch: 017, Loss:0.9253 Train: 0.6833, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:38,157]: Epoch: 018, Loss:0.6860 Train: 0.7583, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:38,165]: Epoch: 019, Loss:0.6900 Train: 0.7917, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:38,171]: Epoch: 020, Loss:0.6800 Train: 0.7833, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0062
[2025-04-01 02:44:38,179]: Epoch: 021, Loss:0.6891 Train: 0.8167, Val:0.4375, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:38,187]: Epoch: 022, Loss:0.6390 Train: 0.8000, Val:0.4750, Test: 0.3529, Time(s/epoch):0.0076
[2025-04-01 02:44:38,194]: Epoch: 023, Loss:0.6467 Train: 0.8167, Val:0.4750, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:38,202]: Epoch: 024, Loss:0.6402 Train: 0.8167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:38,209]: Epoch: 025, Loss:0.6150 Train: 0.7917, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:38,217]: Epoch: 026, Loss:0.6079 Train: 0.7750, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:38,225]: Epoch: 027, Loss:0.6053 Train: 0.7750, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:38,232]: Epoch: 028, Loss:0.6721 Train: 0.7750, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:38,239]: Epoch: 029, Loss:0.5625 Train: 0.8250, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:38,245]: Epoch: 030, Loss:0.5707 Train: 0.8583, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:38,253]: Epoch: 031, Loss:0.5934 Train: 0.8583, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:38,261]: Epoch: 032, Loss:0.5704 Train: 0.8583, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:38,268]: Epoch: 033, Loss:0.5445 Train: 0.8500, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:38,276]: Epoch: 034, Loss:0.5314 Train: 0.8417, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:38,282]: Epoch: 035, Loss:0.5622 Train: 0.8583, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:38,292]: Epoch: 036, Loss:0.5199 Train: 0.8250, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0089
[2025-04-01 02:44:38,300]: Epoch: 037, Loss:0.5118 Train: 0.8500, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:38,309]: Epoch: 038, Loss:0.5113 Train: 0.8333, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:38,317]: Epoch: 039, Loss:0.5416 Train: 0.8417, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:38,325]: Epoch: 040, Loss:0.5226 Train: 0.8417, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:38,332]: Epoch: 041, Loss:0.4640 Train: 0.8500, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:38,339]: Epoch: 042, Loss:0.5071 Train: 0.7833, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:38,348]: Epoch: 043, Loss:0.5283 Train: 0.7583, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0085
[2025-04-01 02:44:38,354]: Epoch: 044, Loss:0.5606 Train: 0.8083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:38,363]: Epoch: 045, Loss:0.4999 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:38,371]: Epoch: 046, Loss:0.5175 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:38,379]: Epoch: 047, Loss:0.4886 Train: 0.8750, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:38,386]: Epoch: 048, Loss:0.5371 Train: 0.8667, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:38,393]: Epoch: 049, Loss:0.5081 Train: 0.8667, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:38,401]: Epoch: 050, Loss:0.4839 Train: 0.8667, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:38,408]: Epoch: 051, Loss:0.4609 Train: 0.8667, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:38,415]: Epoch: 052, Loss:0.4571 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:38,422]: Epoch: 053, Loss:0.4838 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:38,430]: Epoch: 054, Loss:0.4950 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:38,438]: Epoch: 055, Loss:0.4934 Train: 0.8750, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:38,446]: Epoch: 056, Loss:0.4514 Train: 0.8250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:38,454]: Epoch: 057, Loss:0.4775 Train: 0.8833, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:38,462]: Epoch: 058, Loss:0.4889 Train: 0.8667, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:38,469]: Epoch: 059, Loss:0.5478 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:38,476]: Epoch: 060, Loss:0.5273 Train: 0.8583, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:38,485]: Epoch: 061, Loss:0.4755 Train: 0.7750, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:38,490]: Epoch: 062, Loss:0.5074 Train: 0.7833, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0058
[2025-04-01 02:44:38,497]: Epoch: 063, Loss:0.4987 Train: 0.8417, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:38,505]: Epoch: 064, Loss:0.4801 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:38,513]: Epoch: 065, Loss:0.4534 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:38,521]: Epoch: 066, Loss:0.5387 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:38,528]: Epoch: 067, Loss:0.4220 Train: 0.9000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:38,534]: Epoch: 068, Loss:0.4598 Train: 0.8583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:38,541]: Epoch: 069, Loss:0.4443 Train: 0.9083, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:38,549]: Epoch: 070, Loss:0.4120 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:38,557]: Epoch: 071, Loss:0.3896 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:38,564]: Epoch: 072, Loss:0.4264 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:38,570]: Epoch: 073, Loss:0.4430 Train: 0.8750, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:38,577]: Epoch: 074, Loss:0.4147 Train: 0.8417, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:38,584]: Epoch: 075, Loss:0.4124 Train: 0.9000, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:38,592]: Epoch: 076, Loss:0.3928 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:38,600]: Epoch: 077, Loss:0.4193 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:38,608]: Epoch: 078, Loss:0.4875 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:38,615]: Epoch: 079, Loss:0.3899 Train: 0.9167, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:38,621]: Epoch: 080, Loss:0.3857 Train: 0.9250, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:38,629]: Epoch: 081, Loss:0.3430 Train: 0.9000, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:38,637]: Epoch: 082, Loss:0.4146 Train: 0.8833, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:38,645]: Epoch: 083, Loss:0.4103 Train: 0.9167, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:38,653]: Epoch: 084, Loss:0.4090 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:38,661]: Epoch: 085, Loss:0.3836 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:38,667]: Epoch: 086, Loss:0.4331 Train: 0.9167, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:38,674]: Epoch: 087, Loss:0.3772 Train: 0.9250, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:38,682]: Epoch: 088, Loss:0.4169 Train: 0.9250, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:38,689]: Epoch: 089, Loss:0.4002 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:38,696]: Epoch: 090, Loss:0.3652 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:38,703]: Epoch: 091, Loss:0.4127 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:38,711]: Epoch: 092, Loss:0.3596 Train: 0.9417, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:38,719]: Epoch: 093, Loss:0.4042 Train: 0.9250, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:38,727]: Epoch: 094, Loss:0.3700 Train: 0.9417, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:38,733]: Epoch: 095, Loss:0.3994 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:38,741]: Epoch: 096, Loss:0.4355 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:38,749]: Epoch: 097, Loss:0.4694 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:38,755]: Epoch: 098, Loss:0.3806 Train: 0.9167, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:38,762]: Epoch: 099, Loss:0.3717 Train: 0.9250, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0061
[2025-04-01 02:44:38,770]: Epoch: 100, Loss:0.3973 Train: 0.9167, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:38,778]: Epoch: 101, Loss:0.3890 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:38,785]: Epoch: 102, Loss:0.3610 Train: 0.8833, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:38,791]: Epoch: 103, Loss:0.4152 Train: 0.8750, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:38,800]: Epoch: 104, Loss:0.3871 Train: 0.8833, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:38,806]: Epoch: 105, Loss:0.3917 Train: 0.9083, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:38,813]: Epoch: 106, Loss:0.3436 Train: 0.9000, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:38,820]: Epoch: 107, Loss:0.3489 Train: 0.9333, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:38,827]: Epoch: 108, Loss:0.3859 Train: 0.9333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:38,834]: Epoch: 109, Loss:0.3898 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:38,840]: Epoch: 110, Loss:0.4213 Train: 0.9333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:38,847]: Epoch: 111, Loss:0.3342 Train: 0.9250, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:38,855]: Epoch: 112, Loss:0.3465 Train: 0.8833, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0071
[2025-04-01 02:44:38,862]: Epoch: 113, Loss:0.4220 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:38,870]: Epoch: 114, Loss:0.3972 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:38,878]: Epoch: 115, Loss:0.3659 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:38,885]: Epoch: 116, Loss:0.4032 Train: 0.9333, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:38,892]: Epoch: 117, Loss:0.3970 Train: 0.9333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:38,900]: Epoch: 118, Loss:0.3974 Train: 0.8833, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:38,907]: Epoch: 119, Loss:0.3772 Train: 0.8667, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:38,915]: Epoch: 120, Loss:0.3837 Train: 0.9250, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:38,921]: Epoch: 121, Loss:0.3745 Train: 0.9000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:38,928]: Epoch: 122, Loss:0.3661 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:38,934]: Epoch: 123, Loss:0.4199 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:38,941]: Epoch: 124, Loss:0.3641 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:38,948]: Epoch: 125, Loss:0.3860 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:38,956]: Epoch: 126, Loss:0.3344 Train: 0.9000, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:38,962]: Epoch: 127, Loss:0.4125 Train: 0.8833, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:38,969]: Epoch: 128, Loss:0.3670 Train: 0.8750, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:38,977]: Epoch: 129, Loss:0.3657 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:38,984]: Epoch: 130, Loss:0.4078 Train: 0.9000, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:38,990]: Epoch: 131, Loss:0.3406 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:38,996]: Epoch: 132, Loss:0.3291 Train: 0.9417, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:39,004]: Epoch: 133, Loss:0.2995 Train: 0.8833, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:39,010]: Epoch: 134, Loss:0.3907 Train: 0.9250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:39,017]: Epoch: 135, Loss:0.3600 Train: 0.9333, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:39,024]: Epoch: 136, Loss:0.2894 Train: 0.9083, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:39,031]: Epoch: 137, Loss:0.3898 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:39,039]: Epoch: 138, Loss:0.4066 Train: 0.9167, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:39,046]: Epoch: 139, Loss:0.3291 Train: 0.8333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:39,055]: Epoch: 140, Loss:0.5877 Train: 0.9500, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:39,063]: Epoch: 141, Loss:0.3756 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:39,070]: Epoch: 142, Loss:0.3683 Train: 0.8667, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:39,077]: Epoch: 143, Loss:0.5228 Train: 0.9417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:39,085]: Epoch: 144, Loss:0.4164 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:39,091]: Epoch: 145, Loss:0.3883 Train: 0.8917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0058
[2025-04-01 02:44:39,098]: Epoch: 146, Loss:0.4725 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:39,105]: Epoch: 147, Loss:0.4182 Train: 0.9250, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:39,112]: Epoch: 148, Loss:0.4080 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:39,118]: Epoch: 149, Loss:0.4111 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:39,125]: Epoch: 150, Loss:0.4645 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:39,132]: Epoch: 151, Loss:0.4317 Train: 0.8667, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:39,139]: Epoch: 152, Loss:0.3831 Train: 0.8417, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:39,146]: Epoch: 153, Loss:0.4351 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:39,154]: Epoch: 154, Loss:0.3642 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:39,160]: Epoch: 155, Loss:0.3712 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:39,167]: Epoch: 156, Loss:0.3831 Train: 0.8833, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:39,175]: Epoch: 157, Loss:0.3654 Train: 0.9167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:39,183]: Epoch: 158, Loss:0.3456 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:39,191]: Epoch: 159, Loss:0.3557 Train: 0.9250, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:39,199]: Epoch: 160, Loss:0.3963 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:39,207]: Epoch: 161, Loss:0.4079 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:39,215]: Epoch: 162, Loss:0.3642 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:39,223]: Epoch: 163, Loss:0.3567 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:39,231]: Epoch: 164, Loss:0.3842 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:39,239]: Epoch: 165, Loss:0.4314 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:39,247]: Epoch: 166, Loss:0.3637 Train: 0.8667, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:39,255]: Epoch: 167, Loss:0.3536 Train: 0.8417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:39,261]: Epoch: 168, Loss:0.4488 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:39,268]: Epoch: 169, Loss:0.3434 Train: 0.9167, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:39,275]: Epoch: 170, Loss:0.3510 Train: 0.9167, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:39,282]: Epoch: 171, Loss:0.3595 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:39,293]: Epoch: 172, Loss:0.3914 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0108
[2025-04-01 02:44:39,302]: Epoch: 173, Loss:0.3698 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:39,311]: Epoch: 174, Loss:0.4150 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:39,319]: Epoch: 175, Loss:0.3706 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:39,329]: Epoch: 176, Loss:0.4098 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0094
[2025-04-01 02:44:39,338]: Epoch: 177, Loss:0.3124 Train: 0.9417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:39,347]: Epoch: 178, Loss:0.3290 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:39,356]: Epoch: 179, Loss:0.3535 Train: 0.9333, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0095
[2025-04-01 02:44:39,365]: Epoch: 180, Loss:0.3445 Train: 0.9333, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0088
[2025-04-01 02:44:39,374]: Epoch: 181, Loss:0.3688 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:39,381]: Epoch: 182, Loss:0.3149 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:39,392]: Epoch: 183, Loss:0.3291 Train: 0.9333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0105
[2025-04-01 02:44:39,400]: Epoch: 184, Loss:0.2906 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:39,408]: Epoch: 185, Loss:0.3636 Train: 0.9333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:39,416]: Epoch: 186, Loss:0.3468 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:39,424]: Epoch: 187, Loss:0.3155 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:39,431]: Epoch: 188, Loss:0.3254 Train: 0.9417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:39,439]: Epoch: 189, Loss:0.3429 Train: 0.9083, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:39,446]: Epoch: 190, Loss:0.3366 Train: 0.9250, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:39,455]: Epoch: 191, Loss:0.3004 Train: 0.9250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:39,462]: Epoch: 192, Loss:0.3326 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:39,469]: Epoch: 193, Loss:0.3292 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:39,477]: Epoch: 194, Loss:0.4472 Train: 0.9000, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:39,484]: Epoch: 195, Loss:0.3466 Train: 0.9000, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0070
[2025-04-01 02:44:39,490]: Epoch: 196, Loss:0.2956 Train: 0.9167, Val:0.4375, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:44:39,498]: Epoch: 197, Loss:0.3611 Train: 0.9417, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:39,506]: Epoch: 198, Loss:0.2910 Train: 0.9500, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:39,515]: Epoch: 199, Loss:0.3001 Train: 0.9167, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0087
[2025-04-01 02:44:39,522]: Epoch: 200, Loss:0.3293 Train: 0.9083, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:39,522]: [Run-2 score] {'train': 0.675, 'val': 0.575, 'test': 0.49019607843137253}
[2025-04-01 02:44:39,522]: repeat 3/3
[2025-04-01 02:44:39,522]: Manual random seed:0
[2025-04-01 02:44:39,523]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:39,525]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:39,535]: Epoch: 001, Loss:1.7084 Train: 0.5917, Val:0.5375, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:39,541]: Epoch: 002, Loss:1.3862 Train: 0.5417, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0057
[2025-04-01 02:44:39,549]: Epoch: 003, Loss:1.6369 Train: 0.5917, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:39,556]: Epoch: 004, Loss:1.0721 Train: 0.4500, Val:0.2625, Test: 0.2549, Time(s/epoch):0.0075
[2025-04-01 02:44:39,563]: Epoch: 005, Loss:1.3008 Train: 0.5417, Val:0.3750, Test: 0.2549, Time(s/epoch):0.0062
[2025-04-01 02:44:39,569]: Epoch: 006, Loss:1.1627 Train: 0.7167, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:39,576]: Epoch: 007, Loss:0.9852 Train: 0.6833, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:39,584]: Epoch: 008, Loss:0.9224 Train: 0.6333, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:39,590]: Epoch: 009, Loss:0.9218 Train: 0.6333, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:39,598]: Epoch: 010, Loss:1.0737 Train: 0.6667, Val:0.5750, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:39,605]: Epoch: 011, Loss:0.8509 Train: 0.6917, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:39,612]: Epoch: 012, Loss:0.9158 Train: 0.7333, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0061
[2025-04-01 02:44:39,618]: Epoch: 013, Loss:0.8306 Train: 0.7333, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:39,625]: Epoch: 014, Loss:0.7744 Train: 0.7333, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:39,632]: Epoch: 015, Loss:0.7746 Train: 0.7250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:39,639]: Epoch: 016, Loss:0.7780 Train: 0.7250, Val:0.5375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:39,646]: Epoch: 017, Loss:0.7115 Train: 0.7500, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:39,654]: Epoch: 018, Loss:0.7103 Train: 0.7750, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:39,661]: Epoch: 019, Loss:0.6704 Train: 0.8000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:39,669]: Epoch: 020, Loss:0.6958 Train: 0.8167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:39,676]: Epoch: 021, Loss:0.6257 Train: 0.8083, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:39,684]: Epoch: 022, Loss:0.6261 Train: 0.8167, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:39,692]: Epoch: 023, Loss:0.6489 Train: 0.8083, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:39,700]: Epoch: 024, Loss:0.6227 Train: 0.8083, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:39,708]: Epoch: 025, Loss:0.6231 Train: 0.8250, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:39,716]: Epoch: 026, Loss:0.6402 Train: 0.8250, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:39,722]: Epoch: 027, Loss:0.5787 Train: 0.8250, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:39,730]: Epoch: 028, Loss:0.5955 Train: 0.8167, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:39,736]: Epoch: 029, Loss:0.5518 Train: 0.8250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:39,742]: Epoch: 030, Loss:0.5636 Train: 0.8333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:39,749]: Epoch: 031, Loss:0.5576 Train: 0.8167, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:39,756]: Epoch: 032, Loss:0.5778 Train: 0.8333, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:39,762]: Epoch: 033, Loss:0.5374 Train: 0.8750, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0057
[2025-04-01 02:44:39,769]: Epoch: 034, Loss:0.5966 Train: 0.8583, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:39,777]: Epoch: 035, Loss:0.5602 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:39,783]: Epoch: 036, Loss:0.5317 Train: 0.8833, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:39,791]: Epoch: 037, Loss:0.4520 Train: 0.8833, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:39,799]: Epoch: 038, Loss:0.5305 Train: 0.8917, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:39,807]: Epoch: 039, Loss:0.5277 Train: 0.8667, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:39,815]: Epoch: 040, Loss:0.5099 Train: 0.8833, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:39,823]: Epoch: 041, Loss:0.4578 Train: 0.8917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:39,830]: Epoch: 042, Loss:0.4624 Train: 0.8833, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:39,838]: Epoch: 043, Loss:0.4822 Train: 0.8833, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:39,846]: Epoch: 044, Loss:0.4487 Train: 0.8750, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:39,854]: Epoch: 045, Loss:0.5083 Train: 0.9000, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0081
[2025-04-01 02:44:39,864]: Epoch: 046, Loss:0.4800 Train: 0.8667, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0090
[2025-04-01 02:44:39,872]: Epoch: 047, Loss:0.4701 Train: 0.8750, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:39,878]: Epoch: 048, Loss:0.5055 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:39,885]: Epoch: 049, Loss:0.4888 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:39,891]: Epoch: 050, Loss:0.4946 Train: 0.8917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:39,898]: Epoch: 051, Loss:0.4561 Train: 0.8333, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:39,906]: Epoch: 052, Loss:0.4835 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:39,912]: Epoch: 053, Loss:0.4921 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:39,922]: Epoch: 054, Loss:0.4736 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:39,932]: Epoch: 055, Loss:0.4452 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0098
[2025-04-01 02:44:39,941]: Epoch: 056, Loss:0.4599 Train: 0.9000, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0087
[2025-04-01 02:44:39,949]: Epoch: 057, Loss:0.4515 Train: 0.8583, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:39,957]: Epoch: 058, Loss:0.4848 Train: 0.8667, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:39,966]: Epoch: 059, Loss:0.4350 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:39,972]: Epoch: 060, Loss:0.4117 Train: 0.9000, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:39,980]: Epoch: 061, Loss:0.4275 Train: 0.9000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:39,987]: Epoch: 062, Loss:0.4089 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:39,995]: Epoch: 063, Loss:0.5064 Train: 0.8917, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:40,003]: Epoch: 064, Loss:0.4791 Train: 0.8917, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:40,010]: Epoch: 065, Loss:0.4379 Train: 0.9000, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:40,018]: Epoch: 066, Loss:0.4109 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:40,026]: Epoch: 067, Loss:0.4393 Train: 0.9000, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:40,032]: Epoch: 068, Loss:0.4200 Train: 0.9000, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:40,041]: Epoch: 069, Loss:0.4093 Train: 0.9167, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:40,048]: Epoch: 070, Loss:0.4009 Train: 0.9000, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:40,055]: Epoch: 071, Loss:0.4331 Train: 0.9167, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:40,063]: Epoch: 072, Loss:0.3677 Train: 0.9000, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:40,069]: Epoch: 073, Loss:0.4225 Train: 0.9000, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:44:40,077]: Epoch: 074, Loss:0.4284 Train: 0.8750, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:40,084]: Epoch: 075, Loss:0.3961 Train: 0.9250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:40,091]: Epoch: 076, Loss:0.4766 Train: 0.8750, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:40,098]: Epoch: 077, Loss:0.3574 Train: 0.8583, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:40,104]: Epoch: 078, Loss:0.5185 Train: 0.9333, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:40,111]: Epoch: 079, Loss:0.3867 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:40,118]: Epoch: 080, Loss:0.4691 Train: 0.9333, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:40,126]: Epoch: 081, Loss:0.4016 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:40,134]: Epoch: 082, Loss:0.4325 Train: 0.8333, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:40,140]: Epoch: 083, Loss:0.4497 Train: 0.9167, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:44:40,148]: Epoch: 084, Loss:0.3783 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:40,156]: Epoch: 085, Loss:0.4234 Train: 0.8417, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:40,162]: Epoch: 086, Loss:0.4887 Train: 0.9167, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0059
[2025-04-01 02:44:40,170]: Epoch: 087, Loss:0.3925 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:40,178]: Epoch: 088, Loss:0.4389 Train: 0.8667, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:40,186]: Epoch: 089, Loss:0.4390 Train: 0.8667, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:40,192]: Epoch: 090, Loss:0.4746 Train: 0.9167, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0057
[2025-04-01 02:44:40,199]: Epoch: 091, Loss:0.4228 Train: 0.9333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:40,207]: Epoch: 092, Loss:0.4527 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:40,215]: Epoch: 093, Loss:0.3931 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:40,223]: Epoch: 094, Loss:0.3966 Train: 0.9250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:40,230]: Epoch: 095, Loss:0.3661 Train: 0.9250, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:40,238]: Epoch: 096, Loss:0.3568 Train: 0.9333, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:40,246]: Epoch: 097, Loss:0.3479 Train: 0.9333, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:40,255]: Epoch: 098, Loss:0.3950 Train: 0.9083, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:40,262]: Epoch: 099, Loss:0.3757 Train: 0.9167, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:40,269]: Epoch: 100, Loss:0.3796 Train: 0.9167, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:40,276]: Epoch: 101, Loss:0.3595 Train: 0.9250, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:40,282]: Epoch: 102, Loss:0.3690 Train: 0.9417, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:40,292]: Epoch: 103, Loss:0.3433 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0096
[2025-04-01 02:44:40,303]: Epoch: 104, Loss:0.3978 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0100
[2025-04-01 02:44:40,310]: Epoch: 105, Loss:0.4124 Train: 0.9250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:40,317]: Epoch: 106, Loss:0.3558 Train: 0.9333, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:40,326]: Epoch: 107, Loss:0.3231 Train: 0.9417, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0092
[2025-04-01 02:44:40,334]: Epoch: 108, Loss:0.3383 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:40,340]: Epoch: 109, Loss:0.3358 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:40,347]: Epoch: 110, Loss:0.3441 Train: 0.9417, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:40,355]: Epoch: 111, Loss:0.3626 Train: 0.9333, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:40,363]: Epoch: 112, Loss:0.3269 Train: 0.9500, Val:0.5250, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:40,371]: Epoch: 113, Loss:0.3662 Train: 0.9333, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:40,379]: Epoch: 114, Loss:0.3453 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:40,386]: Epoch: 115, Loss:0.3915 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:40,392]: Epoch: 116, Loss:0.3921 Train: 0.9083, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:40,399]: Epoch: 117, Loss:0.3077 Train: 0.9000, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:40,406]: Epoch: 118, Loss:0.3749 Train: 0.9083, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:40,414]: Epoch: 119, Loss:0.3312 Train: 0.9083, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:40,422]: Epoch: 120, Loss:0.3433 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:40,430]: Epoch: 121, Loss:0.3650 Train: 0.9167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:40,438]: Epoch: 122, Loss:0.4211 Train: 0.9250, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:40,445]: Epoch: 123, Loss:0.3999 Train: 0.9417, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:40,453]: Epoch: 124, Loss:0.3175 Train: 0.9167, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:40,461]: Epoch: 125, Loss:0.3462 Train: 0.9333, Val:0.5125, Test: 0.5686, Time(s/epoch):0.0080
[2025-04-01 02:44:40,468]: Epoch: 126, Loss:0.3987 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:40,475]: Epoch: 127, Loss:0.3712 Train: 0.8583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:40,483]: Epoch: 128, Loss:0.5521 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:40,491]: Epoch: 129, Loss:0.3360 Train: 0.9333, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:40,498]: Epoch: 130, Loss:0.4192 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:40,507]: Epoch: 131, Loss:0.3701 Train: 0.9250, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:44:40,516]: Epoch: 132, Loss:0.3359 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:40,523]: Epoch: 133, Loss:0.4130 Train: 0.9333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:40,531]: Epoch: 134, Loss:0.3886 Train: 0.9250, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:40,540]: Epoch: 135, Loss:0.3223 Train: 0.9333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:40,548]: Epoch: 136, Loss:0.3415 Train: 0.9250, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:40,556]: Epoch: 137, Loss:0.3641 Train: 0.9333, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:40,563]: Epoch: 138, Loss:0.3227 Train: 0.9333, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:40,570]: Epoch: 139, Loss:0.3514 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:40,577]: Epoch: 140, Loss:0.3742 Train: 0.9250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:40,584]: Epoch: 141, Loss:0.3256 Train: 0.9417, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:40,590]: Epoch: 142, Loss:0.2977 Train: 0.9417, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:40,596]: Epoch: 143, Loss:0.4009 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:40,603]: Epoch: 144, Loss:0.3385 Train: 0.9333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:40,610]: Epoch: 145, Loss:0.3359 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:40,619]: Epoch: 146, Loss:0.3260 Train: 0.9250, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:40,628]: Epoch: 147, Loss:0.3775 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:40,635]: Epoch: 148, Loss:0.3369 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:40,642]: Epoch: 149, Loss:0.3147 Train: 0.9167, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:40,650]: Epoch: 150, Loss:0.3095 Train: 0.9083, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:40,658]: Epoch: 151, Loss:0.3175 Train: 0.9167, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:40,666]: Epoch: 152, Loss:0.2798 Train: 0.9417, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:40,673]: Epoch: 153, Loss:0.3092 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:40,681]: Epoch: 154, Loss:0.3578 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:40,689]: Epoch: 155, Loss:0.3663 Train: 0.9083, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:40,697]: Epoch: 156, Loss:0.3720 Train: 0.8917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:40,705]: Epoch: 157, Loss:0.3762 Train: 0.9417, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:40,713]: Epoch: 158, Loss:0.3332 Train: 0.9333, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:40,721]: Epoch: 159, Loss:0.3825 Train: 0.9333, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:40,728]: Epoch: 160, Loss:0.3614 Train: 0.9417, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:40,736]: Epoch: 161, Loss:0.3053 Train: 0.8583, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:40,743]: Epoch: 162, Loss:0.3721 Train: 0.9000, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:40,750]: Epoch: 163, Loss:0.3410 Train: 0.9167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:40,757]: Epoch: 164, Loss:0.4076 Train: 0.9250, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:40,765]: Epoch: 165, Loss:0.3644 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:40,771]: Epoch: 166, Loss:0.3217 Train: 0.9333, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:40,779]: Epoch: 167, Loss:0.3640 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:40,786]: Epoch: 168, Loss:0.3634 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:40,794]: Epoch: 169, Loss:0.3128 Train: 0.9083, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:40,802]: Epoch: 170, Loss:0.4156 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:40,810]: Epoch: 171, Loss:0.3493 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:40,818]: Epoch: 172, Loss:0.3924 Train: 0.9500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:40,825]: Epoch: 173, Loss:0.3528 Train: 0.8917, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:40,832]: Epoch: 174, Loss:0.3638 Train: 0.9250, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:40,840]: Epoch: 175, Loss:0.3230 Train: 0.9417, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:40,848]: Epoch: 176, Loss:0.3092 Train: 0.9333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:40,856]: Epoch: 177, Loss:0.3558 Train: 0.9500, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:40,864]: Epoch: 178, Loss:0.3529 Train: 0.9250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:40,872]: Epoch: 179, Loss:0.3371 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:40,879]: Epoch: 180, Loss:0.3736 Train: 0.9333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:40,886]: Epoch: 181, Loss:0.3396 Train: 0.9417, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:40,892]: Epoch: 182, Loss:0.3466 Train: 0.9167, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:40,899]: Epoch: 183, Loss:0.3207 Train: 0.9000, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:40,906]: Epoch: 184, Loss:0.3820 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:40,913]: Epoch: 185, Loss:0.3683 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:40,921]: Epoch: 186, Loss:0.3321 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:40,929]: Epoch: 187, Loss:0.4891 Train: 0.9083, Val:0.5125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:40,937]: Epoch: 188, Loss:0.3677 Train: 0.8917, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:40,944]: Epoch: 189, Loss:0.3659 Train: 0.8667, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:40,952]: Epoch: 190, Loss:0.4840 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:40,961]: Epoch: 191, Loss:0.4102 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:40,969]: Epoch: 192, Loss:0.3782 Train: 0.9167, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:40,977]: Epoch: 193, Loss:0.3822 Train: 0.9167, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:40,983]: Epoch: 194, Loss:0.3535 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:40,989]: Epoch: 195, Loss:0.4420 Train: 0.9333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:40,996]: Epoch: 196, Loss:0.3728 Train: 0.9167, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:41,003]: Epoch: 197, Loss:0.3851 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:41,009]: Epoch: 198, Loss:0.4144 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:41,016]: Epoch: 199, Loss:0.4348 Train: 0.9417, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:41,023]: Epoch: 200, Loss:0.3749 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:41,023]: [Run-3 score] {'train': 0.6833333333333333, 'val': 0.575, 'test': 0.49019607843137253}
[2025-04-01 02:44:41,023]: repeat 1/3
[2025-04-01 02:44:41,023]: Manual random seed:0
[2025-04-01 02:44:41,024]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:41,026]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:41,036]: Epoch: 001, Loss:1.7268 Train: 0.4417, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:41,042]: Epoch: 002, Loss:2.0482 Train: 0.4667, Val:0.4500, Test: 0.3529, Time(s/epoch):0.0056
[2025-04-01 02:44:41,049]: Epoch: 003, Loss:1.5547 Train: 0.6083, Val:0.5000, Test: 0.6078, Time(s/epoch):0.0074
[2025-04-01 02:44:41,056]: Epoch: 004, Loss:1.1322 Train: 0.6417, Val:0.5375, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:44:41,064]: Epoch: 005, Loss:1.1245 Train: 0.6083, Val:0.5500, Test: 0.5686, Time(s/epoch):0.0079
[2025-04-01 02:44:41,072]: Epoch: 006, Loss:1.1941 Train: 0.6083, Val:0.5625, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:41,078]: Epoch: 007, Loss:1.1157 Train: 0.6167, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:41,086]: Epoch: 008, Loss:1.0956 Train: 0.6417, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:41,093]: Epoch: 009, Loss:0.9769 Train: 0.6833, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0076
[2025-04-01 02:44:41,100]: Epoch: 010, Loss:0.9135 Train: 0.6917, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:41,107]: Epoch: 011, Loss:0.9432 Train: 0.7083, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:41,115]: Epoch: 012, Loss:0.8840 Train: 0.7083, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:41,121]: Epoch: 013, Loss:0.8375 Train: 0.7000, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:41,129]: Epoch: 014, Loss:0.8202 Train: 0.7250, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:41,136]: Epoch: 015, Loss:0.8673 Train: 0.7500, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:41,143]: Epoch: 016, Loss:0.7523 Train: 0.7667, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:44:41,151]: Epoch: 017, Loss:0.7557 Train: 0.7583, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:41,158]: Epoch: 018, Loss:0.7856 Train: 0.8000, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:41,166]: Epoch: 019, Loss:0.7573 Train: 0.8083, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0074
[2025-04-01 02:44:41,174]: Epoch: 020, Loss:0.7342 Train: 0.7333, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:41,180]: Epoch: 021, Loss:0.6673 Train: 0.7167, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:44:41,188]: Epoch: 022, Loss:0.6899 Train: 0.7250, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:41,196]: Epoch: 023, Loss:0.6840 Train: 0.7167, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:41,203]: Epoch: 024, Loss:0.6939 Train: 0.7500, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:41,210]: Epoch: 025, Loss:0.6723 Train: 0.7917, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:41,218]: Epoch: 026, Loss:0.6581 Train: 0.8417, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:41,225]: Epoch: 027, Loss:0.6268 Train: 0.8167, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:41,232]: Epoch: 028, Loss:0.5894 Train: 0.8250, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0061
[2025-04-01 02:44:41,239]: Epoch: 029, Loss:0.7090 Train: 0.8500, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:41,245]: Epoch: 030, Loss:0.6151 Train: 0.8583, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:41,253]: Epoch: 031, Loss:0.6476 Train: 0.8333, Val:0.5125, Test: 0.5686, Time(s/epoch):0.0073
[2025-04-01 02:44:41,261]: Epoch: 032, Loss:0.5866 Train: 0.8250, Val:0.4875, Test: 0.5686, Time(s/epoch):0.0081
[2025-04-01 02:44:41,269]: Epoch: 033, Loss:0.5860 Train: 0.8167, Val:0.4625, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:44:41,277]: Epoch: 034, Loss:0.5751 Train: 0.8417, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:41,284]: Epoch: 035, Loss:0.5537 Train: 0.8500, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:41,295]: Epoch: 036, Loss:0.5897 Train: 0.8667, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0099
[2025-04-01 02:44:41,302]: Epoch: 037, Loss:0.5250 Train: 0.8917, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:41,309]: Epoch: 038, Loss:0.5320 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:41,317]: Epoch: 039, Loss:0.5534 Train: 0.8583, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:41,326]: Epoch: 040, Loss:0.5904 Train: 0.8750, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0083
[2025-04-01 02:44:41,334]: Epoch: 041, Loss:0.5474 Train: 0.8750, Val:0.4625, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:44:41,341]: Epoch: 042, Loss:0.5618 Train: 0.8500, Val:0.4625, Test: 0.5686, Time(s/epoch):0.0072
[2025-04-01 02:44:41,348]: Epoch: 043, Loss:0.5568 Train: 0.8667, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:41,356]: Epoch: 044, Loss:0.5133 Train: 0.8750, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:41,363]: Epoch: 045, Loss:0.5265 Train: 0.8667, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:41,370]: Epoch: 046, Loss:0.5489 Train: 0.8750, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:41,379]: Epoch: 047, Loss:0.4966 Train: 0.8583, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0088
[2025-04-01 02:44:41,388]: Epoch: 048, Loss:0.5503 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:44:41,395]: Epoch: 049, Loss:0.4830 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:41,402]: Epoch: 050, Loss:0.5578 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:41,411]: Epoch: 051, Loss:0.6300 Train: 0.8750, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:41,420]: Epoch: 052, Loss:0.4937 Train: 0.8333, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0086
[2025-04-01 02:44:41,428]: Epoch: 053, Loss:0.5399 Train: 0.8667, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:44:41,437]: Epoch: 054, Loss:0.4619 Train: 0.8833, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0092
[2025-04-01 02:44:41,446]: Epoch: 055, Loss:0.4147 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0089
[2025-04-01 02:44:41,454]: Epoch: 056, Loss:0.5111 Train: 0.8750, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:41,462]: Epoch: 057, Loss:0.4410 Train: 0.8667, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:41,470]: Epoch: 058, Loss:0.4964 Train: 0.8500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:41,479]: Epoch: 059, Loss:0.4450 Train: 0.8750, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:41,488]: Epoch: 060, Loss:0.4388 Train: 0.8500, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:44:41,495]: Epoch: 061, Loss:0.4932 Train: 0.8167, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:41,502]: Epoch: 062, Loss:0.5438 Train: 0.8667, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:41,510]: Epoch: 063, Loss:0.4432 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:41,517]: Epoch: 064, Loss:0.5389 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:41,524]: Epoch: 065, Loss:0.4462 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:41,532]: Epoch: 066, Loss:0.4935 Train: 0.8667, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:41,540]: Epoch: 067, Loss:0.5318 Train: 0.8750, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:41,548]: Epoch: 068, Loss:0.5010 Train: 0.8500, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:41,554]: Epoch: 069, Loss:0.4803 Train: 0.8750, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0061
[2025-04-01 02:44:41,561]: Epoch: 070, Loss:0.4808 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:41,569]: Epoch: 071, Loss:0.4707 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:41,576]: Epoch: 072, Loss:0.4336 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:41,584]: Epoch: 073, Loss:0.4568 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:41,590]: Epoch: 074, Loss:0.4136 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:41,597]: Epoch: 075, Loss:0.3921 Train: 0.8833, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:41,605]: Epoch: 076, Loss:0.4455 Train: 0.9000, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:41,612]: Epoch: 077, Loss:0.4390 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:41,620]: Epoch: 078, Loss:0.4124 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:41,627]: Epoch: 079, Loss:0.3879 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:41,635]: Epoch: 080, Loss:0.4453 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:41,643]: Epoch: 081, Loss:0.4153 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:41,650]: Epoch: 082, Loss:0.4467 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:41,659]: Epoch: 083, Loss:0.3922 Train: 0.8917, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:41,666]: Epoch: 084, Loss:0.3876 Train: 0.9083, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:41,673]: Epoch: 085, Loss:0.4100 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:41,680]: Epoch: 086, Loss:0.5220 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:41,688]: Epoch: 087, Loss:0.3736 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:41,696]: Epoch: 088, Loss:0.4049 Train: 0.8500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:41,704]: Epoch: 089, Loss:0.5509 Train: 0.8750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:41,711]: Epoch: 090, Loss:0.4216 Train: 0.8750, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:41,718]: Epoch: 091, Loss:0.4502 Train: 0.8667, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:41,726]: Epoch: 092, Loss:0.5688 Train: 0.8583, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:41,732]: Epoch: 093, Loss:0.4347 Train: 0.8333, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0061
[2025-04-01 02:44:41,741]: Epoch: 094, Loss:0.5416 Train: 0.8500, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:41,749]: Epoch: 095, Loss:0.4467 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:41,757]: Epoch: 096, Loss:0.5256 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:41,765]: Epoch: 097, Loss:0.4454 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:41,771]: Epoch: 098, Loss:0.4186 Train: 0.9083, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:41,779]: Epoch: 099, Loss:0.4472 Train: 0.8667, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:41,787]: Epoch: 100, Loss:0.4894 Train: 0.8500, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:41,793]: Epoch: 101, Loss:0.4332 Train: 0.8417, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:41,801]: Epoch: 102, Loss:0.4707 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:41,809]: Epoch: 103, Loss:0.5742 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:41,816]: Epoch: 104, Loss:0.4343 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:41,825]: Epoch: 105, Loss:0.4497 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:41,833]: Epoch: 106, Loss:0.4490 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:41,840]: Epoch: 107, Loss:0.4617 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:41,847]: Epoch: 108, Loss:0.4577 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:41,855]: Epoch: 109, Loss:0.4334 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:41,862]: Epoch: 110, Loss:0.4681 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:41,870]: Epoch: 111, Loss:0.4269 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:41,879]: Epoch: 112, Loss:0.4217 Train: 0.9250, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:41,887]: Epoch: 113, Loss:0.4147 Train: 0.9250, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:41,895]: Epoch: 114, Loss:0.3965 Train: 0.9250, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:41,901]: Epoch: 115, Loss:0.4179 Train: 0.9250, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:41,909]: Epoch: 116, Loss:0.3816 Train: 0.9250, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:41,917]: Epoch: 117, Loss:0.5198 Train: 0.9250, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:41,925]: Epoch: 118, Loss:0.4341 Train: 0.9167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:41,933]: Epoch: 119, Loss:0.4229 Train: 0.9167, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:41,940]: Epoch: 120, Loss:0.4120 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:41,948]: Epoch: 121, Loss:0.4808 Train: 0.9417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:41,956]: Epoch: 122, Loss:0.3305 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:41,964]: Epoch: 123, Loss:0.5078 Train: 0.9083, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:41,972]: Epoch: 124, Loss:0.3802 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:41,980]: Epoch: 125, Loss:0.4087 Train: 0.9167, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:41,988]: Epoch: 126, Loss:0.3645 Train: 0.9167, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:41,994]: Epoch: 127, Loss:0.3752 Train: 0.9167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:42,001]: Epoch: 128, Loss:0.3286 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:42,009]: Epoch: 129, Loss:0.3584 Train: 0.9000, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:42,017]: Epoch: 130, Loss:0.5001 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:42,023]: Epoch: 131, Loss:0.4021 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0056
[2025-04-01 02:44:42,029]: Epoch: 132, Loss:0.4104 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:42,036]: Epoch: 133, Loss:0.4151 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:42,043]: Epoch: 134, Loss:0.4726 Train: 0.9167, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:42,050]: Epoch: 135, Loss:0.4110 Train: 0.9000, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:42,058]: Epoch: 136, Loss:0.4126 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:42,066]: Epoch: 137, Loss:0.3643 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:42,072]: Epoch: 138, Loss:0.4210 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:42,080]: Epoch: 139, Loss:0.3626 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:42,087]: Epoch: 140, Loss:0.4024 Train: 0.9333, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:42,096]: Epoch: 141, Loss:0.3475 Train: 0.9083, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:42,102]: Epoch: 142, Loss:0.3738 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:42,110]: Epoch: 143, Loss:0.3552 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:42,118]: Epoch: 144, Loss:0.3786 Train: 0.9250, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:42,125]: Epoch: 145, Loss:0.4251 Train: 0.9250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:42,133]: Epoch: 146, Loss:0.3875 Train: 0.9250, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:42,139]: Epoch: 147, Loss:0.4158 Train: 0.9167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:42,147]: Epoch: 148, Loss:0.4107 Train: 0.9167, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:42,156]: Epoch: 149, Loss:0.4021 Train: 0.9333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:42,163]: Epoch: 150, Loss:0.3931 Train: 0.9333, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:42,170]: Epoch: 151, Loss:0.4026 Train: 0.9333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:42,178]: Epoch: 152, Loss:0.3278 Train: 0.9333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:42,184]: Epoch: 153, Loss:0.3183 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:42,192]: Epoch: 154, Loss:0.3270 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:42,200]: Epoch: 155, Loss:0.3007 Train: 0.9250, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:42,208]: Epoch: 156, Loss:0.3526 Train: 0.9250, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:42,216]: Epoch: 157, Loss:0.3340 Train: 0.9333, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:42,222]: Epoch: 158, Loss:0.3875 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:42,229]: Epoch: 159, Loss:0.3499 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:42,237]: Epoch: 160, Loss:0.3742 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:42,245]: Epoch: 161, Loss:0.3111 Train: 0.8833, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:42,253]: Epoch: 162, Loss:0.5419 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:42,261]: Epoch: 163, Loss:0.3967 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:42,269]: Epoch: 164, Loss:0.4553 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:42,277]: Epoch: 165, Loss:0.3606 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:42,285]: Epoch: 166, Loss:0.4016 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:42,292]: Epoch: 167, Loss:0.4180 Train: 0.9167, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:42,300]: Epoch: 168, Loss:0.3701 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:42,311]: Epoch: 169, Loss:0.3998 Train: 0.9167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0109
[2025-04-01 02:44:42,323]: Epoch: 170, Loss:0.4056 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0120
[2025-04-01 02:44:42,335]: Epoch: 171, Loss:0.4325 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0113
[2025-04-01 02:44:42,344]: Epoch: 172, Loss:0.4071 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:42,352]: Epoch: 173, Loss:0.3569 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:42,361]: Epoch: 174, Loss:0.3897 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:42,370]: Epoch: 175, Loss:0.3605 Train: 0.9250, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0091
[2025-04-01 02:44:42,378]: Epoch: 176, Loss:0.4059 Train: 0.9417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:42,386]: Epoch: 177, Loss:0.4139 Train: 0.9417, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:42,394]: Epoch: 178, Loss:0.4181 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:42,401]: Epoch: 179, Loss:0.3864 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:42,410]: Epoch: 180, Loss:0.3749 Train: 0.9417, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:42,417]: Epoch: 181, Loss:0.4250 Train: 0.9417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:42,424]: Epoch: 182, Loss:0.3526 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:42,431]: Epoch: 183, Loss:0.3532 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:42,438]: Epoch: 184, Loss:0.4012 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:42,445]: Epoch: 185, Loss:0.3247 Train: 0.9333, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:42,452]: Epoch: 186, Loss:0.3347 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:42,462]: Epoch: 187, Loss:0.3168 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0098
[2025-04-01 02:44:42,470]: Epoch: 188, Loss:0.3463 Train: 0.9333, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:42,478]: Epoch: 189, Loss:0.3098 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:42,486]: Epoch: 190, Loss:0.3853 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:42,492]: Epoch: 191, Loss:0.3554 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:42,500]: Epoch: 192, Loss:0.3644 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:42,507]: Epoch: 193, Loss:0.3536 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:42,513]: Epoch: 194, Loss:0.3365 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:42,522]: Epoch: 195, Loss:0.3662 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:42,529]: Epoch: 196, Loss:0.3775 Train: 0.9167, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:42,536]: Epoch: 197, Loss:0.3775 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:42,543]: Epoch: 198, Loss:0.4310 Train: 0.8917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:42,550]: Epoch: 199, Loss:0.3916 Train: 0.9083, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:42,557]: Epoch: 200, Loss:0.4545 Train: 0.9417, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:42,557]: [Run-1 score] {'train': 0.6083333333333333, 'val': 0.5625, 'test': 0.5294117647058824}
[2025-04-01 02:44:42,557]: repeat 2/3
[2025-04-01 02:44:42,557]: Manual random seed:0
[2025-04-01 02:44:42,557]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:42,560]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:42,571]: Epoch: 001, Loss:1.6512 Train: 0.5250, Val:0.5625, Test: 0.5490, Time(s/epoch):0.0068
[2025-04-01 02:44:42,577]: Epoch: 002, Loss:1.8081 Train: 0.2917, Val:0.2750, Test: 0.2745, Time(s/epoch):0.0058
[2025-04-01 02:44:42,584]: Epoch: 003, Loss:1.7561 Train: 0.5000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:42,590]: Epoch: 004, Loss:1.1955 Train: 0.6333, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:42,598]: Epoch: 005, Loss:1.0363 Train: 0.6083, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:44:42,606]: Epoch: 006, Loss:1.1387 Train: 0.5917, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0078
[2025-04-01 02:44:42,612]: Epoch: 007, Loss:1.1127 Train: 0.6083, Val:0.5125, Test: 0.5686, Time(s/epoch):0.0058
[2025-04-01 02:44:42,619]: Epoch: 008, Loss:1.1799 Train: 0.6583, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:42,626]: Epoch: 009, Loss:0.9974 Train: 0.6667, Val:0.5250, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:42,632]: Epoch: 010, Loss:0.9673 Train: 0.7250, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:44:42,639]: Epoch: 011, Loss:0.8881 Train: 0.7583, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0064
[2025-04-01 02:44:42,647]: Epoch: 012, Loss:0.8956 Train: 0.7500, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:42,655]: Epoch: 013, Loss:0.9991 Train: 0.7167, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:42,661]: Epoch: 014, Loss:0.9039 Train: 0.7500, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0058
[2025-04-01 02:44:42,667]: Epoch: 015, Loss:0.8322 Train: 0.7250, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0066
[2025-04-01 02:44:42,674]: Epoch: 016, Loss:0.8336 Train: 0.7083, Val:0.5000, Test: 0.5686, Time(s/epoch):0.0064
[2025-04-01 02:44:42,681]: Epoch: 017, Loss:0.7790 Train: 0.6833, Val:0.5125, Test: 0.5686, Time(s/epoch):0.0068
[2025-04-01 02:44:42,687]: Epoch: 018, Loss:0.7565 Train: 0.6917, Val:0.4875, Test: 0.5686, Time(s/epoch):0.0063
[2025-04-01 02:44:42,695]: Epoch: 019, Loss:0.7983 Train: 0.7000, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:44:42,703]: Epoch: 020, Loss:0.7668 Train: 0.7417, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:42,710]: Epoch: 021, Loss:0.7559 Train: 0.7917, Val:0.5250, Test: 0.5490, Time(s/epoch):0.0062
[2025-04-01 02:44:42,717]: Epoch: 022, Loss:0.7443 Train: 0.8083, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:42,724]: Epoch: 023, Loss:0.6853 Train: 0.7917, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:42,733]: Epoch: 024, Loss:0.6657 Train: 0.8167, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0082
[2025-04-01 02:44:42,740]: Epoch: 025, Loss:0.7042 Train: 0.8333, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:42,749]: Epoch: 026, Loss:0.7025 Train: 0.8417, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0080
[2025-04-01 02:44:42,757]: Epoch: 027, Loss:0.6928 Train: 0.8167, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:42,763]: Epoch: 028, Loss:0.6311 Train: 0.7917, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0063
[2025-04-01 02:44:42,771]: Epoch: 029, Loss:0.6584 Train: 0.7417, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:42,779]: Epoch: 030, Loss:0.6514 Train: 0.7500, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:42,787]: Epoch: 031, Loss:0.6170 Train: 0.7750, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:42,795]: Epoch: 032, Loss:0.6104 Train: 0.8000, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0077
[2025-04-01 02:44:42,803]: Epoch: 033, Loss:0.6321 Train: 0.8250, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:42,811]: Epoch: 034, Loss:0.5566 Train: 0.8583, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:42,818]: Epoch: 035, Loss:0.6127 Train: 0.8667, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:42,825]: Epoch: 036, Loss:0.6065 Train: 0.8750, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:42,831]: Epoch: 037, Loss:0.5676 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:42,839]: Epoch: 038, Loss:0.5733 Train: 0.8500, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:42,847]: Epoch: 039, Loss:0.5933 Train: 0.8500, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:42,853]: Epoch: 040, Loss:0.5650 Train: 0.8333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0061
[2025-04-01 02:44:42,860]: Epoch: 041, Loss:0.5215 Train: 0.7917, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:42,867]: Epoch: 042, Loss:0.5613 Train: 0.7917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:42,875]: Epoch: 043, Loss:0.6114 Train: 0.7917, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:42,883]: Epoch: 044, Loss:0.6474 Train: 0.8583, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:42,891]: Epoch: 045, Loss:0.5744 Train: 0.8667, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:42,898]: Epoch: 046, Loss:0.5133 Train: 0.8667, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:42,904]: Epoch: 047, Loss:0.5583 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:42,911]: Epoch: 048, Loss:0.5987 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:42,917]: Epoch: 049, Loss:0.4944 Train: 0.7833, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:42,925]: Epoch: 050, Loss:0.5467 Train: 0.7833, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:42,932]: Epoch: 051, Loss:0.6515 Train: 0.8417, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:42,940]: Epoch: 052, Loss:0.5405 Train: 0.8833, Val:0.4875, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:42,947]: Epoch: 053, Loss:0.5153 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:42,953]: Epoch: 054, Loss:0.5441 Train: 0.8583, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:42,961]: Epoch: 055, Loss:0.6436 Train: 0.8667, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:42,969]: Epoch: 056, Loss:0.4930 Train: 0.8750, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:42,977]: Epoch: 057, Loss:0.5352 Train: 0.8167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:42,984]: Epoch: 058, Loss:0.5484 Train: 0.8583, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:42,991]: Epoch: 059, Loss:0.4868 Train: 0.8917, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:42,999]: Epoch: 060, Loss:0.5254 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:43,007]: Epoch: 061, Loss:0.4872 Train: 0.8750, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:43,014]: Epoch: 062, Loss:0.4890 Train: 0.8750, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:43,020]: Epoch: 063, Loss:0.5516 Train: 0.8417, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0059
[2025-04-01 02:44:43,028]: Epoch: 064, Loss:0.5054 Train: 0.8583, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:43,036]: Epoch: 065, Loss:0.4767 Train: 0.8583, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:43,042]: Epoch: 066, Loss:0.4707 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0055
[2025-04-01 02:44:43,049]: Epoch: 067, Loss:0.4554 Train: 0.8667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:43,056]: Epoch: 068, Loss:0.4914 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:43,062]: Epoch: 069, Loss:0.4676 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:43,069]: Epoch: 070, Loss:0.4622 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:43,076]: Epoch: 071, Loss:0.4684 Train: 0.8917, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:43,083]: Epoch: 072, Loss:0.4293 Train: 0.8667, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:43,089]: Epoch: 073, Loss:0.4421 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:43,096]: Epoch: 074, Loss:0.4981 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:43,102]: Epoch: 075, Loss:0.4578 Train: 0.8917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:43,108]: Epoch: 076, Loss:0.5265 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:43,115]: Epoch: 077, Loss:0.4356 Train: 0.9083, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:43,123]: Epoch: 078, Loss:0.4174 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,130]: Epoch: 079, Loss:0.5217 Train: 0.9000, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:43,138]: Epoch: 080, Loss:0.3901 Train: 0.9000, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0076
[2025-04-01 02:44:43,145]: Epoch: 081, Loss:0.4838 Train: 0.8917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:43,153]: Epoch: 082, Loss:0.5082 Train: 0.8500, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,161]: Epoch: 083, Loss:0.5614 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:43,169]: Epoch: 084, Loss:0.4494 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:43,176]: Epoch: 085, Loss:0.4966 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:43,182]: Epoch: 086, Loss:0.4955 Train: 0.9167, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0057
[2025-04-01 02:44:43,190]: Epoch: 087, Loss:0.4532 Train: 0.9083, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:43,198]: Epoch: 088, Loss:0.4245 Train: 0.8750, Val:0.4625, Test: 0.5686, Time(s/epoch):0.0077
[2025-04-01 02:44:43,204]: Epoch: 089, Loss:0.4691 Train: 0.9000, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0065
[2025-04-01 02:44:43,211]: Epoch: 090, Loss:0.4905 Train: 0.8917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:43,218]: Epoch: 091, Loss:0.4572 Train: 0.8917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:43,226]: Epoch: 092, Loss:0.5053 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:43,234]: Epoch: 093, Loss:0.4177 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:43,241]: Epoch: 094, Loss:0.3806 Train: 0.8750, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:43,249]: Epoch: 095, Loss:0.4279 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:43,257]: Epoch: 096, Loss:0.4377 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:43,265]: Epoch: 097, Loss:0.4250 Train: 0.9333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:43,272]: Epoch: 098, Loss:0.4161 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:43,281]: Epoch: 099, Loss:0.4173 Train: 0.9333, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:43,291]: Epoch: 100, Loss:0.4234 Train: 0.9167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0100
[2025-04-01 02:44:43,302]: Epoch: 101, Loss:0.3730 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0113
[2025-04-01 02:44:43,311]: Epoch: 102, Loss:0.3773 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:43,320]: Epoch: 103, Loss:0.4423 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:43,329]: Epoch: 104, Loss:0.3264 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:43,337]: Epoch: 105, Loss:0.3841 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:43,345]: Epoch: 106, Loss:0.3707 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:43,352]: Epoch: 107, Loss:0.4041 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:43,360]: Epoch: 108, Loss:0.3696 Train: 0.9250, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:43,369]: Epoch: 109, Loss:0.4013 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:43,377]: Epoch: 110, Loss:0.3112 Train: 0.8417, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,385]: Epoch: 111, Loss:0.4258 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:43,391]: Epoch: 112, Loss:0.4064 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0057
[2025-04-01 02:44:43,398]: Epoch: 113, Loss:0.3816 Train: 0.9250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:43,405]: Epoch: 114, Loss:0.4548 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:43,414]: Epoch: 115, Loss:0.3142 Train: 0.9083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:43,422]: Epoch: 116, Loss:0.4059 Train: 0.8583, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,429]: Epoch: 117, Loss:0.3842 Train: 0.8667, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:43,436]: Epoch: 118, Loss:0.4144 Train: 0.8667, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:43,444]: Epoch: 119, Loss:0.4287 Train: 0.8833, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:43,451]: Epoch: 120, Loss:0.4356 Train: 0.8917, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:43,459]: Epoch: 121, Loss:0.4417 Train: 0.8583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:43,469]: Epoch: 122, Loss:0.4627 Train: 0.8833, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0093
[2025-04-01 02:44:43,477]: Epoch: 123, Loss:0.4326 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:43,485]: Epoch: 124, Loss:0.3823 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:43,494]: Epoch: 125, Loss:0.4592 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:43,502]: Epoch: 126, Loss:0.4976 Train: 0.9250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:43,511]: Epoch: 127, Loss:0.4382 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:43,520]: Epoch: 128, Loss:0.4016 Train: 0.8750, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:43,529]: Epoch: 129, Loss:0.4619 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0090
[2025-04-01 02:44:43,538]: Epoch: 130, Loss:0.4905 Train: 0.9083, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:43,545]: Epoch: 131, Loss:0.4881 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:43,552]: Epoch: 132, Loss:0.4484 Train: 0.9250, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:43,562]: Epoch: 133, Loss:0.3407 Train: 0.9333, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0094
[2025-04-01 02:44:43,569]: Epoch: 134, Loss:0.4429 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:43,577]: Epoch: 135, Loss:0.3835 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:43,585]: Epoch: 136, Loss:0.4658 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:43,593]: Epoch: 137, Loss:0.4804 Train: 0.9083, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:43,600]: Epoch: 138, Loss:0.3845 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:43,608]: Epoch: 139, Loss:0.3842 Train: 0.8917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:43,616]: Epoch: 140, Loss:0.4931 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:43,624]: Epoch: 141, Loss:0.4247 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:43,631]: Epoch: 142, Loss:0.4504 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:43,639]: Epoch: 143, Loss:0.3959 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:43,647]: Epoch: 144, Loss:0.3752 Train: 0.9333, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:43,654]: Epoch: 145, Loss:0.3681 Train: 0.9333, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:43,661]: Epoch: 146, Loss:0.4192 Train: 0.9333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:43,668]: Epoch: 147, Loss:0.4285 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:43,676]: Epoch: 148, Loss:0.3886 Train: 0.8917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:43,683]: Epoch: 149, Loss:0.4005 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:43,690]: Epoch: 150, Loss:0.4067 Train: 0.9167, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:43,698]: Epoch: 151, Loss:0.4372 Train: 0.9333, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:43,704]: Epoch: 152, Loss:0.3916 Train: 0.9333, Val:0.4875, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:43,711]: Epoch: 153, Loss:0.4606 Train: 0.9417, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:43,718]: Epoch: 154, Loss:0.3416 Train: 0.9333, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:43,726]: Epoch: 155, Loss:0.3115 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:43,733]: Epoch: 156, Loss:0.4177 Train: 0.8583, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:43,741]: Epoch: 157, Loss:0.5621 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:43,748]: Epoch: 158, Loss:0.3062 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:43,755]: Epoch: 159, Loss:0.4125 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:43,762]: Epoch: 160, Loss:0.4874 Train: 0.9000, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:43,768]: Epoch: 161, Loss:0.4190 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:43,775]: Epoch: 162, Loss:0.3572 Train: 0.9000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:43,781]: Epoch: 163, Loss:0.3548 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:43,789]: Epoch: 164, Loss:0.3734 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:43,797]: Epoch: 165, Loss:0.4269 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:43,805]: Epoch: 166, Loss:0.3926 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:43,812]: Epoch: 167, Loss:0.3621 Train: 0.9417, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:43,819]: Epoch: 168, Loss:0.4073 Train: 0.9500, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:43,826]: Epoch: 169, Loss:0.3383 Train: 0.9250, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:43,833]: Epoch: 170, Loss:0.3931 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:43,840]: Epoch: 171, Loss:0.3873 Train: 0.9167, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:43,849]: Epoch: 172, Loss:0.3556 Train: 0.8917, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:43,856]: Epoch: 173, Loss:0.4303 Train: 0.9250, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:43,864]: Epoch: 174, Loss:0.4354 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:43,872]: Epoch: 175, Loss:0.3966 Train: 0.8750, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:43,880]: Epoch: 176, Loss:0.4756 Train: 0.9250, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,886]: Epoch: 177, Loss:0.3271 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:43,893]: Epoch: 178, Loss:0.3444 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:43,900]: Epoch: 179, Loss:0.3549 Train: 0.9167, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:43,908]: Epoch: 180, Loss:0.3873 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:43,916]: Epoch: 181, Loss:0.3111 Train: 0.8833, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:43,924]: Epoch: 182, Loss:0.5036 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:43,932]: Epoch: 183, Loss:0.3877 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:43,939]: Epoch: 184, Loss:0.3623 Train: 0.9167, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:43,949]: Epoch: 185, Loss:0.4403 Train: 0.9333, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0091
[2025-04-01 02:44:43,957]: Epoch: 186, Loss:0.2942 Train: 0.9333, Val:0.4750, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:43,965]: Epoch: 187, Loss:0.3351 Train: 0.9250, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:43,973]: Epoch: 188, Loss:0.3578 Train: 0.8917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:43,981]: Epoch: 189, Loss:0.3964 Train: 0.9417, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:43,990]: Epoch: 190, Loss:0.3419 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0090
[2025-04-01 02:44:43,997]: Epoch: 191, Loss:0.4132 Train: 0.9167, Val:0.4125, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:44,006]: Epoch: 192, Loss:0.4261 Train: 0.9333, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:44,014]: Epoch: 193, Loss:0.3483 Train: 0.9250, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:44,022]: Epoch: 194, Loss:0.3421 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:44,030]: Epoch: 195, Loss:0.4347 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:44,038]: Epoch: 196, Loss:0.3366 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:44,046]: Epoch: 197, Loss:0.3711 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:44,053]: Epoch: 198, Loss:0.3340 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:44,062]: Epoch: 199, Loss:0.4065 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:44,070]: Epoch: 200, Loss:0.3921 Train: 0.9167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:44,070]: [Run-2 score] {'train': 0.525, 'val': 0.5625, 'test': 0.5490196078431373}
[2025-04-01 02:44:44,070]: repeat 3/3
[2025-04-01 02:44:44,070]: Manual random seed:0
[2025-04-01 02:44:44,070]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:44,073]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:44,082]: Epoch: 001, Loss:1.6929 Train: 0.5583, Val:0.5625, Test: 0.5490, Time(s/epoch):0.0071
[2025-04-01 02:44:44,089]: Epoch: 002, Loss:1.3159 Train: 0.5917, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:44,096]: Epoch: 003, Loss:1.4485 Train: 0.6000, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:44,104]: Epoch: 004, Loss:1.1733 Train: 0.6000, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:44,112]: Epoch: 005, Loss:1.1834 Train: 0.6750, Val:0.4625, Test: 0.5882, Time(s/epoch):0.0078
[2025-04-01 02:44:44,120]: Epoch: 006, Loss:1.1475 Train: 0.6667, Val:0.4625, Test: 0.5490, Time(s/epoch):0.0076
[2025-04-01 02:44:44,127]: Epoch: 007, Loss:0.9869 Train: 0.6167, Val:0.5125, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:44,134]: Epoch: 008, Loss:0.9949 Train: 0.6250, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:44,140]: Epoch: 009, Loss:0.9455 Train: 0.6667, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0057
[2025-04-01 02:44:44,148]: Epoch: 010, Loss:0.9445 Train: 0.7000, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:44,156]: Epoch: 011, Loss:0.8927 Train: 0.7250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:44,164]: Epoch: 012, Loss:0.8834 Train: 0.7000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:44,170]: Epoch: 013, Loss:0.8019 Train: 0.7333, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0057
[2025-04-01 02:44:44,176]: Epoch: 014, Loss:0.7326 Train: 0.7500, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:44,184]: Epoch: 015, Loss:0.7845 Train: 0.7500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:44,191]: Epoch: 016, Loss:0.8139 Train: 0.7667, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:44,199]: Epoch: 017, Loss:0.7294 Train: 0.7917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:44,206]: Epoch: 018, Loss:0.7282 Train: 0.7833, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:44,215]: Epoch: 019, Loss:0.7006 Train: 0.7833, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:44,220]: Epoch: 020, Loss:0.6703 Train: 0.8083, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0056
[2025-04-01 02:44:44,227]: Epoch: 021, Loss:0.6244 Train: 0.7917, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:44,235]: Epoch: 022, Loss:0.5828 Train: 0.8000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:44,242]: Epoch: 023, Loss:0.6445 Train: 0.8167, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:44,248]: Epoch: 024, Loss:0.6509 Train: 0.8250, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:44,255]: Epoch: 025, Loss:0.6478 Train: 0.8083, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:44,264]: Epoch: 026, Loss:0.5689 Train: 0.8000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:44,270]: Epoch: 027, Loss:0.5952 Train: 0.8167, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:44,279]: Epoch: 028, Loss:0.6038 Train: 0.8250, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0083
[2025-04-01 02:44:44,287]: Epoch: 029, Loss:0.5274 Train: 0.8250, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:44,295]: Epoch: 030, Loss:0.5523 Train: 0.8333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:44,301]: Epoch: 031, Loss:0.5607 Train: 0.8333, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:44:44,312]: Epoch: 032, Loss:0.5596 Train: 0.8500, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0113
[2025-04-01 02:44:44,323]: Epoch: 033, Loss:0.5973 Train: 0.8500, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0100
[2025-04-01 02:44:44,330]: Epoch: 034, Loss:0.5852 Train: 0.8750, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0071
[2025-04-01 02:44:44,337]: Epoch: 035, Loss:0.5786 Train: 0.8667, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:44,344]: Epoch: 036, Loss:0.5515 Train: 0.8500, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:44,351]: Epoch: 037, Loss:0.5220 Train: 0.8750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:44,359]: Epoch: 038, Loss:0.5012 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:44,368]: Epoch: 039, Loss:0.5141 Train: 0.8750, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0088
[2025-04-01 02:44:44,377]: Epoch: 040, Loss:0.5202 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:44,383]: Epoch: 041, Loss:0.4904 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:44,390]: Epoch: 042, Loss:0.4571 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:44,399]: Epoch: 043, Loss:0.4910 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0082
[2025-04-01 02:44:44,407]: Epoch: 044, Loss:0.4918 Train: 0.9000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:44,414]: Epoch: 045, Loss:0.5047 Train: 0.8667, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:44,422]: Epoch: 046, Loss:0.5370 Train: 0.8917, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:44,430]: Epoch: 047, Loss:0.5059 Train: 0.8833, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:44,438]: Epoch: 048, Loss:0.5033 Train: 0.8583, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:44,445]: Epoch: 049, Loss:0.4472 Train: 0.8917, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:44,451]: Epoch: 050, Loss:0.5857 Train: 0.8917, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:44:44,459]: Epoch: 051, Loss:0.4331 Train: 0.8833, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:44,468]: Epoch: 052, Loss:0.4138 Train: 0.9250, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:44,475]: Epoch: 053, Loss:0.4368 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:44,482]: Epoch: 054, Loss:0.4955 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:44,491]: Epoch: 055, Loss:0.5042 Train: 0.9167, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:44,499]: Epoch: 056, Loss:0.4352 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:44,507]: Epoch: 057, Loss:0.4594 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:44,515]: Epoch: 058, Loss:0.4388 Train: 0.9000, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:44,522]: Epoch: 059, Loss:0.4006 Train: 0.9167, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:44,530]: Epoch: 060, Loss:0.4798 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:44,538]: Epoch: 061, Loss:0.4202 Train: 0.9000, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:44,545]: Epoch: 062, Loss:0.3897 Train: 0.8833, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:44,553]: Epoch: 063, Loss:0.4258 Train: 0.8833, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:44,561]: Epoch: 064, Loss:0.4902 Train: 0.8917, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:44,569]: Epoch: 065, Loss:0.4886 Train: 0.9000, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:44,577]: Epoch: 066, Loss:0.4545 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:44,584]: Epoch: 067, Loss:0.4282 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:44,592]: Epoch: 068, Loss:0.4788 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:44,599]: Epoch: 069, Loss:0.4594 Train: 0.8833, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:44,607]: Epoch: 070, Loss:0.4948 Train: 0.9167, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:44,613]: Epoch: 071, Loss:0.4358 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:44,620]: Epoch: 072, Loss:0.3938 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:44,628]: Epoch: 073, Loss:0.4355 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:44,636]: Epoch: 074, Loss:0.4372 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:44,644]: Epoch: 075, Loss:0.4609 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:44,651]: Epoch: 076, Loss:0.4421 Train: 0.8917, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0072
[2025-04-01 02:44:44,660]: Epoch: 077, Loss:0.4830 Train: 0.8917, Val:0.4375, Test: 0.5686, Time(s/epoch):0.0082
[2025-04-01 02:44:44,668]: Epoch: 078, Loss:0.4741 Train: 0.9167, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0079
[2025-04-01 02:44:44,676]: Epoch: 079, Loss:0.4693 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:44,686]: Epoch: 080, Loss:0.3841 Train: 0.9417, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0090
[2025-04-01 02:44:44,692]: Epoch: 081, Loss:0.3895 Train: 0.9250, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:44,701]: Epoch: 082, Loss:0.5186 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:44,709]: Epoch: 083, Loss:0.4102 Train: 0.9333, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:44,716]: Epoch: 084, Loss:0.3680 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:44,722]: Epoch: 085, Loss:0.3421 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:44,730]: Epoch: 086, Loss:0.4227 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:44,736]: Epoch: 087, Loss:0.3940 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:44,744]: Epoch: 088, Loss:0.4065 Train: 0.9250, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:44,751]: Epoch: 089, Loss:0.3948 Train: 0.9000, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:44,760]: Epoch: 090, Loss:0.3834 Train: 0.9167, Val:0.5000, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:44,766]: Epoch: 091, Loss:0.3866 Train: 0.9083, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:44,774]: Epoch: 092, Loss:0.3716 Train: 0.9083, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:44,782]: Epoch: 093, Loss:0.3867 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:44,789]: Epoch: 094, Loss:0.3429 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:44,796]: Epoch: 095, Loss:0.3395 Train: 0.9417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:44,804]: Epoch: 096, Loss:0.3566 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:44,810]: Epoch: 097, Loss:0.3550 Train: 0.9250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:44,817]: Epoch: 098, Loss:0.4243 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:44,825]: Epoch: 099, Loss:0.3642 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:44,833]: Epoch: 100, Loss:0.3757 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:44,840]: Epoch: 101, Loss:0.3866 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:44,848]: Epoch: 102, Loss:0.3949 Train: 0.9083, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:44,856]: Epoch: 103, Loss:0.3510 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:44,862]: Epoch: 104, Loss:0.4274 Train: 0.9083, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:44,871]: Epoch: 105, Loss:0.4426 Train: 0.9333, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0088
[2025-04-01 02:44:44,878]: Epoch: 106, Loss:0.3581 Train: 0.9583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:44,887]: Epoch: 107, Loss:0.3597 Train: 0.9500, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:44,894]: Epoch: 108, Loss:0.4150 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:44,902]: Epoch: 109, Loss:0.3504 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:44,908]: Epoch: 110, Loss:0.3366 Train: 0.9083, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:44,915]: Epoch: 111, Loss:0.3739 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:44,924]: Epoch: 112, Loss:0.3770 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:44,931]: Epoch: 113, Loss:0.4171 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:44,938]: Epoch: 114, Loss:0.4263 Train: 0.8833, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:44,945]: Epoch: 115, Loss:0.4000 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:44,952]: Epoch: 116, Loss:0.5446 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:44,960]: Epoch: 117, Loss:0.3769 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:44,967]: Epoch: 118, Loss:0.4981 Train: 0.8833, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0073
[2025-04-01 02:44:44,974]: Epoch: 119, Loss:0.3681 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:44,981]: Epoch: 120, Loss:0.4558 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:44,989]: Epoch: 121, Loss:0.3919 Train: 0.9333, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:44,996]: Epoch: 122, Loss:0.4094 Train: 0.8917, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:45,004]: Epoch: 123, Loss:0.4622 Train: 0.8833, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:45,010]: Epoch: 124, Loss:0.3587 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:45,018]: Epoch: 125, Loss:0.4309 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:45,026]: Epoch: 126, Loss:0.3637 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:45,033]: Epoch: 127, Loss:0.4062 Train: 0.9000, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:45,041]: Epoch: 128, Loss:0.4831 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:45,049]: Epoch: 129, Loss:0.3801 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:45,056]: Epoch: 130, Loss:0.3720 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:45,064]: Epoch: 131, Loss:0.4509 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:45,071]: Epoch: 132, Loss:0.3535 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:45,079]: Epoch: 133, Loss:0.3662 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:45,085]: Epoch: 134, Loss:0.3498 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:45,092]: Epoch: 135, Loss:0.4082 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:45,099]: Epoch: 136, Loss:0.3606 Train: 0.9250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:45,108]: Epoch: 137, Loss:0.3392 Train: 0.9083, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:45,115]: Epoch: 138, Loss:0.3603 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:45,123]: Epoch: 139, Loss:0.3641 Train: 0.9583, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:45,129]: Epoch: 140, Loss:0.3357 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:45,136]: Epoch: 141, Loss:0.3453 Train: 0.9250, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:45,142]: Epoch: 142, Loss:0.3871 Train: 0.9250, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:45,149]: Epoch: 143, Loss:0.3752 Train: 0.9000, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:45,157]: Epoch: 144, Loss:0.3744 Train: 0.9250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:45,163]: Epoch: 145, Loss:0.3620 Train: 0.9500, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:45,171]: Epoch: 146, Loss:0.3566 Train: 0.9333, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:45,179]: Epoch: 147, Loss:0.3464 Train: 0.9250, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:45,186]: Epoch: 148, Loss:0.3566 Train: 0.9250, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:45,193]: Epoch: 149, Loss:0.2721 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:45,202]: Epoch: 150, Loss:0.3041 Train: 0.9250, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:45,209]: Epoch: 151, Loss:0.3756 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:45,217]: Epoch: 152, Loss:0.3901 Train: 0.9333, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:45,225]: Epoch: 153, Loss:0.3677 Train: 0.9333, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0078
[2025-04-01 02:44:45,231]: Epoch: 154, Loss:0.3272 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:45,238]: Epoch: 155, Loss:0.5401 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:45,247]: Epoch: 156, Loss:0.2996 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:45,255]: Epoch: 157, Loss:0.5598 Train: 0.9000, Val:0.4750, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:45,263]: Epoch: 158, Loss:0.4048 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:45,269]: Epoch: 159, Loss:0.3548 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:45,276]: Epoch: 160, Loss:0.4582 Train: 0.9167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:45,282]: Epoch: 161, Loss:0.4267 Train: 0.9250, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:45,291]: Epoch: 162, Loss:0.3699 Train: 0.9000, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:45,298]: Epoch: 163, Loss:0.3695 Train: 0.8667, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:45,307]: Epoch: 164, Loss:0.4175 Train: 0.8833, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0094
[2025-04-01 02:44:45,317]: Epoch: 165, Loss:0.3819 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0094
[2025-04-01 02:44:45,323]: Epoch: 166, Loss:0.3840 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:45,331]: Epoch: 167, Loss:0.3927 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:45,339]: Epoch: 168, Loss:0.4530 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:45,347]: Epoch: 169, Loss:0.3560 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:45,355]: Epoch: 170, Loss:0.3887 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:45,364]: Epoch: 171, Loss:0.4220 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:45,372]: Epoch: 172, Loss:0.3514 Train: 0.9500, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:45,380]: Epoch: 173, Loss:0.4338 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:45,387]: Epoch: 174, Loss:0.3820 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:45,395]: Epoch: 175, Loss:0.4517 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:45,404]: Epoch: 176, Loss:0.2901 Train: 0.8750, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:45,412]: Epoch: 177, Loss:0.3637 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:45,420]: Epoch: 178, Loss:0.4167 Train: 0.9333, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:45,428]: Epoch: 179, Loss:0.3644 Train: 0.9417, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:45,436]: Epoch: 180, Loss:0.3492 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0082
[2025-04-01 02:44:45,444]: Epoch: 181, Loss:0.3568 Train: 0.9333, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:45,451]: Epoch: 182, Loss:0.5244 Train: 0.9667, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:45,460]: Epoch: 183, Loss:0.3401 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:45,468]: Epoch: 184, Loss:0.3442 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:45,476]: Epoch: 185, Loss:0.3891 Train: 0.9000, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:45,485]: Epoch: 186, Loss:0.3932 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:45,491]: Epoch: 187, Loss:0.3414 Train: 0.9167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:45,499]: Epoch: 188, Loss:0.3373 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:45,508]: Epoch: 189, Loss:0.3555 Train: 0.9417, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:45,517]: Epoch: 190, Loss:0.3667 Train: 0.9250, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:45,527]: Epoch: 191, Loss:0.3563 Train: 0.9083, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0100
[2025-04-01 02:44:45,535]: Epoch: 192, Loss:0.3436 Train: 0.8917, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:45,541]: Epoch: 193, Loss:0.3154 Train: 0.9000, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:45,550]: Epoch: 194, Loss:0.2883 Train: 0.9083, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:45,559]: Epoch: 195, Loss:0.4506 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:45,565]: Epoch: 196, Loss:0.2968 Train: 0.8500, Val:0.4250, Test: 0.3529, Time(s/epoch):0.0064
[2025-04-01 02:44:45,574]: Epoch: 197, Loss:0.6400 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:45,582]: Epoch: 198, Loss:0.4173 Train: 0.8167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:45,592]: Epoch: 199, Loss:0.4594 Train: 0.8333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:45,600]: Epoch: 200, Loss:0.4109 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:45,601]: [Run-3 score] {'train': 0.5583333333333333, 'val': 0.5625, 'test': 0.5490196078431373}
[2025-04-01 02:44:45,601]: repeat 1/3
[2025-04-01 02:44:45,601]: Manual random seed:0
[2025-04-01 02:44:45,601]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:45,605]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:45,617]: Epoch: 001, Loss:1.6731 Train: 0.5333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:45,626]: Epoch: 002, Loss:2.2659 Train: 0.6083, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:45,634]: Epoch: 003, Loss:1.3636 Train: 0.6167, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:45,640]: Epoch: 004, Loss:1.1637 Train: 0.6417, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0058
[2025-04-01 02:44:45,649]: Epoch: 005, Loss:0.9745 Train: 0.6583, Val:0.4750, Test: 0.5686, Time(s/epoch):0.0089
[2025-04-01 02:44:45,657]: Epoch: 006, Loss:0.9429 Train: 0.6667, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:45,665]: Epoch: 007, Loss:0.9355 Train: 0.6750, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:45,673]: Epoch: 008, Loss:0.9551 Train: 0.7083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:45,681]: Epoch: 009, Loss:0.9770 Train: 0.7083, Val:0.4500, Test: 0.5686, Time(s/epoch):0.0075
[2025-04-01 02:44:45,689]: Epoch: 010, Loss:0.7993 Train: 0.7250, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:45,697]: Epoch: 011, Loss:0.8941 Train: 0.7333, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0081
[2025-04-01 02:44:45,703]: Epoch: 012, Loss:0.7319 Train: 0.7250, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:45,710]: Epoch: 013, Loss:0.7875 Train: 0.7500, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:45,718]: Epoch: 014, Loss:0.7976 Train: 0.7750, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:45,724]: Epoch: 015, Loss:0.8408 Train: 0.7833, Val:0.4875, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:45,731]: Epoch: 016, Loss:0.7825 Train: 0.8083, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:45,738]: Epoch: 017, Loss:0.6766 Train: 0.8083, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:45,746]: Epoch: 018, Loss:0.7653 Train: 0.7917, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:45,752]: Epoch: 019, Loss:0.7026 Train: 0.7917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:45,759]: Epoch: 020, Loss:0.7269 Train: 0.7333, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:45,768]: Epoch: 021, Loss:0.7185 Train: 0.7250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:45,774]: Epoch: 022, Loss:0.6805 Train: 0.7167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:45,781]: Epoch: 023, Loss:0.6668 Train: 0.7583, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:45,790]: Epoch: 024, Loss:0.6442 Train: 0.8000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:45,797]: Epoch: 025, Loss:0.6621 Train: 0.8250, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:45,805]: Epoch: 026, Loss:0.5730 Train: 0.8583, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:45,813]: Epoch: 027, Loss:0.5873 Train: 0.8417, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:45,819]: Epoch: 028, Loss:0.7172 Train: 0.8500, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:45,826]: Epoch: 029, Loss:0.6056 Train: 0.8417, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:45,833]: Epoch: 030, Loss:0.5701 Train: 0.8667, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:45,840]: Epoch: 031, Loss:0.5631 Train: 0.8333, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:45,848]: Epoch: 032, Loss:0.5538 Train: 0.7917, Val:0.4750, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:45,855]: Epoch: 033, Loss:0.6002 Train: 0.8000, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0063
[2025-04-01 02:44:45,863]: Epoch: 034, Loss:0.5820 Train: 0.8167, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:45,871]: Epoch: 035, Loss:0.5468 Train: 0.8583, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:45,878]: Epoch: 036, Loss:0.5160 Train: 0.8583, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:45,886]: Epoch: 037, Loss:0.4804 Train: 0.8333, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:45,892]: Epoch: 038, Loss:0.6427 Train: 0.8583, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0061
[2025-04-01 02:44:45,901]: Epoch: 039, Loss:0.5207 Train: 0.8667, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:45,908]: Epoch: 040, Loss:0.5208 Train: 0.8417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:45,915]: Epoch: 041, Loss:0.4823 Train: 0.8417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:45,923]: Epoch: 042, Loss:0.5358 Train: 0.8333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:45,929]: Epoch: 043, Loss:0.5367 Train: 0.8417, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:45,937]: Epoch: 044, Loss:0.5337 Train: 0.8667, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:45,945]: Epoch: 045, Loss:0.5117 Train: 0.8667, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:45,953]: Epoch: 046, Loss:0.5086 Train: 0.8500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:45,978]: Epoch: 047, Loss:0.6086 Train: 0.8750, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0244
[2025-04-01 02:44:45,990]: Epoch: 048, Loss:0.4598 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0115
[2025-04-01 02:44:46,003]: Epoch: 049, Loss:0.4690 Train: 0.8667, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0132
[2025-04-01 02:44:46,014]: Epoch: 050, Loss:0.4931 Train: 0.8417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0106
[2025-04-01 02:44:46,023]: Epoch: 051, Loss:0.4985 Train: 0.8500, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0085
[2025-04-01 02:44:46,030]: Epoch: 052, Loss:0.4646 Train: 0.8583, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:46,038]: Epoch: 053, Loss:0.4730 Train: 0.8750, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:46,044]: Epoch: 054, Loss:0.4262 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:46,051]: Epoch: 055, Loss:0.4586 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:46,059]: Epoch: 056, Loss:0.4814 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,067]: Epoch: 057, Loss:0.4225 Train: 0.9000, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:46,076]: Epoch: 058, Loss:0.4610 Train: 0.8917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:46,083]: Epoch: 059, Loss:0.4386 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:46,091]: Epoch: 060, Loss:0.4149 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:46,099]: Epoch: 061, Loss:0.3789 Train: 0.8750, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:46,107]: Epoch: 062, Loss:0.3852 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:46,114]: Epoch: 063, Loss:0.4327 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:46,122]: Epoch: 064, Loss:0.4738 Train: 0.8667, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:46,130]: Epoch: 065, Loss:0.4019 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:46,138]: Epoch: 066, Loss:0.3747 Train: 0.8833, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:46,146]: Epoch: 067, Loss:0.3854 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:46,153]: Epoch: 068, Loss:0.4705 Train: 0.8417, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0061
[2025-04-01 02:44:46,160]: Epoch: 069, Loss:0.4326 Train: 0.8167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:46,167]: Epoch: 070, Loss:0.4826 Train: 0.8417, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:46,174]: Epoch: 071, Loss:0.5326 Train: 0.8833, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:46,181]: Epoch: 072, Loss:0.3625 Train: 0.9000, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:46,188]: Epoch: 073, Loss:0.4622 Train: 0.9000, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:46,194]: Epoch: 074, Loss:0.4277 Train: 0.8917, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:46,203]: Epoch: 075, Loss:0.3636 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:46,209]: Epoch: 076, Loss:0.4952 Train: 0.8833, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:46,217]: Epoch: 077, Loss:0.4248 Train: 0.8833, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:46,225]: Epoch: 078, Loss:0.4348 Train: 0.8833, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:46,232]: Epoch: 079, Loss:0.3777 Train: 0.9167, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:46,240]: Epoch: 080, Loss:0.4164 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:46,247]: Epoch: 081, Loss:0.4249 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:46,255]: Epoch: 082, Loss:0.4736 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:46,261]: Epoch: 083, Loss:0.4004 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0058
[2025-04-01 02:44:46,270]: Epoch: 084, Loss:0.4543 Train: 0.8667, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:46,278]: Epoch: 085, Loss:0.3866 Train: 0.8667, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:46,285]: Epoch: 086, Loss:0.4029 Train: 0.8500, Val:0.3750, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:46,291]: Epoch: 087, Loss:0.4381 Train: 0.8917, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:46,299]: Epoch: 088, Loss:0.3932 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:46,308]: Epoch: 089, Loss:0.3763 Train: 0.8917, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0085
[2025-04-01 02:44:46,317]: Epoch: 090, Loss:0.4308 Train: 0.8750, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0092
[2025-04-01 02:44:46,325]: Epoch: 091, Loss:0.4013 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:46,331]: Epoch: 092, Loss:0.4090 Train: 0.8833, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:46,339]: Epoch: 093, Loss:0.3581 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:46,348]: Epoch: 094, Loss:0.4138 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:46,356]: Epoch: 095, Loss:0.3653 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:46,365]: Epoch: 096, Loss:0.4149 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:46,372]: Epoch: 097, Loss:0.4037 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:46,380]: Epoch: 098, Loss:0.3454 Train: 0.8917, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:46,388]: Epoch: 099, Loss:0.3766 Train: 0.8833, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:46,396]: Epoch: 100, Loss:0.4392 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:46,403]: Epoch: 101, Loss:0.3831 Train: 0.8833, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:46,411]: Epoch: 102, Loss:0.3991 Train: 0.8833, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:46,419]: Epoch: 103, Loss:0.4351 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:46,427]: Epoch: 104, Loss:0.4013 Train: 0.9167, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:46,435]: Epoch: 105, Loss:0.3919 Train: 0.8917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:46,441]: Epoch: 106, Loss:0.4644 Train: 0.9083, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:46,448]: Epoch: 107, Loss:0.4385 Train: 0.8917, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:46,456]: Epoch: 108, Loss:0.3713 Train: 0.8750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,462]: Epoch: 109, Loss:0.4629 Train: 0.8917, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0060
[2025-04-01 02:44:46,469]: Epoch: 110, Loss:0.4151 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:46,477]: Epoch: 111, Loss:0.3586 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:46,485]: Epoch: 112, Loss:0.4069 Train: 0.9000, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:46,491]: Epoch: 113, Loss:0.4520 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:46,499]: Epoch: 114, Loss:0.4286 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:46,508]: Epoch: 115, Loss:0.3521 Train: 0.9250, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0083
[2025-04-01 02:44:46,516]: Epoch: 116, Loss:0.3586 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:46,524]: Epoch: 117, Loss:0.3709 Train: 0.9083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,531]: Epoch: 118, Loss:0.3961 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:46,540]: Epoch: 119, Loss:0.3776 Train: 0.9000, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:46,546]: Epoch: 120, Loss:0.3677 Train: 0.8917, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:46,554]: Epoch: 121, Loss:0.4474 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:46,561]: Epoch: 122, Loss:0.3403 Train: 0.9083, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:46,569]: Epoch: 123, Loss:0.4169 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:46,577]: Epoch: 124, Loss:0.4020 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:46,584]: Epoch: 125, Loss:0.4120 Train: 0.9417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:46,591]: Epoch: 126, Loss:0.3585 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:46,598]: Epoch: 127, Loss:0.3826 Train: 0.9583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,606]: Epoch: 128, Loss:0.3570 Train: 0.9333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:46,612]: Epoch: 129, Loss:0.3625 Train: 0.9167, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0057
[2025-04-01 02:44:46,619]: Epoch: 130, Loss:0.3572 Train: 0.9167, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:46,627]: Epoch: 131, Loss:0.3645 Train: 0.9333, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:46,635]: Epoch: 132, Loss:0.3226 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:46,641]: Epoch: 133, Loss:0.4251 Train: 0.9333, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0058
[2025-04-01 02:44:46,649]: Epoch: 134, Loss:0.3784 Train: 0.9333, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0078
[2025-04-01 02:44:46,657]: Epoch: 135, Loss:0.3404 Train: 0.9250, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:46,663]: Epoch: 136, Loss:0.3394 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:46,670]: Epoch: 137, Loss:0.3540 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:46,678]: Epoch: 138, Loss:0.3858 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:46,685]: Epoch: 139, Loss:0.3423 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:46,693]: Epoch: 140, Loss:0.3938 Train: 0.9083, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:46,700]: Epoch: 141, Loss:0.3358 Train: 0.9167, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:46,706]: Epoch: 142, Loss:0.3972 Train: 0.9333, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:46,714]: Epoch: 143, Loss:0.3105 Train: 0.9417, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:46,722]: Epoch: 144, Loss:0.3387 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:46,728]: Epoch: 145, Loss:0.3503 Train: 0.9417, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:46,735]: Epoch: 146, Loss:0.3573 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:46,743]: Epoch: 147, Loss:0.3998 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:46,750]: Epoch: 148, Loss:0.3191 Train: 0.9250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:46,758]: Epoch: 149, Loss:0.3059 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:46,765]: Epoch: 150, Loss:0.3661 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:46,774]: Epoch: 151, Loss:0.3802 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:46,781]: Epoch: 152, Loss:0.3294 Train: 0.9167, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:46,789]: Epoch: 153, Loss:0.3131 Train: 0.9167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:46,796]: Epoch: 154, Loss:0.3349 Train: 0.9417, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:46,803]: Epoch: 155, Loss:0.3079 Train: 0.9167, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:46,811]: Epoch: 156, Loss:0.2908 Train: 0.9167, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:46,819]: Epoch: 157, Loss:0.3226 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:46,826]: Epoch: 158, Loss:0.3370 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:46,833]: Epoch: 159, Loss:0.3710 Train: 0.9167, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:46,839]: Epoch: 160, Loss:0.3097 Train: 0.9333, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:46,847]: Epoch: 161, Loss:0.3189 Train: 0.9083, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:46,855]: Epoch: 162, Loss:0.3551 Train: 0.9250, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:46,862]: Epoch: 163, Loss:0.4156 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:46,870]: Epoch: 164, Loss:0.3733 Train: 0.8917, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:46,879]: Epoch: 165, Loss:0.3224 Train: 0.8833, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:46,885]: Epoch: 166, Loss:0.3839 Train: 0.9167, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0062
[2025-04-01 02:44:46,893]: Epoch: 167, Loss:0.3603 Train: 0.9250, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0083
[2025-04-01 02:44:46,901]: Epoch: 168, Loss:0.3019 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:46,907]: Epoch: 169, Loss:0.3448 Train: 0.9333, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:46,914]: Epoch: 170, Loss:0.3586 Train: 0.9333, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:46,922]: Epoch: 171, Loss:0.3434 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,929]: Epoch: 172, Loss:0.3647 Train: 0.8917, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:46,937]: Epoch: 173, Loss:0.3363 Train: 0.8917, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:46,945]: Epoch: 174, Loss:0.3466 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:46,953]: Epoch: 175, Loss:0.3785 Train: 0.9250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:46,959]: Epoch: 176, Loss:0.3572 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:46,966]: Epoch: 177, Loss:0.4340 Train: 0.9333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:46,973]: Epoch: 178, Loss:0.4222 Train: 0.8500, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:46,980]: Epoch: 179, Loss:0.3816 Train: 0.8083, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:46,987]: Epoch: 180, Loss:0.5932 Train: 0.8750, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:46,995]: Epoch: 181, Loss:0.4198 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:47,003]: Epoch: 182, Loss:0.3177 Train: 0.9167, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:47,010]: Epoch: 183, Loss:0.4054 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0069
[2025-04-01 02:44:47,017]: Epoch: 184, Loss:0.4561 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:47,024]: Epoch: 185, Loss:0.3511 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,032]: Epoch: 186, Loss:0.3215 Train: 0.8917, Val:0.3875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:47,039]: Epoch: 187, Loss:0.3146 Train: 0.8750, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:47,046]: Epoch: 188, Loss:0.3451 Train: 0.8833, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:47,052]: Epoch: 189, Loss:0.3719 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0058
[2025-04-01 02:44:47,060]: Epoch: 190, Loss:0.3907 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0071
[2025-04-01 02:44:47,067]: Epoch: 191, Loss:0.3405 Train: 0.9000, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:47,074]: Epoch: 192, Loss:0.4658 Train: 0.9000, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:47,081]: Epoch: 193, Loss:0.3586 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:47,089]: Epoch: 194, Loss:0.4219 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:47,097]: Epoch: 195, Loss:0.3093 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:47,104]: Epoch: 196, Loss:0.2983 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:47,112]: Epoch: 197, Loss:0.3263 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:47,120]: Epoch: 198, Loss:0.3589 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:47,128]: Epoch: 199, Loss:0.3726 Train: 0.9167, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:47,136]: Epoch: 200, Loss:0.3819 Train: 0.9250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:47,136]: [Run-1 score] {'train': 0.8666666666666667, 'val': 0.5125, 'test': 0.47058823529411764}
[2025-04-01 02:44:47,136]: repeat 2/3
[2025-04-01 02:44:47,136]: Manual random seed:0
[2025-04-01 02:44:47,136]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:47,139]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:47,150]: Epoch: 001, Loss:1.6396 Train: 0.5333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:47,157]: Epoch: 002, Loss:1.9089 Train: 0.6000, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:47,165]: Epoch: 003, Loss:1.2316 Train: 0.5667, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:47,172]: Epoch: 004, Loss:1.2160 Train: 0.6833, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0075
[2025-04-01 02:44:47,180]: Epoch: 005, Loss:1.0500 Train: 0.6417, Val:0.5125, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:47,188]: Epoch: 006, Loss:1.1047 Train: 0.6750, Val:0.4875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:47,196]: Epoch: 007, Loss:0.9172 Train: 0.6833, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:47,204]: Epoch: 008, Loss:0.9579 Train: 0.6667, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:47,210]: Epoch: 009, Loss:0.9632 Train: 0.6917, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:47,217]: Epoch: 010, Loss:0.9399 Train: 0.7083, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:47,225]: Epoch: 011, Loss:0.8702 Train: 0.7333, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:47,233]: Epoch: 012, Loss:0.7748 Train: 0.7000, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:47,241]: Epoch: 013, Loss:0.8589 Train: 0.6917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:47,247]: Epoch: 014, Loss:0.8019 Train: 0.7667, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,254]: Epoch: 015, Loss:0.7588 Train: 0.7417, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0068
[2025-04-01 02:44:47,263]: Epoch: 016, Loss:0.8376 Train: 0.7583, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:47,271]: Epoch: 017, Loss:0.7630 Train: 0.7500, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:47,277]: Epoch: 018, Loss:0.7109 Train: 0.7500, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:47,285]: Epoch: 019, Loss:0.7689 Train: 0.7583, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:47,291]: Epoch: 020, Loss:0.6536 Train: 0.7750, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:47,298]: Epoch: 021, Loss:0.6217 Train: 0.7917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:47,307]: Epoch: 022, Loss:0.6497 Train: 0.7917, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:47,316]: Epoch: 023, Loss:0.7758 Train: 0.8083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:47,324]: Epoch: 024, Loss:0.6244 Train: 0.8000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:47,332]: Epoch: 025, Loss:0.6161 Train: 0.7917, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:47,340]: Epoch: 026, Loss:0.5689 Train: 0.8083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:47,348]: Epoch: 027, Loss:0.5810 Train: 0.8083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:47,355]: Epoch: 028, Loss:0.5825 Train: 0.7917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:47,361]: Epoch: 029, Loss:0.5574 Train: 0.7833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:47,369]: Epoch: 030, Loss:0.5369 Train: 0.7917, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:47,376]: Epoch: 031, Loss:0.5000 Train: 0.8167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:47,383]: Epoch: 032, Loss:0.5745 Train: 0.8333, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:47,389]: Epoch: 033, Loss:0.5761 Train: 0.8500, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,397]: Epoch: 034, Loss:0.4924 Train: 0.8500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0072
[2025-04-01 02:44:47,404]: Epoch: 035, Loss:0.5251 Train: 0.8583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:47,413]: Epoch: 036, Loss:0.5229 Train: 0.8500, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:47,419]: Epoch: 037, Loss:0.5029 Train: 0.8583, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:47,427]: Epoch: 038, Loss:0.5347 Train: 0.8500, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:47,435]: Epoch: 039, Loss:0.4990 Train: 0.8250, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:47,443]: Epoch: 040, Loss:0.5169 Train: 0.8333, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:47,451]: Epoch: 041, Loss:0.4644 Train: 0.8583, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:47,458]: Epoch: 042, Loss:0.5192 Train: 0.8667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:47,466]: Epoch: 043, Loss:0.5384 Train: 0.8500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:47,474]: Epoch: 044, Loss:0.4878 Train: 0.8417, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:47,480]: Epoch: 045, Loss:0.5397 Train: 0.8583, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:47,486]: Epoch: 046, Loss:0.5031 Train: 0.8750, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,495]: Epoch: 047, Loss:0.4722 Train: 0.8583, Val:0.4000, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:47,502]: Epoch: 048, Loss:0.5181 Train: 0.8417, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:47,509]: Epoch: 049, Loss:0.4930 Train: 0.8167, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:47,516]: Epoch: 050, Loss:0.5044 Train: 0.8167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:47,523]: Epoch: 051, Loss:0.4780 Train: 0.8917, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0068
[2025-04-01 02:44:47,531]: Epoch: 052, Loss:0.4387 Train: 0.8833, Val:0.4125, Test: 0.5294, Time(s/epoch):0.0077
[2025-04-01 02:44:47,539]: Epoch: 053, Loss:0.5538 Train: 0.8750, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:47,547]: Epoch: 054, Loss:0.4894 Train: 0.8750, Val:0.4500, Test: 0.5490, Time(s/epoch):0.0073
[2025-04-01 02:44:47,553]: Epoch: 055, Loss:0.4888 Train: 0.8500, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,560]: Epoch: 056, Loss:0.4967 Train: 0.8583, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:47,568]: Epoch: 057, Loss:0.5056 Train: 0.8417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:47,574]: Epoch: 058, Loss:0.5614 Train: 0.8667, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0066
[2025-04-01 02:44:47,582]: Epoch: 059, Loss:0.4008 Train: 0.8583, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0074
[2025-04-01 02:44:47,590]: Epoch: 060, Loss:0.4757 Train: 0.9000, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:47,596]: Epoch: 061, Loss:0.4514 Train: 0.8917, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,603]: Epoch: 062, Loss:0.4452 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:47,610]: Epoch: 063, Loss:0.4446 Train: 0.8667, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0070
[2025-04-01 02:44:47,617]: Epoch: 064, Loss:0.4189 Train: 0.8667, Val:0.3875, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:47,624]: Epoch: 065, Loss:0.4514 Train: 0.8750, Val:0.4250, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:47,631]: Epoch: 066, Loss:0.4337 Train: 0.8917, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:47,640]: Epoch: 067, Loss:0.4068 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:47,648]: Epoch: 068, Loss:0.3970 Train: 0.8750, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0084
[2025-04-01 02:44:47,656]: Epoch: 069, Loss:0.3801 Train: 0.8583, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:47,663]: Epoch: 070, Loss:0.4406 Train: 0.8833, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0069
[2025-04-01 02:44:47,673]: Epoch: 071, Loss:0.4718 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0091
[2025-04-01 02:44:47,683]: Epoch: 072, Loss:0.3653 Train: 0.8917, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0100
[2025-04-01 02:44:47,692]: Epoch: 073, Loss:0.4413 Train: 0.8833, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0090
[2025-04-01 02:44:47,701]: Epoch: 074, Loss:0.4769 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:47,710]: Epoch: 075, Loss:0.3759 Train: 0.9000, Val:0.4750, Test: 0.5098, Time(s/epoch):0.0094
[2025-04-01 02:44:47,718]: Epoch: 076, Loss:0.3955 Train: 0.9000, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:47,725]: Epoch: 077, Loss:0.4318 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:47,733]: Epoch: 078, Loss:0.4182 Train: 0.9000, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:47,741]: Epoch: 079, Loss:0.4544 Train: 0.8750, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:47,749]: Epoch: 080, Loss:0.4303 Train: 0.8500, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:47,757]: Epoch: 081, Loss:0.3932 Train: 0.8500, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:47,765]: Epoch: 082, Loss:0.4209 Train: 0.9000, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:47,773]: Epoch: 083, Loss:0.4070 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:47,780]: Epoch: 084, Loss:0.3805 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:47,788]: Epoch: 085, Loss:0.5052 Train: 0.8917, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:47,796]: Epoch: 086, Loss:0.4556 Train: 0.9000, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:47,805]: Epoch: 087, Loss:0.3904 Train: 0.8500, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:47,811]: Epoch: 088, Loss:0.4506 Train: 0.8667, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0057
[2025-04-01 02:44:47,818]: Epoch: 089, Loss:0.4764 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:47,825]: Epoch: 090, Loss:0.3853 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:47,834]: Epoch: 091, Loss:0.4088 Train: 0.8583, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:47,841]: Epoch: 092, Loss:0.5018 Train: 0.8917, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:47,848]: Epoch: 093, Loss:0.4438 Train: 0.8667, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:47,856]: Epoch: 094, Loss:0.3971 Train: 0.8583, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:47,862]: Epoch: 095, Loss:0.4878 Train: 0.8750, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:47,871]: Epoch: 096, Loss:0.4936 Train: 0.8917, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:47,877]: Epoch: 097, Loss:0.4339 Train: 0.8750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:47,884]: Epoch: 098, Loss:0.4263 Train: 0.8750, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:47,891]: Epoch: 099, Loss:0.3797 Train: 0.9083, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0065
[2025-04-01 02:44:47,898]: Epoch: 100, Loss:0.3575 Train: 0.9250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:47,905]: Epoch: 101, Loss:0.3855 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:47,912]: Epoch: 102, Loss:0.3407 Train: 0.9000, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:47,920]: Epoch: 103, Loss:0.4892 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:47,929]: Epoch: 104, Loss:0.3306 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:47,936]: Epoch: 105, Loss:0.4623 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:47,945]: Epoch: 106, Loss:0.3484 Train: 0.9083, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:47,952]: Epoch: 107, Loss:0.3889 Train: 0.8833, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:47,959]: Epoch: 108, Loss:0.3511 Train: 0.8583, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:47,967]: Epoch: 109, Loss:0.3489 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:47,975]: Epoch: 110, Loss:0.3677 Train: 0.9167, Val:0.4500, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:47,981]: Epoch: 111, Loss:0.2969 Train: 0.8917, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0061
[2025-04-01 02:44:47,989]: Epoch: 112, Loss:0.3467 Train: 0.9000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:47,997]: Epoch: 113, Loss:0.3885 Train: 0.9000, Val:0.4750, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:48,005]: Epoch: 114, Loss:0.3237 Train: 0.8583, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:48,012]: Epoch: 115, Loss:0.3442 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:48,022]: Epoch: 116, Loss:0.3881 Train: 0.8583, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0093
[2025-04-01 02:44:48,028]: Epoch: 117, Loss:0.3875 Train: 0.8833, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:48,035]: Epoch: 118, Loss:0.3747 Train: 0.8917, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:48,043]: Epoch: 119, Loss:0.4116 Train: 0.9167, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:48,050]: Epoch: 120, Loss:0.3865 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:48,058]: Epoch: 121, Loss:0.4144 Train: 0.9250, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:48,066]: Epoch: 122, Loss:0.3380 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:48,072]: Epoch: 123, Loss:0.4356 Train: 0.8833, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:48,079]: Epoch: 124, Loss:0.3254 Train: 0.9083, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:48,086]: Epoch: 125, Loss:0.4087 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:48,094]: Epoch: 126, Loss:0.3719 Train: 0.9167, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:48,102]: Epoch: 127, Loss:0.4666 Train: 0.9250, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:48,110]: Epoch: 128, Loss:0.3424 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:48,118]: Epoch: 129, Loss:0.3309 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:48,127]: Epoch: 130, Loss:0.3301 Train: 0.9083, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0088
[2025-04-01 02:44:48,135]: Epoch: 131, Loss:0.3488 Train: 0.8917, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:48,141]: Epoch: 132, Loss:0.3840 Train: 0.9167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:48,150]: Epoch: 133, Loss:0.3127 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:48,158]: Epoch: 134, Loss:0.3331 Train: 0.9083, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:48,166]: Epoch: 135, Loss:0.3226 Train: 0.9000, Val:0.4625, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:48,175]: Epoch: 136, Loss:0.3376 Train: 0.9333, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0085
[2025-04-01 02:44:48,183]: Epoch: 137, Loss:0.3075 Train: 0.9417, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:48,192]: Epoch: 138, Loss:0.3560 Train: 0.9167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:48,200]: Epoch: 139, Loss:0.3241 Train: 0.9417, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0086
[2025-04-01 02:44:48,208]: Epoch: 140, Loss:0.2806 Train: 0.9333, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:48,214]: Epoch: 141, Loss:0.3860 Train: 0.9083, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:48,221]: Epoch: 142, Loss:0.3220 Train: 0.9083, Val:0.3750, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:48,229]: Epoch: 143, Loss:0.3745 Train: 0.9083, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:48,236]: Epoch: 144, Loss:0.3315 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:48,245]: Epoch: 145, Loss:0.3279 Train: 0.9000, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0085
[2025-04-01 02:44:48,251]: Epoch: 146, Loss:0.3835 Train: 0.9167, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0059
[2025-04-01 02:44:48,259]: Epoch: 147, Loss:0.3388 Train: 0.9083, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:48,266]: Epoch: 148, Loss:0.3593 Train: 0.9167, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:48,273]: Epoch: 149, Loss:0.3507 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:48,279]: Epoch: 150, Loss:0.3744 Train: 0.9333, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:48,287]: Epoch: 151, Loss:0.4098 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:48,295]: Epoch: 152, Loss:0.3449 Train: 0.8750, Val:0.3250, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:48,303]: Epoch: 153, Loss:0.3417 Train: 0.8833, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:48,312]: Epoch: 154, Loss:0.3779 Train: 0.9167, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0094
[2025-04-01 02:44:48,321]: Epoch: 155, Loss:0.3544 Train: 0.9167, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:48,328]: Epoch: 156, Loss:0.4175 Train: 0.8833, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:48,336]: Epoch: 157, Loss:0.3505 Train: 0.9000, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:48,344]: Epoch: 158, Loss:0.3270 Train: 0.9083, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:48,352]: Epoch: 159, Loss:0.3640 Train: 0.9333, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:48,359]: Epoch: 160, Loss:0.3541 Train: 0.9250, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:48,367]: Epoch: 161, Loss:0.3403 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:48,375]: Epoch: 162, Loss:0.3290 Train: 0.9083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:48,382]: Epoch: 163, Loss:0.3127 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:48,389]: Epoch: 164, Loss:0.3050 Train: 0.9083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:48,398]: Epoch: 165, Loss:0.4296 Train: 0.8917, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:48,404]: Epoch: 166, Loss:0.4232 Train: 0.9000, Val:0.3625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:48,411]: Epoch: 167, Loss:0.3194 Train: 0.8833, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:48,418]: Epoch: 168, Loss:0.3924 Train: 0.8667, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:48,426]: Epoch: 169, Loss:0.3654 Train: 0.8750, Val:0.4125, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:48,432]: Epoch: 170, Loss:0.4173 Train: 0.9083, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:48,440]: Epoch: 171, Loss:0.3631 Train: 0.9167, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:48,448]: Epoch: 172, Loss:0.3834 Train: 0.9250, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0082
[2025-04-01 02:44:48,456]: Epoch: 173, Loss:0.3669 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:48,464]: Epoch: 174, Loss:0.3840 Train: 0.8833, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:48,472]: Epoch: 175, Loss:0.4321 Train: 0.8917, Val:0.3625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:48,480]: Epoch: 176, Loss:0.3720 Train: 0.9167, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:48,488]: Epoch: 177, Loss:0.3294 Train: 0.8917, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:48,496]: Epoch: 178, Loss:0.3418 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:48,503]: Epoch: 179, Loss:0.3779 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:48,511]: Epoch: 180, Loss:0.3863 Train: 0.9417, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:48,520]: Epoch: 181, Loss:0.3366 Train: 0.9083, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0085
[2025-04-01 02:44:48,527]: Epoch: 182, Loss:0.3534 Train: 0.8917, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:48,535]: Epoch: 183, Loss:0.3311 Train: 0.9417, Val:0.4375, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:48,542]: Epoch: 184, Loss:0.3106 Train: 0.9167, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:48,549]: Epoch: 185, Loss:0.3965 Train: 0.9333, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:48,556]: Epoch: 186, Loss:0.2980 Train: 0.9000, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:48,564]: Epoch: 187, Loss:0.3722 Train: 0.8750, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:48,571]: Epoch: 188, Loss:0.3321 Train: 0.9000, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:48,578]: Epoch: 189, Loss:0.2866 Train: 0.9167, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:48,585]: Epoch: 190, Loss:0.3755 Train: 0.9500, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:48,594]: Epoch: 191, Loss:0.4018 Train: 0.9167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0082
[2025-04-01 02:44:48,601]: Epoch: 192, Loss:0.3746 Train: 0.9500, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:48,608]: Epoch: 193, Loss:0.3862 Train: 0.9417, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:48,616]: Epoch: 194, Loss:0.3459 Train: 0.9000, Val:0.3500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:48,622]: Epoch: 195, Loss:0.3625 Train: 0.8833, Val:0.3750, Test: 0.3922, Time(s/epoch):0.0057
[2025-04-01 02:44:48,629]: Epoch: 196, Loss:0.4074 Train: 0.8667, Val:0.3750, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:48,636]: Epoch: 197, Loss:0.4134 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:48,644]: Epoch: 198, Loss:0.3120 Train: 0.9250, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:48,651]: Epoch: 199, Loss:0.3928 Train: 0.8917, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:48,658]: Epoch: 200, Loss:0.3560 Train: 0.8833, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:48,659]: [Run-2 score] {'train': 0.6416666666666667, 'val': 0.5125, 'test': 0.5294117647058824}
[2025-04-01 02:44:48,659]: repeat 3/3
[2025-04-01 02:44:48,659]: Manual random seed:0
[2025-04-01 02:44:48,659]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:48,662]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:48,673]: Epoch: 001, Loss:1.7184 Train: 0.5333, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:48,680]: Epoch: 002, Loss:1.3998 Train: 0.6000, Val:0.4500, Test: 0.5686, Time(s/epoch):0.0065
[2025-04-01 02:44:48,686]: Epoch: 003, Loss:1.1339 Train: 0.5917, Val:0.4000, Test: 0.5294, Time(s/epoch):0.0062
[2025-04-01 02:44:48,693]: Epoch: 004, Loss:1.1510 Train: 0.7000, Val:0.4375, Test: 0.5490, Time(s/epoch):0.0070
[2025-04-01 02:44:48,700]: Epoch: 005, Loss:1.2370 Train: 0.6750, Val:0.4625, Test: 0.5098, Time(s/epoch):0.0063
[2025-04-01 02:44:48,708]: Epoch: 006, Loss:0.9355 Train: 0.6583, Val:0.5000, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:48,716]: Epoch: 007, Loss:0.9197 Train: 0.6750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:48,723]: Epoch: 008, Loss:0.9209 Train: 0.6750, Val:0.4750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:48,731]: Epoch: 009, Loss:0.9895 Train: 0.7417, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0075
[2025-04-01 02:44:48,738]: Epoch: 010, Loss:0.8473 Train: 0.7417, Val:0.4750, Test: 0.5490, Time(s/epoch):0.0069
[2025-04-01 02:44:48,746]: Epoch: 011, Loss:0.9379 Train: 0.7667, Val:0.5000, Test: 0.5490, Time(s/epoch):0.0078
[2025-04-01 02:44:48,754]: Epoch: 012, Loss:0.8448 Train: 0.7417, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:48,760]: Epoch: 013, Loss:0.8652 Train: 0.7333, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:48,767]: Epoch: 014, Loss:0.8052 Train: 0.7500, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:48,775]: Epoch: 015, Loss:0.6961 Train: 0.7417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:48,781]: Epoch: 016, Loss:0.7637 Train: 0.7500, Val:0.4500, Test: 0.5098, Time(s/epoch):0.0059
[2025-04-01 02:44:48,789]: Epoch: 017, Loss:0.6540 Train: 0.7833, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:48,795]: Epoch: 018, Loss:0.6581 Train: 0.7917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0067
[2025-04-01 02:44:48,804]: Epoch: 019, Loss:0.7013 Train: 0.8000, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0086
[2025-04-01 02:44:48,811]: Epoch: 020, Loss:0.6782 Train: 0.8000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0063
[2025-04-01 02:44:48,818]: Epoch: 021, Loss:0.6345 Train: 0.8083, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:48,827]: Epoch: 022, Loss:0.5673 Train: 0.8000, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:48,835]: Epoch: 023, Loss:0.5890 Train: 0.8167, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:48,843]: Epoch: 024, Loss:0.6461 Train: 0.8167, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:48,851]: Epoch: 025, Loss:0.5965 Train: 0.8250, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:48,858]: Epoch: 026, Loss:0.5512 Train: 0.8417, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:48,867]: Epoch: 027, Loss:0.5860 Train: 0.8167, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:48,875]: Epoch: 028, Loss:0.5382 Train: 0.8333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:48,882]: Epoch: 029, Loss:0.5324 Train: 0.8250, Val:0.4000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:48,890]: Epoch: 030, Loss:0.5226 Train: 0.8250, Val:0.3750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:48,898]: Epoch: 031, Loss:0.4939 Train: 0.8500, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:48,906]: Epoch: 032, Loss:0.4974 Train: 0.8417, Val:0.3875, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:48,914]: Epoch: 033, Loss:0.5254 Train: 0.8667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:48,921]: Epoch: 034, Loss:0.6150 Train: 0.8583, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:48,929]: Epoch: 035, Loss:0.5778 Train: 0.8917, Val:0.4625, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:48,936]: Epoch: 036, Loss:0.4814 Train: 0.8583, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0066
[2025-04-01 02:44:48,943]: Epoch: 037, Loss:0.4324 Train: 0.8667, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:48,951]: Epoch: 038, Loss:0.5416 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:48,958]: Epoch: 039, Loss:0.4564 Train: 0.8250, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:48,967]: Epoch: 040, Loss:0.5444 Train: 0.8667, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:48,975]: Epoch: 041, Loss:0.4425 Train: 0.8583, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:48,982]: Epoch: 042, Loss:0.4765 Train: 0.8417, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:48,990]: Epoch: 043, Loss:0.4666 Train: 0.8500, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:48,996]: Epoch: 044, Loss:0.4647 Train: 0.8667, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:49,003]: Epoch: 045, Loss:0.4767 Train: 0.8833, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:49,010]: Epoch: 046, Loss:0.4814 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:49,017]: Epoch: 047, Loss:0.4938 Train: 0.8750, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:49,025]: Epoch: 048, Loss:0.4683 Train: 0.8750, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0072
[2025-04-01 02:44:49,031]: Epoch: 049, Loss:0.4626 Train: 0.8583, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:49,038]: Epoch: 050, Loss:0.4654 Train: 0.8417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:49,045]: Epoch: 051, Loss:0.4953 Train: 0.8667, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:49,052]: Epoch: 052, Loss:0.4478 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:49,059]: Epoch: 053, Loss:0.4910 Train: 0.9083, Val:0.3875, Test: 0.5098, Time(s/epoch):0.0069
[2025-04-01 02:44:49,066]: Epoch: 054, Loss:0.4744 Train: 0.8750, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:49,074]: Epoch: 055, Loss:0.4726 Train: 0.8583, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:49,081]: Epoch: 056, Loss:0.4913 Train: 0.9000, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:49,089]: Epoch: 057, Loss:0.4103 Train: 0.8667, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:49,097]: Epoch: 058, Loss:0.4418 Train: 0.8417, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:49,105]: Epoch: 059, Loss:0.4930 Train: 0.8583, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:49,111]: Epoch: 060, Loss:0.4502 Train: 0.8750, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:49,118]: Epoch: 061, Loss:0.3931 Train: 0.9000, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0069
[2025-04-01 02:44:49,126]: Epoch: 062, Loss:0.4317 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:49,132]: Epoch: 063, Loss:0.4767 Train: 0.9000, Val:0.4500, Test: 0.4706, Time(s/epoch):0.0066
[2025-04-01 02:44:49,139]: Epoch: 064, Loss:0.4666 Train: 0.8917, Val:0.4250, Test: 0.5098, Time(s/epoch):0.0064
[2025-04-01 02:44:49,146]: Epoch: 065, Loss:0.4749 Train: 0.8750, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0067
[2025-04-01 02:44:49,154]: Epoch: 066, Loss:0.4532 Train: 0.8250, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:49,161]: Epoch: 067, Loss:0.5602 Train: 0.8417, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:49,170]: Epoch: 068, Loss:0.4277 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:49,176]: Epoch: 069, Loss:0.4775 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0059
[2025-04-01 02:44:49,194]: Epoch: 070, Loss:0.4563 Train: 0.8667, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0186
[2025-04-01 02:44:49,206]: Epoch: 071, Loss:0.5136 Train: 0.8750, Val:0.4000, Test: 0.4902, Time(s/epoch):0.0109
[2025-04-01 02:44:49,214]: Epoch: 072, Loss:0.4480 Train: 0.9083, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:49,220]: Epoch: 073, Loss:0.4131 Train: 0.8750, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0056
[2025-04-01 02:44:49,227]: Epoch: 074, Loss:0.4685 Train: 0.8333, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:49,234]: Epoch: 075, Loss:0.5553 Train: 0.8333, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:49,240]: Epoch: 076, Loss:0.4262 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0062
[2025-04-01 02:44:49,248]: Epoch: 077, Loss:0.4136 Train: 0.8833, Val:0.4625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:49,256]: Epoch: 078, Loss:0.4205 Train: 0.8833, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:49,263]: Epoch: 079, Loss:0.4671 Train: 0.9000, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:49,271]: Epoch: 080, Loss:0.3792 Train: 0.9167, Val:0.4500, Test: 0.5294, Time(s/epoch):0.0069
[2025-04-01 02:44:49,279]: Epoch: 081, Loss:0.4196 Train: 0.9083, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0079
[2025-04-01 02:44:49,286]: Epoch: 082, Loss:0.4030 Train: 0.8833, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:49,295]: Epoch: 083, Loss:0.3514 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0084
[2025-04-01 02:44:49,302]: Epoch: 084, Loss:0.4147 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0066
[2025-04-01 02:44:49,310]: Epoch: 085, Loss:0.3889 Train: 0.9167, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0083
[2025-04-01 02:44:49,320]: Epoch: 086, Loss:0.4737 Train: 0.8917, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0099
[2025-04-01 02:44:49,329]: Epoch: 087, Loss:0.4792 Train: 0.8917, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0082
[2025-04-01 02:44:49,335]: Epoch: 088, Loss:0.4340 Train: 0.9000, Val:0.4375, Test: 0.5098, Time(s/epoch):0.0062
[2025-04-01 02:44:49,342]: Epoch: 089, Loss:0.3620 Train: 0.9083, Val:0.4125, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:49,350]: Epoch: 090, Loss:0.3657 Train: 0.8667, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:49,358]: Epoch: 091, Loss:0.4180 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:49,366]: Epoch: 092, Loss:0.4476 Train: 0.9250, Val:0.4375, Test: 0.5294, Time(s/epoch):0.0080
[2025-04-01 02:44:49,374]: Epoch: 093, Loss:0.3621 Train: 0.9083, Val:0.4625, Test: 0.5294, Time(s/epoch):0.0074
[2025-04-01 02:44:49,381]: Epoch: 094, Loss:0.4034 Train: 0.8750, Val:0.4375, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:49,389]: Epoch: 095, Loss:0.3365 Train: 0.8833, Val:0.4250, Test: 0.4902, Time(s/epoch):0.0074
[2025-04-01 02:44:49,397]: Epoch: 096, Loss:0.3759 Train: 0.9000, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0081
[2025-04-01 02:44:49,406]: Epoch: 097, Loss:0.3623 Train: 0.9083, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:49,413]: Epoch: 098, Loss:0.4353 Train: 0.9000, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:49,420]: Epoch: 099, Loss:0.3711 Train: 0.9000, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:49,428]: Epoch: 100, Loss:0.3378 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:49,436]: Epoch: 101, Loss:0.4710 Train: 0.9333, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:49,444]: Epoch: 102, Loss:0.4052 Train: 0.9250, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:49,452]: Epoch: 103, Loss:0.3658 Train: 0.8917, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:49,459]: Epoch: 104, Loss:0.4687 Train: 0.8667, Val:0.4500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:49,467]: Epoch: 105, Loss:0.4159 Train: 0.8417, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0083
[2025-04-01 02:44:49,474]: Epoch: 106, Loss:0.4245 Train: 0.8833, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:49,482]: Epoch: 107, Loss:0.3755 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:49,489]: Epoch: 108, Loss:0.4343 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:49,497]: Epoch: 109, Loss:0.3729 Train: 0.9250, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:49,505]: Epoch: 110, Loss:0.3428 Train: 0.9167, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:49,511]: Epoch: 111, Loss:0.4332 Train: 0.9000, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0057
[2025-04-01 02:44:49,519]: Epoch: 112, Loss:0.3846 Train: 0.8500, Val:0.3875, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:49,527]: Epoch: 113, Loss:0.4804 Train: 0.8917, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:49,535]: Epoch: 114, Loss:0.4032 Train: 0.8750, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:49,542]: Epoch: 115, Loss:0.3615 Train: 0.8833, Val:0.4250, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:49,550]: Epoch: 116, Loss:0.4240 Train: 0.8750, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:49,556]: Epoch: 117, Loss:0.3949 Train: 0.8833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0065
[2025-04-01 02:44:49,562]: Epoch: 118, Loss:0.3785 Train: 0.9083, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:49,570]: Epoch: 119, Loss:0.3713 Train: 0.8917, Val:0.4250, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:49,578]: Epoch: 120, Loss:0.3784 Train: 0.8917, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:49,586]: Epoch: 121, Loss:0.4296 Train: 0.9083, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:49,592]: Epoch: 122, Loss:0.3571 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:49,600]: Epoch: 123, Loss:0.3888 Train: 0.8917, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:49,608]: Epoch: 124, Loss:0.3858 Train: 0.9250, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:49,616]: Epoch: 125, Loss:0.3323 Train: 0.9167, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:49,624]: Epoch: 126, Loss:0.4072 Train: 0.9250, Val:0.4125, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:49,632]: Epoch: 127, Loss:0.3771 Train: 0.9083, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:49,638]: Epoch: 128, Loss:0.4237 Train: 0.9083, Val:0.4250, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:49,645]: Epoch: 129, Loss:0.3769 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:49,653]: Epoch: 130, Loss:0.3368 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:49,660]: Epoch: 131, Loss:0.3862 Train: 0.9167, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:49,667]: Epoch: 132, Loss:0.3167 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:49,674]: Epoch: 133, Loss:0.3392 Train: 0.9000, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:49,681]: Epoch: 134, Loss:0.3507 Train: 0.9083, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:49,688]: Epoch: 135, Loss:0.3650 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:49,695]: Epoch: 136, Loss:0.3502 Train: 0.9500, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:49,702]: Epoch: 137, Loss:0.3951 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:49,709]: Epoch: 138, Loss:0.3858 Train: 0.9000, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:49,717]: Epoch: 139, Loss:0.3669 Train: 0.8750, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:49,726]: Epoch: 140, Loss:0.4341 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:49,734]: Epoch: 141, Loss:0.3379 Train: 0.9083, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:49,741]: Epoch: 142, Loss:0.3626 Train: 0.9083, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:49,750]: Epoch: 143, Loss:0.3621 Train: 0.9083, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:49,760]: Epoch: 144, Loss:0.3498 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0092
[2025-04-01 02:44:49,768]: Epoch: 145, Loss:0.3154 Train: 0.9083, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:49,776]: Epoch: 146, Loss:0.3193 Train: 0.9000, Val:0.3750, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:49,786]: Epoch: 147, Loss:0.3813 Train: 0.9000, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0101
[2025-04-01 02:44:49,796]: Epoch: 148, Loss:0.3689 Train: 0.9083, Val:0.3750, Test: 0.4314, Time(s/epoch):0.0094
[2025-04-01 02:44:49,805]: Epoch: 149, Loss:0.3375 Train: 0.9083, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0088
[2025-04-01 02:44:49,811]: Epoch: 150, Loss:0.3131 Train: 0.9250, Val:0.4375, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:49,820]: Epoch: 151, Loss:0.3043 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0086
[2025-04-01 02:44:49,827]: Epoch: 152, Loss:0.3495 Train: 0.9500, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:49,836]: Epoch: 153, Loss:0.3119 Train: 0.9167, Val:0.3875, Test: 0.3922, Time(s/epoch):0.0086
[2025-04-01 02:44:49,842]: Epoch: 154, Loss:0.3452 Train: 0.9000, Val:0.4000, Test: 0.3922, Time(s/epoch):0.0056
[2025-04-01 02:44:49,849]: Epoch: 155, Loss:0.3305 Train: 0.9167, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:49,856]: Epoch: 156, Loss:0.3320 Train: 0.9250, Val:0.3875, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:49,863]: Epoch: 157, Loss:0.3432 Train: 0.9333, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:49,872]: Epoch: 158, Loss:0.3221 Train: 0.9000, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0090
[2025-04-01 02:44:49,880]: Epoch: 159, Loss:0.3863 Train: 0.9167, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:49,888]: Epoch: 160, Loss:0.3097 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:49,897]: Epoch: 161, Loss:0.3276 Train: 0.8667, Val:0.4250, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:49,904]: Epoch: 162, Loss:0.3584 Train: 0.8417, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:49,911]: Epoch: 163, Loss:0.4145 Train: 0.9000, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:49,918]: Epoch: 164, Loss:0.3529 Train: 0.9333, Val:0.4625, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:49,926]: Epoch: 165, Loss:0.3698 Train: 0.9000, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:49,933]: Epoch: 166, Loss:0.5704 Train: 0.9083, Val:0.4125, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:49,942]: Epoch: 167, Loss:0.3566 Train: 0.8917, Val:0.4250, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:49,948]: Epoch: 168, Loss:0.4724 Train: 0.9167, Val:0.4000, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:49,955]: Epoch: 169, Loss:0.3084 Train: 0.9167, Val:0.4000, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:49,964]: Epoch: 170, Loss:0.3496 Train: 0.9000, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0085
[2025-04-01 02:44:49,970]: Epoch: 171, Loss:0.4101 Train: 0.9167, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0058
[2025-04-01 02:44:49,977]: Epoch: 172, Loss:0.3364 Train: 0.8917, Val:0.4375, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:49,984]: Epoch: 173, Loss:0.3810 Train: 0.9000, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:49,990]: Epoch: 174, Loss:0.4291 Train: 0.8667, Val:0.4125, Test: 0.4118, Time(s/epoch):0.0056
[2025-04-01 02:44:49,997]: Epoch: 175, Loss:0.3350 Train: 0.9167, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:50,004]: Epoch: 176, Loss:0.3333 Train: 0.9167, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:50,012]: Epoch: 177, Loss:0.2922 Train: 0.8917, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:50,019]: Epoch: 178, Loss:0.4440 Train: 0.9417, Val:0.4375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:50,028]: Epoch: 179, Loss:0.3129 Train: 0.9250, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:50,036]: Epoch: 180, Loss:0.3880 Train: 0.9250, Val:0.4750, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:50,041]: Epoch: 181, Loss:0.2829 Train: 0.9333, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0054
[2025-04-01 02:44:50,048]: Epoch: 182, Loss:0.3902 Train: 0.9000, Val:0.4500, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:50,056]: Epoch: 183, Loss:0.3274 Train: 0.9083, Val:0.4500, Test: 0.3529, Time(s/epoch):0.0077
[2025-04-01 02:44:50,063]: Epoch: 184, Loss:0.3729 Train: 0.9333, Val:0.4125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:50,071]: Epoch: 185, Loss:0.3357 Train: 0.9417, Val:0.4500, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:50,079]: Epoch: 186, Loss:0.3174 Train: 0.9500, Val:0.4250, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:50,086]: Epoch: 187, Loss:0.3040 Train: 0.9750, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:50,094]: Epoch: 188, Loss:0.3398 Train: 0.9417, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:50,100]: Epoch: 189, Loss:0.3341 Train: 0.9250, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0058
[2025-04-01 02:44:50,107]: Epoch: 190, Loss:0.3164 Train: 0.9250, Val:0.4125, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:50,113]: Epoch: 191, Loss:0.4277 Train: 0.9417, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:50,121]: Epoch: 192, Loss:0.3450 Train: 0.9583, Val:0.3875, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:50,129]: Epoch: 193, Loss:0.2935 Train: 0.9667, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:50,137]: Epoch: 194, Loss:0.3075 Train: 0.9500, Val:0.3875, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:50,143]: Epoch: 195, Loss:0.3825 Train: 0.9417, Val:0.4000, Test: 0.4314, Time(s/epoch):0.0062
[2025-04-01 02:44:50,150]: Epoch: 196, Loss:0.2986 Train: 0.9167, Val:0.4125, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:50,158]: Epoch: 197, Loss:0.3523 Train: 0.9083, Val:0.4375, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:50,166]: Epoch: 198, Loss:0.3935 Train: 0.9083, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:50,172]: Epoch: 199, Loss:0.3948 Train: 0.9167, Val:0.4375, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:50,180]: Epoch: 200, Loss:0.3131 Train: 0.9250, Val:0.4000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:50,180]: [Run-3 score] {'train': 0.6583333333333333, 'val': 0.5, 'test': 0.49019607843137253}
[2025-04-01 02:44:50,180]: repeat 1/3
[2025-04-01 02:44:50,180]: Manual random seed:0
[2025-04-01 02:44:50,181]: auto fixed data split seed to 0, model init seed to 0
[2025-04-01 02:44:50,184]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:50,194]: Epoch: 001, Loss:1.6910 Train: 0.4833, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:50,202]: Epoch: 002, Loss:2.3727 Train: 0.6250, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:50,211]: Epoch: 003, Loss:1.4455 Train: 0.6083, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0087
[2025-04-01 02:44:50,219]: Epoch: 004, Loss:1.1911 Train: 0.6250, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:50,227]: Epoch: 005, Loss:1.0296 Train: 0.6500, Val:0.5625, Test: 0.4902, Time(s/epoch):0.0080
[2025-04-01 02:44:50,236]: Epoch: 006, Loss:1.0008 Train: 0.6667, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0086
[2025-04-01 02:44:50,243]: Epoch: 007, Loss:1.0462 Train: 0.6833, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:50,252]: Epoch: 008, Loss:1.0676 Train: 0.6667, Val:0.5750, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:50,260]: Epoch: 009, Loss:1.0413 Train: 0.6417, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0078
[2025-04-01 02:44:50,268]: Epoch: 010, Loss:0.8615 Train: 0.6583, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:50,275]: Epoch: 011, Loss:0.9293 Train: 0.6750, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:50,282]: Epoch: 012, Loss:0.8677 Train: 0.6667, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:50,289]: Epoch: 013, Loss:0.8573 Train: 0.6833, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:50,297]: Epoch: 014, Loss:0.7698 Train: 0.7083, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:50,306]: Epoch: 015, Loss:0.8544 Train: 0.7250, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:50,315]: Epoch: 016, Loss:0.8358 Train: 0.7500, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0095
[2025-04-01 02:44:50,324]: Epoch: 017, Loss:0.7400 Train: 0.7417, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0087
[2025-04-01 02:44:50,332]: Epoch: 018, Loss:0.7565 Train: 0.7500, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:50,339]: Epoch: 019, Loss:0.7112 Train: 0.7833, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:50,347]: Epoch: 020, Loss:0.7791 Train: 0.7167, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:50,355]: Epoch: 021, Loss:0.7047 Train: 0.7167, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:50,362]: Epoch: 022, Loss:0.7443 Train: 0.7500, Val:0.4875, Test: 0.4706, Time(s/epoch):0.0070
[2025-04-01 02:44:50,369]: Epoch: 023, Loss:0.7052 Train: 0.7583, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:50,377]: Epoch: 024, Loss:0.7080 Train: 0.7917, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:50,384]: Epoch: 025, Loss:0.6537 Train: 0.7667, Val:0.5625, Test: 0.4902, Time(s/epoch):0.0063
[2025-04-01 02:44:50,390]: Epoch: 026, Loss:0.6953 Train: 0.7500, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0060
[2025-04-01 02:44:50,398]: Epoch: 027, Loss:0.6723 Train: 0.7500, Val:0.5750, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:50,406]: Epoch: 028, Loss:0.5961 Train: 0.7583, Val:0.5875, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:50,414]: Epoch: 029, Loss:0.7002 Train: 0.7667, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:50,422]: Epoch: 030, Loss:0.6206 Train: 0.7917, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:50,430]: Epoch: 031, Loss:0.6504 Train: 0.8000, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0078
[2025-04-01 02:44:50,436]: Epoch: 032, Loss:0.6151 Train: 0.8167, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:50,443]: Epoch: 033, Loss:0.6290 Train: 0.8000, Val:0.6000, Test: 0.4706, Time(s/epoch):0.0068
[2025-04-01 02:44:50,451]: Epoch: 034, Loss:0.5869 Train: 0.8083, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:50,460]: Epoch: 035, Loss:0.5679 Train: 0.7917, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:50,468]: Epoch: 036, Loss:0.5604 Train: 0.8250, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:50,475]: Epoch: 037, Loss:0.5545 Train: 0.8333, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:50,483]: Epoch: 038, Loss:0.5884 Train: 0.8333, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:50,491]: Epoch: 039, Loss:0.5558 Train: 0.8417, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:50,498]: Epoch: 040, Loss:0.5531 Train: 0.7833, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:50,506]: Epoch: 041, Loss:0.6086 Train: 0.8500, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:50,512]: Epoch: 042, Loss:0.5501 Train: 0.8250, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0061
[2025-04-01 02:44:50,521]: Epoch: 043, Loss:0.5598 Train: 0.8083, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:50,528]: Epoch: 044, Loss:0.5430 Train: 0.8333, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:50,536]: Epoch: 045, Loss:0.5724 Train: 0.8500, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0084
[2025-04-01 02:44:50,543]: Epoch: 046, Loss:0.5629 Train: 0.8417, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:50,550]: Epoch: 047, Loss:0.5535 Train: 0.8417, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:50,558]: Epoch: 048, Loss:0.5417 Train: 0.8417, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:50,566]: Epoch: 049, Loss:0.5102 Train: 0.8417, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0076
[2025-04-01 02:44:50,574]: Epoch: 050, Loss:0.5138 Train: 0.9000, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:50,581]: Epoch: 051, Loss:0.6004 Train: 0.8583, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:50,589]: Epoch: 052, Loss:0.5022 Train: 0.8583, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:50,596]: Epoch: 053, Loss:0.4974 Train: 0.8333, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:50,604]: Epoch: 054, Loss:0.4723 Train: 0.8167, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0079
[2025-04-01 02:44:50,610]: Epoch: 055, Loss:0.5161 Train: 0.8583, Val:0.5875, Test: 0.3725, Time(s/epoch):0.0058
[2025-04-01 02:44:50,618]: Epoch: 056, Loss:0.5503 Train: 0.8417, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:50,624]: Epoch: 057, Loss:0.4986 Train: 0.8583, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:50,631]: Epoch: 058, Loss:0.4802 Train: 0.8750, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:50,638]: Epoch: 059, Loss:0.5399 Train: 0.8750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:50,644]: Epoch: 060, Loss:0.5081 Train: 0.8167, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:50,652]: Epoch: 061, Loss:0.5148 Train: 0.7833, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:50,660]: Epoch: 062, Loss:0.5122 Train: 0.8167, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:50,667]: Epoch: 063, Loss:0.5244 Train: 0.8833, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:50,673]: Epoch: 064, Loss:0.5664 Train: 0.8583, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:50,680]: Epoch: 065, Loss:0.5238 Train: 0.8667, Val:0.6000, Test: 0.4510, Time(s/epoch):0.0064
[2025-04-01 02:44:50,687]: Epoch: 066, Loss:0.4820 Train: 0.8583, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:50,695]: Epoch: 067, Loss:0.5170 Train: 0.8500, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:50,703]: Epoch: 068, Loss:0.5798 Train: 0.8833, Val:0.6000, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:50,710]: Epoch: 069, Loss:0.5044 Train: 0.8333, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:50,718]: Epoch: 070, Loss:0.5191 Train: 0.8583, Val:0.5875, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:50,726]: Epoch: 071, Loss:0.5037 Train: 0.8667, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:50,736]: Epoch: 072, Loss:0.4819 Train: 0.8500, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0095
[2025-04-01 02:44:50,745]: Epoch: 073, Loss:0.5937 Train: 0.8083, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0088
[2025-04-01 02:44:50,751]: Epoch: 074, Loss:0.4810 Train: 0.7750, Val:0.5125, Test: 0.4706, Time(s/epoch):0.0059
[2025-04-01 02:44:50,758]: Epoch: 075, Loss:0.5395 Train: 0.8583, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:50,766]: Epoch: 076, Loss:0.5549 Train: 0.8917, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:50,776]: Epoch: 077, Loss:0.4827 Train: 0.8667, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0088
[2025-04-01 02:44:50,783]: Epoch: 078, Loss:0.5179 Train: 0.8667, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:50,790]: Epoch: 079, Loss:0.5414 Train: 0.8917, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:50,799]: Epoch: 080, Loss:0.5098 Train: 0.8667, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0083
[2025-04-01 02:44:50,806]: Epoch: 081, Loss:0.5167 Train: 0.8250, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:50,812]: Epoch: 082, Loss:0.5054 Train: 0.8000, Val:0.5000, Test: 0.3725, Time(s/epoch):0.0059
[2025-04-01 02:44:50,820]: Epoch: 083, Loss:0.5578 Train: 0.8000, Val:0.5375, Test: 0.3529, Time(s/epoch):0.0077
[2025-04-01 02:44:50,828]: Epoch: 084, Loss:0.5192 Train: 0.8583, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:50,836]: Epoch: 085, Loss:0.4776 Train: 0.8833, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:50,842]: Epoch: 086, Loss:0.5172 Train: 0.8750, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:50,850]: Epoch: 087, Loss:0.5099 Train: 0.8667, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:50,858]: Epoch: 088, Loss:0.5049 Train: 0.8917, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:50,866]: Epoch: 089, Loss:0.4472 Train: 0.8667, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:50,872]: Epoch: 090, Loss:0.4933 Train: 0.8833, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:50,879]: Epoch: 091, Loss:0.5911 Train: 0.8583, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0066
[2025-04-01 02:44:50,887]: Epoch: 092, Loss:0.4592 Train: 0.8417, Val:0.5125, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:50,893]: Epoch: 093, Loss:0.4906 Train: 0.8167, Val:0.5000, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:50,900]: Epoch: 094, Loss:0.4937 Train: 0.8333, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:50,908]: Epoch: 095, Loss:0.5308 Train: 0.8583, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:50,915]: Epoch: 096, Loss:0.4867 Train: 0.8667, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:50,921]: Epoch: 097, Loss:0.5295 Train: 0.8333, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:50,930]: Epoch: 098, Loss:0.5152 Train: 0.8417, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:50,937]: Epoch: 099, Loss:0.5197 Train: 0.8667, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:50,945]: Epoch: 100, Loss:0.4505 Train: 0.8833, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0084
[2025-04-01 02:44:50,953]: Epoch: 101, Loss:0.4648 Train: 0.8750, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0071
[2025-04-01 02:44:50,961]: Epoch: 102, Loss:0.4636 Train: 0.8917, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0086
[2025-04-01 02:44:50,968]: Epoch: 103, Loss:0.4669 Train: 0.9167, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:50,975]: Epoch: 104, Loss:0.5066 Train: 0.8750, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:50,983]: Epoch: 105, Loss:0.4676 Train: 0.8833, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:50,990]: Epoch: 106, Loss:0.4999 Train: 0.8750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:50,998]: Epoch: 107, Loss:0.4610 Train: 0.8917, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:51,006]: Epoch: 108, Loss:0.4591 Train: 0.8917, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:51,013]: Epoch: 109, Loss:0.4530 Train: 0.9083, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:51,021]: Epoch: 110, Loss:0.4606 Train: 0.9000, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0074
[2025-04-01 02:44:51,029]: Epoch: 111, Loss:0.4221 Train: 0.8750, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:51,036]: Epoch: 112, Loss:0.4512 Train: 0.9083, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:51,042]: Epoch: 113, Loss:0.4374 Train: 0.9083, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:51,050]: Epoch: 114, Loss:0.4152 Train: 0.9000, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:51,057]: Epoch: 115, Loss:0.4860 Train: 0.8750, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:51,065]: Epoch: 116, Loss:0.5210 Train: 0.8750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:51,073]: Epoch: 117, Loss:0.4835 Train: 0.8667, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:51,079]: Epoch: 118, Loss:0.4575 Train: 0.8500, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:51,086]: Epoch: 119, Loss:0.5312 Train: 0.8167, Val:0.5375, Test: 0.3333, Time(s/epoch):0.0063
[2025-04-01 02:44:51,093]: Epoch: 120, Loss:0.5037 Train: 0.8500, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:51,101]: Epoch: 121, Loss:0.5262 Train: 0.8833, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:51,108]: Epoch: 122, Loss:0.4160 Train: 0.8583, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:51,115]: Epoch: 123, Loss:0.5961 Train: 0.8833, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:51,122]: Epoch: 124, Loss:0.4873 Train: 0.8583, Val:0.5500, Test: 0.3333, Time(s/epoch):0.0063
[2025-04-01 02:44:51,130]: Epoch: 125, Loss:0.5015 Train: 0.8667, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0079
[2025-04-01 02:44:51,137]: Epoch: 126, Loss:0.4240 Train: 0.9000, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:51,145]: Epoch: 127, Loss:0.4094 Train: 0.8917, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:51,152]: Epoch: 128, Loss:0.4704 Train: 0.8833, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:51,159]: Epoch: 129, Loss:0.4664 Train: 0.8917, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:51,167]: Epoch: 130, Loss:0.4632 Train: 0.8667, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:51,175]: Epoch: 131, Loss:0.4213 Train: 0.8750, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:51,182]: Epoch: 132, Loss:0.4477 Train: 0.8667, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:51,190]: Epoch: 133, Loss:0.4666 Train: 0.8750, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:51,199]: Epoch: 134, Loss:0.4723 Train: 0.8167, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0083
[2025-04-01 02:44:51,205]: Epoch: 135, Loss:0.5286 Train: 0.8750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0062
[2025-04-01 02:44:51,213]: Epoch: 136, Loss:0.4582 Train: 0.9000, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:51,219]: Epoch: 137, Loss:0.3857 Train: 0.8833, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:51,226]: Epoch: 138, Loss:0.4400 Train: 0.8917, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:51,234]: Epoch: 139, Loss:0.4391 Train: 0.9000, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:51,240]: Epoch: 140, Loss:0.4486 Train: 0.9000, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:51,248]: Epoch: 141, Loss:0.4500 Train: 0.9000, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0074
[2025-04-01 02:44:51,255]: Epoch: 142, Loss:0.4184 Train: 0.9000, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:51,262]: Epoch: 143, Loss:0.4370 Train: 0.8833, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:51,269]: Epoch: 144, Loss:0.3995 Train: 0.9000, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:51,276]: Epoch: 145, Loss:0.4195 Train: 0.8833, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:51,285]: Epoch: 146, Loss:0.4338 Train: 0.8833, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:51,292]: Epoch: 147, Loss:0.4809 Train: 0.8917, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:51,301]: Epoch: 148, Loss:0.3947 Train: 0.8833, Val:0.5500, Test: 0.3137, Time(s/epoch):0.0089
[2025-04-01 02:44:51,313]: Epoch: 149, Loss:0.3994 Train: 0.8917, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0112
[2025-04-01 02:44:51,325]: Epoch: 150, Loss:0.4368 Train: 0.9000, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0124
[2025-04-01 02:44:51,335]: Epoch: 151, Loss:0.5290 Train: 0.9083, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0095
[2025-04-01 02:44:51,344]: Epoch: 152, Loss:0.3937 Train: 0.9000, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0085
[2025-04-01 02:44:51,351]: Epoch: 153, Loss:0.4029 Train: 0.8917, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:51,359]: Epoch: 154, Loss:0.4426 Train: 0.8667, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:51,367]: Epoch: 155, Loss:0.4042 Train: 0.8750, Val:0.5875, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:51,375]: Epoch: 156, Loss:0.4799 Train: 0.8833, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:51,382]: Epoch: 157, Loss:0.4147 Train: 0.9167, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:51,388]: Epoch: 158, Loss:0.4202 Train: 0.8833, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0065
[2025-04-01 02:44:51,396]: Epoch: 159, Loss:0.4466 Train: 0.8917, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:51,404]: Epoch: 160, Loss:0.4592 Train: 0.9000, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0082
[2025-04-01 02:44:51,411]: Epoch: 161, Loss:0.4110 Train: 0.8917, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0060
[2025-04-01 02:44:51,417]: Epoch: 162, Loss:0.4104 Train: 0.8333, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:51,425]: Epoch: 163, Loss:0.4553 Train: 0.8833, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:51,431]: Epoch: 164, Loss:0.4124 Train: 0.8917, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0055
[2025-04-01 02:44:51,439]: Epoch: 165, Loss:0.3905 Train: 0.8667, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:51,446]: Epoch: 166, Loss:0.4597 Train: 0.8750, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:51,453]: Epoch: 167, Loss:0.4226 Train: 0.9167, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:51,462]: Epoch: 168, Loss:0.4020 Train: 0.9250, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:51,468]: Epoch: 169, Loss:0.4145 Train: 0.9167, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:51,474]: Epoch: 170, Loss:0.4125 Train: 0.9250, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:51,481]: Epoch: 171, Loss:0.4280 Train: 0.9000, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:51,488]: Epoch: 172, Loss:0.4529 Train: 0.9000, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0072
[2025-04-01 02:44:51,496]: Epoch: 173, Loss:0.4259 Train: 0.9083, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:51,504]: Epoch: 174, Loss:0.3729 Train: 0.8917, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0079
[2025-04-01 02:44:51,512]: Epoch: 175, Loss:0.4502 Train: 0.8917, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:51,519]: Epoch: 176, Loss:0.4522 Train: 0.9083, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:51,527]: Epoch: 177, Loss:0.3655 Train: 0.9000, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:51,535]: Epoch: 178, Loss:0.4089 Train: 0.9083, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:51,542]: Epoch: 179, Loss:0.4198 Train: 0.9083, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:51,550]: Epoch: 180, Loss:0.4111 Train: 0.9167, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:51,558]: Epoch: 181, Loss:0.4605 Train: 0.9417, Val:0.5875, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:51,564]: Epoch: 182, Loss:0.3626 Train: 0.8667, Val:0.5125, Test: 0.3529, Time(s/epoch):0.0063
[2025-04-01 02:44:51,571]: Epoch: 183, Loss:0.4299 Train: 0.8417, Val:0.5000, Test: 0.3725, Time(s/epoch):0.0062
[2025-04-01 02:44:51,578]: Epoch: 184, Loss:0.4403 Train: 0.8583, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0070
[2025-04-01 02:44:51,584]: Epoch: 185, Loss:0.4798 Train: 0.8833, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:51,591]: Epoch: 186, Loss:0.4062 Train: 0.8833, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:51,599]: Epoch: 187, Loss:0.4800 Train: 0.9000, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:51,607]: Epoch: 188, Loss:0.4278 Train: 0.8833, Val:0.5875, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:51,612]: Epoch: 189, Loss:0.4478 Train: 0.8833, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0055
[2025-04-01 02:44:51,620]: Epoch: 190, Loss:0.4418 Train: 0.8417, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:51,627]: Epoch: 191, Loss:0.4498 Train: 0.8333, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:51,634]: Epoch: 192, Loss:0.4890 Train: 0.8583, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:51,641]: Epoch: 193, Loss:0.4384 Train: 0.8750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:51,648]: Epoch: 194, Loss:0.3693 Train: 0.8833, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:51,656]: Epoch: 195, Loss:0.4115 Train: 0.8667, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:51,662]: Epoch: 196, Loss:0.3971 Train: 0.8750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:51,669]: Epoch: 197, Loss:0.4901 Train: 0.8833, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:51,676]: Epoch: 198, Loss:0.3525 Train: 0.8750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:51,685]: Epoch: 199, Loss:0.4065 Train: 0.8750, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:51,691]: Epoch: 200, Loss:0.3620 Train: 0.8667, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:51,691]: [Run-1 score] {'train': 0.8, 'val': 0.6, 'test': 0.47058823529411764}
[2025-04-01 02:44:51,691]: repeat 2/3
[2025-04-01 02:44:51,691]: Manual random seed:0
[2025-04-01 02:44:51,692]: auto fixed data split seed to 0, model init seed to 1
[2025-04-01 02:44:51,695]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:51,705]: Epoch: 001, Loss:1.6398 Train: 0.5000, Val:0.4625, Test: 0.4510, Time(s/epoch):0.0079
[2025-04-01 02:44:51,713]: Epoch: 002, Loss:1.8346 Train: 0.3250, Val:0.2250, Test: 0.3137, Time(s/epoch):0.0071
[2025-04-01 02:44:51,719]: Epoch: 003, Loss:1.8107 Train: 0.5500, Val:0.4500, Test: 0.4118, Time(s/epoch):0.0060
[2025-04-01 02:44:51,726]: Epoch: 004, Loss:1.2734 Train: 0.6083, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0071
[2025-04-01 02:44:51,733]: Epoch: 005, Loss:1.0708 Train: 0.6333, Val:0.5375, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:51,740]: Epoch: 006, Loss:1.0749 Train: 0.6333, Val:0.5000, Test: 0.4706, Time(s/epoch):0.0075
[2025-04-01 02:44:51,748]: Epoch: 007, Loss:1.0545 Train: 0.6250, Val:0.4625, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:51,754]: Epoch: 008, Loss:1.0794 Train: 0.6333, Val:0.4875, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:51,761]: Epoch: 009, Loss:0.9950 Train: 0.6583, Val:0.5375, Test: 0.5294, Time(s/epoch):0.0065
[2025-04-01 02:44:51,769]: Epoch: 010, Loss:1.0329 Train: 0.6667, Val:0.5125, Test: 0.5098, Time(s/epoch):0.0079
[2025-04-01 02:44:51,776]: Epoch: 011, Loss:0.9606 Train: 0.7000, Val:0.5375, Test: 0.5098, Time(s/epoch):0.0066
[2025-04-01 02:44:51,784]: Epoch: 012, Loss:0.8775 Train: 0.6583, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:51,791]: Epoch: 013, Loss:0.9232 Train: 0.7000, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:51,798]: Epoch: 014, Loss:0.8903 Train: 0.6500, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:51,807]: Epoch: 015, Loss:0.8430 Train: 0.6583, Val:0.5875, Test: 0.5294, Time(s/epoch):0.0082
[2025-04-01 02:44:51,814]: Epoch: 016, Loss:0.8255 Train: 0.6583, Val:0.5625, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:51,823]: Epoch: 017, Loss:0.8750 Train: 0.6583, Val:0.5750, Test: 0.5294, Time(s/epoch):0.0084
[2025-04-01 02:44:51,831]: Epoch: 018, Loss:0.8165 Train: 0.6833, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0076
[2025-04-01 02:44:51,840]: Epoch: 019, Loss:0.7944 Train: 0.6917, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0088
[2025-04-01 02:44:51,849]: Epoch: 020, Loss:0.7880 Train: 0.6917, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0093
[2025-04-01 02:44:51,858]: Epoch: 021, Loss:0.7351 Train: 0.6917, Val:0.5625, Test: 0.4902, Time(s/epoch):0.0090
[2025-04-01 02:44:51,866]: Epoch: 022, Loss:0.7023 Train: 0.7250, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0079
[2025-04-01 02:44:51,876]: Epoch: 023, Loss:0.7483 Train: 0.7417, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0092
[2025-04-01 02:44:51,884]: Epoch: 024, Loss:0.7000 Train: 0.7583, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0078
[2025-04-01 02:44:51,892]: Epoch: 025, Loss:0.6780 Train: 0.7250, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0080
[2025-04-01 02:44:51,899]: Epoch: 026, Loss:0.7002 Train: 0.7083, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0068
[2025-04-01 02:44:51,909]: Epoch: 027, Loss:0.6867 Train: 0.6917, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0094
[2025-04-01 02:44:51,917]: Epoch: 028, Loss:0.7354 Train: 0.6917, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0080
[2025-04-01 02:44:51,926]: Epoch: 029, Loss:0.6559 Train: 0.7167, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0088
[2025-04-01 02:44:51,934]: Epoch: 030, Loss:0.6796 Train: 0.7333, Val:0.5875, Test: 0.4902, Time(s/epoch):0.0084
[2025-04-01 02:44:51,942]: Epoch: 031, Loss:0.6315 Train: 0.7583, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0073
[2025-04-01 02:44:51,950]: Epoch: 032, Loss:0.7206 Train: 0.7667, Val:0.5875, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:51,957]: Epoch: 033, Loss:0.7045 Train: 0.7833, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:51,965]: Epoch: 034, Loss:0.6350 Train: 0.7500, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:51,971]: Epoch: 035, Loss:0.6424 Train: 0.7667, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0059
[2025-04-01 02:44:51,978]: Epoch: 036, Loss:0.6381 Train: 0.7667, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:51,985]: Epoch: 037, Loss:0.6145 Train: 0.8083, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:51,992]: Epoch: 038, Loss:0.6383 Train: 0.8000, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:51,999]: Epoch: 039, Loss:0.6643 Train: 0.8000, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:52,007]: Epoch: 040, Loss:0.6456 Train: 0.7833, Val:0.6000, Test: 0.4902, Time(s/epoch):0.0077
[2025-04-01 02:44:52,014]: Epoch: 041, Loss:0.6130 Train: 0.7667, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:52,021]: Epoch: 042, Loss:0.5737 Train: 0.7333, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0062
[2025-04-01 02:44:52,028]: Epoch: 043, Loss:0.6255 Train: 0.7417, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:52,036]: Epoch: 044, Loss:0.6065 Train: 0.7583, Val:0.6000, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:52,044]: Epoch: 045, Loss:0.6247 Train: 0.7833, Val:0.6125, Test: 0.4510, Time(s/epoch):0.0082
[2025-04-01 02:44:52,053]: Epoch: 046, Loss:0.6211 Train: 0.8167, Val:0.6000, Test: 0.4510, Time(s/epoch):0.0081
[2025-04-01 02:44:52,060]: Epoch: 047, Loss:0.6036 Train: 0.8250, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:52,068]: Epoch: 048, Loss:0.6253 Train: 0.8167, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:52,076]: Epoch: 049, Loss:0.5718 Train: 0.7917, Val:0.6000, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:52,083]: Epoch: 050, Loss:0.6153 Train: 0.7833, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:52,089]: Epoch: 051, Loss:0.5956 Train: 0.7750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:52,097]: Epoch: 052, Loss:0.6054 Train: 0.8250, Val:0.6000, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:52,105]: Epoch: 053, Loss:0.5579 Train: 0.8167, Val:0.6000, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:52,112]: Epoch: 054, Loss:0.5965 Train: 0.8333, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:52,118]: Epoch: 055, Loss:0.6021 Train: 0.8000, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:52,126]: Epoch: 056, Loss:0.6077 Train: 0.8333, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:52,134]: Epoch: 057, Loss:0.6351 Train: 0.8500, Val:0.6125, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:52,140]: Epoch: 058, Loss:0.5414 Train: 0.8333, Val:0.6125, Test: 0.3922, Time(s/epoch):0.0062
[2025-04-01 02:44:52,148]: Epoch: 059, Loss:0.5693 Train: 0.8500, Val:0.6250, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:52,155]: Epoch: 060, Loss:0.5617 Train: 0.7917, Val:0.6125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:52,161]: Epoch: 061, Loss:0.5510 Train: 0.8000, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0060
[2025-04-01 02:44:52,170]: Epoch: 062, Loss:0.5724 Train: 0.8000, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:52,177]: Epoch: 063, Loss:0.5328 Train: 0.8083, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:52,184]: Epoch: 064, Loss:0.5536 Train: 0.8167, Val:0.6000, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:52,191]: Epoch: 065, Loss:0.5170 Train: 0.8167, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:52,198]: Epoch: 066, Loss:0.5282 Train: 0.7917, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:52,206]: Epoch: 067, Loss:0.5062 Train: 0.8250, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:52,212]: Epoch: 068, Loss:0.5553 Train: 0.7917, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0057
[2025-04-01 02:44:52,219]: Epoch: 069, Loss:0.5244 Train: 0.8750, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:52,227]: Epoch: 070, Loss:0.5793 Train: 0.8833, Val:0.6000, Test: 0.4706, Time(s/epoch):0.0074
[2025-04-01 02:44:52,235]: Epoch: 071, Loss:0.5393 Train: 0.8083, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:52,242]: Epoch: 072, Loss:0.5279 Train: 0.8167, Val:0.6125, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:52,250]: Epoch: 073, Loss:0.5445 Train: 0.8083, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:52,258]: Epoch: 074, Loss:0.4790 Train: 0.7833, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:52,266]: Epoch: 075, Loss:0.5443 Train: 0.8167, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:52,274]: Epoch: 076, Loss:0.6043 Train: 0.8083, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:52,282]: Epoch: 077, Loss:0.5069 Train: 0.7000, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0072
[2025-04-01 02:44:52,290]: Epoch: 078, Loss:0.6785 Train: 0.7500, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0087
[2025-04-01 02:44:52,299]: Epoch: 079, Loss:0.5714 Train: 0.7917, Val:0.6000, Test: 0.4706, Time(s/epoch):0.0085
[2025-04-01 02:44:52,308]: Epoch: 080, Loss:0.5556 Train: 0.8250, Val:0.5250, Test: 0.4510, Time(s/epoch):0.0085
[2025-04-01 02:44:52,319]: Epoch: 081, Loss:0.6615 Train: 0.8417, Val:0.4875, Test: 0.4118, Time(s/epoch):0.0113
[2025-04-01 02:44:52,327]: Epoch: 082, Loss:0.5975 Train: 0.8333, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:52,337]: Epoch: 083, Loss:0.5104 Train: 0.8583, Val:0.5750, Test: 0.3529, Time(s/epoch):0.0092
[2025-04-01 02:44:52,346]: Epoch: 084, Loss:0.5514 Train: 0.8333, Val:0.5500, Test: 0.3333, Time(s/epoch):0.0088
[2025-04-01 02:44:52,352]: Epoch: 085, Loss:0.5353 Train: 0.8750, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0066
[2025-04-01 02:44:52,361]: Epoch: 086, Loss:0.5151 Train: 0.8833, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:52,369]: Epoch: 087, Loss:0.5089 Train: 0.8583, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:52,377]: Epoch: 088, Loss:0.5340 Train: 0.8583, Val:0.6125, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:52,384]: Epoch: 089, Loss:0.5304 Train: 0.8583, Val:0.6000, Test: 0.4314, Time(s/epoch):0.0073
[2025-04-01 02:44:52,392]: Epoch: 090, Loss:0.4977 Train: 0.8583, Val:0.6125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:52,400]: Epoch: 091, Loss:0.4895 Train: 0.8500, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:52,406]: Epoch: 092, Loss:0.4941 Train: 0.8417, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:52,413]: Epoch: 093, Loss:0.4675 Train: 0.8250, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:52,420]: Epoch: 094, Loss:0.4985 Train: 0.8250, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:52,426]: Epoch: 095, Loss:0.5075 Train: 0.8500, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:52,433]: Epoch: 096, Loss:0.5061 Train: 0.8500, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:52,441]: Epoch: 097, Loss:0.5640 Train: 0.8583, Val:0.6125, Test: 0.4510, Time(s/epoch):0.0074
[2025-04-01 02:44:52,448]: Epoch: 098, Loss:0.5366 Train: 0.8417, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:52,456]: Epoch: 099, Loss:0.4589 Train: 0.8000, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:52,464]: Epoch: 100, Loss:0.5103 Train: 0.8333, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:52,470]: Epoch: 101, Loss:0.5114 Train: 0.8667, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0059
[2025-04-01 02:44:52,476]: Epoch: 102, Loss:0.4401 Train: 0.8417, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:52,484]: Epoch: 103, Loss:0.5399 Train: 0.8417, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:52,492]: Epoch: 104, Loss:0.4862 Train: 0.8667, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:52,499]: Epoch: 105, Loss:0.4897 Train: 0.8500, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:52,505]: Epoch: 106, Loss:0.4349 Train: 0.7667, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0064
[2025-04-01 02:44:52,513]: Epoch: 107, Loss:0.5121 Train: 0.8000, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0081
[2025-04-01 02:44:52,520]: Epoch: 108, Loss:0.4908 Train: 0.8833, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:52,527]: Epoch: 109, Loss:0.4526 Train: 0.8750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:52,536]: Epoch: 110, Loss:0.4772 Train: 0.8667, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:52,543]: Epoch: 111, Loss:0.4662 Train: 0.8750, Val:0.6125, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:52,552]: Epoch: 112, Loss:0.5273 Train: 0.8333, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:52,559]: Epoch: 113, Loss:0.4731 Train: 0.8250, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:52,566]: Epoch: 114, Loss:0.4950 Train: 0.8083, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0069
[2025-04-01 02:44:52,574]: Epoch: 115, Loss:0.4705 Train: 0.8417, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:52,580]: Epoch: 116, Loss:0.4277 Train: 0.8833, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:52,588]: Epoch: 117, Loss:0.4671 Train: 0.8917, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0075
[2025-04-01 02:44:52,596]: Epoch: 118, Loss:0.4721 Train: 0.8917, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:52,604]: Epoch: 119, Loss:0.4412 Train: 0.8750, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0081
[2025-04-01 02:44:52,610]: Epoch: 120, Loss:0.4853 Train: 0.8333, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:52,616]: Epoch: 121, Loss:0.4948 Train: 0.8083, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0064
[2025-04-01 02:44:52,624]: Epoch: 122, Loss:0.4632 Train: 0.8500, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:52,632]: Epoch: 123, Loss:0.5229 Train: 0.8833, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:52,639]: Epoch: 124, Loss:0.4910 Train: 0.8833, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:52,646]: Epoch: 125, Loss:0.4918 Train: 0.8917, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:52,653]: Epoch: 126, Loss:0.4927 Train: 0.9250, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0070
[2025-04-01 02:44:52,661]: Epoch: 127, Loss:0.5455 Train: 0.8500, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:52,668]: Epoch: 128, Loss:0.4789 Train: 0.8083, Val:0.5125, Test: 0.4510, Time(s/epoch):0.0073
[2025-04-01 02:44:52,674]: Epoch: 129, Loss:0.4634 Train: 0.8417, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:52,681]: Epoch: 130, Loss:0.5104 Train: 0.8833, Val:0.5125, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:52,689]: Epoch: 131, Loss:0.4467 Train: 0.8917, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:52,697]: Epoch: 132, Loss:0.4452 Train: 0.8917, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0079
[2025-04-01 02:44:52,705]: Epoch: 133, Loss:0.4192 Train: 0.9083, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:52,711]: Epoch: 134, Loss:0.4377 Train: 0.8583, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0057
[2025-04-01 02:44:52,719]: Epoch: 135, Loss:0.4407 Train: 0.8167, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:52,726]: Epoch: 136, Loss:0.5502 Train: 0.8583, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:52,734]: Epoch: 137, Loss:0.4151 Train: 0.9000, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:52,742]: Epoch: 138, Loss:0.4673 Train: 0.9083, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:52,748]: Epoch: 139, Loss:0.4548 Train: 0.8917, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:52,756]: Epoch: 140, Loss:0.4171 Train: 0.9000, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:52,763]: Epoch: 141, Loss:0.4573 Train: 0.8667, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:52,770]: Epoch: 142, Loss:0.4530 Train: 0.8750, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:52,777]: Epoch: 143, Loss:0.4851 Train: 0.8583, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0073
[2025-04-01 02:44:52,785]: Epoch: 144, Loss:0.4490 Train: 0.8750, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:52,793]: Epoch: 145, Loss:0.4307 Train: 0.8500, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:52,801]: Epoch: 146, Loss:0.4631 Train: 0.8583, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0081
[2025-04-01 02:44:52,809]: Epoch: 147, Loss:0.4799 Train: 0.8667, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0080
[2025-04-01 02:44:52,818]: Epoch: 148, Loss:0.4436 Train: 0.8750, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0089
[2025-04-01 02:44:52,826]: Epoch: 149, Loss:0.5211 Train: 0.8833, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:52,836]: Epoch: 150, Loss:0.4442 Train: 0.8750, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0091
[2025-04-01 02:44:52,842]: Epoch: 151, Loss:0.5099 Train: 0.8667, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:52,849]: Epoch: 152, Loss:0.4090 Train: 0.8500, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:52,856]: Epoch: 153, Loss:0.4502 Train: 0.8750, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:52,863]: Epoch: 154, Loss:0.5022 Train: 0.8667, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:52,869]: Epoch: 155, Loss:0.4643 Train: 0.8750, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:52,878]: Epoch: 156, Loss:0.4773 Train: 0.9083, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0084
[2025-04-01 02:44:52,886]: Epoch: 157, Loss:0.4279 Train: 0.8583, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:52,893]: Epoch: 158, Loss:0.4166 Train: 0.8417, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:52,900]: Epoch: 159, Loss:0.5139 Train: 0.8833, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:52,907]: Epoch: 160, Loss:0.4592 Train: 0.8750, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:52,915]: Epoch: 161, Loss:0.5590 Train: 0.8833, Val:0.5000, Test: 0.3725, Time(s/epoch):0.0076
[2025-04-01 02:44:52,921]: Epoch: 162, Loss:0.5659 Train: 0.8750, Val:0.5000, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:52,928]: Epoch: 163, Loss:0.4833 Train: 0.8583, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:52,936]: Epoch: 164, Loss:0.5022 Train: 0.8667, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0074
[2025-04-01 02:44:52,944]: Epoch: 165, Loss:0.4852 Train: 0.8750, Val:0.5750, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:52,951]: Epoch: 166, Loss:0.4825 Train: 0.8583, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:52,958]: Epoch: 167, Loss:0.4839 Train: 0.8500, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0074
[2025-04-01 02:44:52,966]: Epoch: 168, Loss:0.4758 Train: 0.8417, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:52,974]: Epoch: 169, Loss:0.4906 Train: 0.8500, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:52,981]: Epoch: 170, Loss:0.4792 Train: 0.8333, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:52,988]: Epoch: 171, Loss:0.4218 Train: 0.8500, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:52,995]: Epoch: 172, Loss:0.4617 Train: 0.8750, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:53,002]: Epoch: 173, Loss:0.4215 Train: 0.8833, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:53,010]: Epoch: 174, Loss:0.4229 Train: 0.8583, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0078
[2025-04-01 02:44:53,017]: Epoch: 175, Loss:0.4988 Train: 0.8583, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0067
[2025-04-01 02:44:53,024]: Epoch: 176, Loss:0.4293 Train: 0.8750, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0067
[2025-04-01 02:44:53,030]: Epoch: 177, Loss:0.4250 Train: 0.9083, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:53,039]: Epoch: 178, Loss:0.3973 Train: 0.9167, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:53,045]: Epoch: 179, Loss:0.4647 Train: 0.9083, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:53,052]: Epoch: 180, Loss:0.4266 Train: 0.8917, Val:0.5250, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:53,059]: Epoch: 181, Loss:0.5042 Train: 0.9083, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:53,066]: Epoch: 182, Loss:0.4523 Train: 0.9083, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:53,073]: Epoch: 183, Loss:0.4325 Train: 0.8583, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:53,079]: Epoch: 184, Loss:0.4245 Train: 0.8917, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0061
[2025-04-01 02:44:53,087]: Epoch: 185, Loss:0.4481 Train: 0.8667, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:53,096]: Epoch: 186, Loss:0.4313 Train: 0.8333, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0088
[2025-04-01 02:44:53,102]: Epoch: 187, Loss:0.4008 Train: 0.8250, Val:0.5000, Test: 0.3725, Time(s/epoch):0.0056
[2025-04-01 02:44:53,110]: Epoch: 188, Loss:0.5148 Train: 0.9000, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:53,117]: Epoch: 189, Loss:0.4020 Train: 0.9083, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:53,125]: Epoch: 190, Loss:0.3960 Train: 0.8750, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:53,132]: Epoch: 191, Loss:0.4803 Train: 0.9083, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0064
[2025-04-01 02:44:53,139]: Epoch: 192, Loss:0.4290 Train: 0.8917, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0071
[2025-04-01 02:44:53,145]: Epoch: 193, Loss:0.4922 Train: 0.8500, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:53,153]: Epoch: 194, Loss:0.4433 Train: 0.8500, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:53,159]: Epoch: 195, Loss:0.4405 Train: 0.8750, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:53,167]: Epoch: 196, Loss:0.4253 Train: 0.8667, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0074
[2025-04-01 02:44:53,175]: Epoch: 197, Loss:0.3932 Train: 0.8750, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:53,181]: Epoch: 198, Loss:0.4303 Train: 0.8583, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0058
[2025-04-01 02:44:53,187]: Epoch: 199, Loss:0.3778 Train: 0.8333, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:53,194]: Epoch: 200, Loss:0.4935 Train: 0.8583, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0064
[2025-04-01 02:44:53,194]: [Run-2 score] {'train': 0.85, 'val': 0.625, 'test': 0.4117647058823529}
[2025-04-01 02:44:53,194]: repeat 3/3
[2025-04-01 02:44:53,194]: Manual random seed:0
[2025-04-01 02:44:53,195]: auto fixed data split seed to 0, model init seed to 2
[2025-04-01 02:44:53,197]: [Dataset-Wisconsin] train_num:120, val_num:80, test_num:51, class_num:5
[2025-04-01 02:44:53,207]: Epoch: 001, Loss:1.7090 Train: 0.5417, Val:0.4875, Test: 0.4314, Time(s/epoch):0.0071
[2025-04-01 02:44:53,213]: Epoch: 002, Loss:1.3569 Train: 0.5417, Val:0.4500, Test: 0.4314, Time(s/epoch):0.0057
[2025-04-01 02:44:53,219]: Epoch: 003, Loss:1.2773 Train: 0.6167, Val:0.5000, Test: 0.5294, Time(s/epoch):0.0060
[2025-04-01 02:44:53,226]: Epoch: 004, Loss:1.3371 Train: 0.6250, Val:0.5500, Test: 0.5490, Time(s/epoch):0.0067
[2025-04-01 02:44:53,233]: Epoch: 005, Loss:1.2938 Train: 0.6500, Val:0.5500, Test: 0.5098, Time(s/epoch):0.0073
[2025-04-01 02:44:53,241]: Epoch: 006, Loss:0.9587 Train: 0.6750, Val:0.5250, Test: 0.4706, Time(s/epoch):0.0072
[2025-04-01 02:44:53,247]: Epoch: 007, Loss:0.9807 Train: 0.6833, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0067
[2025-04-01 02:44:53,254]: Epoch: 008, Loss:0.9596 Train: 0.6833, Val:0.5375, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:53,261]: Epoch: 009, Loss:0.9657 Train: 0.6750, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0064
[2025-04-01 02:44:53,268]: Epoch: 010, Loss:0.8622 Train: 0.6833, Val:0.5250, Test: 0.5294, Time(s/epoch):0.0075
[2025-04-01 02:44:53,275]: Epoch: 011, Loss:0.8714 Train: 0.6917, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0065
[2025-04-01 02:44:53,282]: Epoch: 012, Loss:0.8126 Train: 0.6833, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0067
[2025-04-01 02:44:53,290]: Epoch: 013, Loss:0.8928 Train: 0.6750, Val:0.5000, Test: 0.5098, Time(s/epoch):0.0086
[2025-04-01 02:44:53,298]: Epoch: 014, Loss:0.8868 Train: 0.7583, Val:0.5250, Test: 0.5098, Time(s/epoch):0.0071
[2025-04-01 02:44:53,305]: Epoch: 015, Loss:0.7522 Train: 0.7750, Val:0.5375, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:53,316]: Epoch: 016, Loss:0.8473 Train: 0.7333, Val:0.5500, Test: 0.4706, Time(s/epoch):0.0100
[2025-04-01 02:44:53,322]: Epoch: 017, Loss:0.7085 Train: 0.7333, Val:0.5375, Test: 0.4902, Time(s/epoch):0.0064
[2025-04-01 02:44:53,330]: Epoch: 018, Loss:0.7237 Train: 0.7417, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0077
[2025-04-01 02:44:53,338]: Epoch: 019, Loss:0.7081 Train: 0.7500, Val:0.5625, Test: 0.5098, Time(s/epoch):0.0081
[2025-04-01 02:44:53,346]: Epoch: 020, Loss:0.7211 Train: 0.7667, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0070
[2025-04-01 02:44:53,352]: Epoch: 021, Loss:0.6699 Train: 0.7500, Val:0.5750, Test: 0.4902, Time(s/epoch):0.0060
[2025-04-01 02:44:53,359]: Epoch: 022, Loss:0.6294 Train: 0.7667, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0068
[2025-04-01 02:44:53,366]: Epoch: 023, Loss:0.6579 Train: 0.7750, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0076
[2025-04-01 02:44:53,373]: Epoch: 024, Loss:0.6739 Train: 0.7583, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:53,381]: Epoch: 025, Loss:0.6302 Train: 0.7833, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0077
[2025-04-01 02:44:53,389]: Epoch: 026, Loss:0.6148 Train: 0.7750, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0076
[2025-04-01 02:44:53,397]: Epoch: 027, Loss:0.6385 Train: 0.7667, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:53,405]: Epoch: 028, Loss:0.6177 Train: 0.7750, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0081
[2025-04-01 02:44:53,410]: Epoch: 029, Loss:0.6594 Train: 0.7917, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0055
[2025-04-01 02:44:53,419]: Epoch: 030, Loss:0.5937 Train: 0.8167, Val:0.5875, Test: 0.4902, Time(s/epoch):0.0082
[2025-04-01 02:44:53,425]: Epoch: 031, Loss:0.5807 Train: 0.8083, Val:0.5625, Test: 0.4706, Time(s/epoch):0.0061
[2025-04-01 02:44:53,433]: Epoch: 032, Loss:0.5579 Train: 0.7750, Val:0.5500, Test: 0.4902, Time(s/epoch):0.0075
[2025-04-01 02:44:53,439]: Epoch: 033, Loss:0.6309 Train: 0.8417, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:53,447]: Epoch: 034, Loss:0.5821 Train: 0.8583, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:53,455]: Epoch: 035, Loss:0.6343 Train: 0.8417, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:53,463]: Epoch: 036, Loss:0.5768 Train: 0.8417, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0080
[2025-04-01 02:44:53,471]: Epoch: 037, Loss:0.5247 Train: 0.8083, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:53,479]: Epoch: 038, Loss:0.6272 Train: 0.7917, Val:0.6000, Test: 0.4314, Time(s/epoch):0.0078
[2025-04-01 02:44:53,486]: Epoch: 039, Loss:0.5334 Train: 0.7833, Val:0.6125, Test: 0.4314, Time(s/epoch):0.0072
[2025-04-01 02:44:53,493]: Epoch: 040, Loss:0.6083 Train: 0.8000, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0071
[2025-04-01 02:44:53,500]: Epoch: 041, Loss:0.6033 Train: 0.8250, Val:0.5875, Test: 0.4510, Time(s/epoch):0.0063
[2025-04-01 02:44:53,507]: Epoch: 042, Loss:0.5075 Train: 0.8417, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0068
[2025-04-01 02:44:53,515]: Epoch: 043, Loss:0.5582 Train: 0.8417, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:53,522]: Epoch: 044, Loss:0.6053 Train: 0.8333, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:53,530]: Epoch: 045, Loss:0.5552 Train: 0.8250, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:53,537]: Epoch: 046, Loss:0.5193 Train: 0.8250, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:53,545]: Epoch: 047, Loss:0.5616 Train: 0.8250, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:53,554]: Epoch: 048, Loss:0.5186 Train: 0.8083, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0082
[2025-04-01 02:44:53,561]: Epoch: 049, Loss:0.4977 Train: 0.8417, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:53,569]: Epoch: 050, Loss:0.5349 Train: 0.8333, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0083
[2025-04-01 02:44:53,576]: Epoch: 051, Loss:0.4863 Train: 0.8500, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0061
[2025-04-01 02:44:53,582]: Epoch: 052, Loss:0.4894 Train: 0.8500, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:53,589]: Epoch: 053, Loss:0.5121 Train: 0.8333, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:53,597]: Epoch: 054, Loss:0.5271 Train: 0.8500, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:53,606]: Epoch: 055, Loss:0.4915 Train: 0.8500, Val:0.6000, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:53,613]: Epoch: 056, Loss:0.4819 Train: 0.8917, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:53,621]: Epoch: 057, Loss:0.4550 Train: 0.8500, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:53,628]: Epoch: 058, Loss:0.5233 Train: 0.9000, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0077
[2025-04-01 02:44:53,636]: Epoch: 059, Loss:0.5150 Train: 0.9000, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:53,644]: Epoch: 060, Loss:0.4584 Train: 0.8833, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:53,652]: Epoch: 061, Loss:0.4899 Train: 0.8583, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:53,659]: Epoch: 062, Loss:0.4597 Train: 0.8333, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0063
[2025-04-01 02:44:53,667]: Epoch: 063, Loss:0.4652 Train: 0.8500, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:53,675]: Epoch: 064, Loss:0.5180 Train: 0.8333, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:53,681]: Epoch: 065, Loss:0.4485 Train: 0.8667, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0059
[2025-04-01 02:44:53,688]: Epoch: 066, Loss:0.4905 Train: 0.8333, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:53,696]: Epoch: 067, Loss:0.5115 Train: 0.8583, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:53,702]: Epoch: 068, Loss:0.5339 Train: 0.8667, Val:0.6250, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:53,709]: Epoch: 069, Loss:0.4248 Train: 0.8583, Val:0.6000, Test: 0.3922, Time(s/epoch):0.0068
[2025-04-01 02:44:53,717]: Epoch: 070, Loss:0.4795 Train: 0.8917, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0075
[2025-04-01 02:44:53,725]: Epoch: 071, Loss:0.4726 Train: 0.8917, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:53,731]: Epoch: 072, Loss:0.4223 Train: 0.8667, Val:0.5750, Test: 0.3529, Time(s/epoch):0.0058
[2025-04-01 02:44:53,738]: Epoch: 073, Loss:0.4519 Train: 0.8583, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:53,746]: Epoch: 074, Loss:0.4461 Train: 0.8000, Val:0.5125, Test: 0.3725, Time(s/epoch):0.0078
[2025-04-01 02:44:53,754]: Epoch: 075, Loss:0.5103 Train: 0.8750, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:53,761]: Epoch: 076, Loss:0.5007 Train: 0.8750, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:53,769]: Epoch: 077, Loss:0.5409 Train: 0.7917, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0081
[2025-04-01 02:44:53,777]: Epoch: 078, Loss:0.5470 Train: 0.8417, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0074
[2025-04-01 02:44:53,784]: Epoch: 079, Loss:0.5071 Train: 0.8417, Val:0.5375, Test: 0.3333, Time(s/epoch):0.0074
[2025-04-01 02:44:53,791]: Epoch: 080, Loss:0.5042 Train: 0.8583, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0063
[2025-04-01 02:44:53,799]: Epoch: 081, Loss:0.5336 Train: 0.8667, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:53,806]: Epoch: 082, Loss:0.5174 Train: 0.8833, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:53,811]: Epoch: 083, Loss:0.4812 Train: 0.8667, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0058
[2025-04-01 02:44:53,819]: Epoch: 084, Loss:0.4683 Train: 0.8083, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0076
[2025-04-01 02:44:53,825]: Epoch: 085, Loss:0.4604 Train: 0.8000, Val:0.5375, Test: 0.4510, Time(s/epoch):0.0060
[2025-04-01 02:44:53,832]: Epoch: 086, Loss:0.5108 Train: 0.8333, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0070
[2025-04-01 02:44:53,839]: Epoch: 087, Loss:0.5181 Train: 0.8667, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0066
[2025-04-01 02:44:53,846]: Epoch: 088, Loss:0.4707 Train: 0.8583, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:53,855]: Epoch: 089, Loss:0.5723 Train: 0.9083, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0081
[2025-04-01 02:44:53,862]: Epoch: 090, Loss:0.4483 Train: 0.8583, Val:0.5750, Test: 0.4314, Time(s/epoch):0.0076
[2025-04-01 02:44:53,869]: Epoch: 091, Loss:0.4532 Train: 0.8417, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0062
[2025-04-01 02:44:53,875]: Epoch: 092, Loss:0.5129 Train: 0.9000, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0064
[2025-04-01 02:44:53,883]: Epoch: 093, Loss:0.4313 Train: 0.8917, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:53,891]: Epoch: 094, Loss:0.4776 Train: 0.9083, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0080
[2025-04-01 02:44:53,900]: Epoch: 095, Loss:0.4023 Train: 0.8750, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:53,908]: Epoch: 096, Loss:0.4187 Train: 0.8667, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:53,916]: Epoch: 097, Loss:0.4205 Train: 0.8667, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0078
[2025-04-01 02:44:53,925]: Epoch: 098, Loss:0.4445 Train: 0.8667, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0092
[2025-04-01 02:44:53,934]: Epoch: 099, Loss:0.4366 Train: 0.8917, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0088
[2025-04-01 02:44:53,943]: Epoch: 100, Loss:0.3947 Train: 0.8750, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0083
[2025-04-01 02:44:53,952]: Epoch: 101, Loss:0.4306 Train: 0.8833, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0090
[2025-04-01 02:44:53,962]: Epoch: 102, Loss:0.4270 Train: 0.8583, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0099
[2025-04-01 02:44:53,972]: Epoch: 103, Loss:0.3941 Train: 0.8500, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0097
[2025-04-01 02:44:53,981]: Epoch: 104, Loss:0.4528 Train: 0.8667, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0086
[2025-04-01 02:44:53,989]: Epoch: 105, Loss:0.4384 Train: 0.8750, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:53,998]: Epoch: 106, Loss:0.4291 Train: 0.8833, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:54,006]: Epoch: 107, Loss:0.4164 Train: 0.9000, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0083
[2025-04-01 02:44:54,012]: Epoch: 108, Loss:0.4222 Train: 0.9167, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0056
[2025-04-01 02:44:54,020]: Epoch: 109, Loss:0.3871 Train: 0.9000, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0077
[2025-04-01 02:44:54,026]: Epoch: 110, Loss:0.4279 Train: 0.9000, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0060
[2025-04-01 02:44:54,033]: Epoch: 111, Loss:0.4135 Train: 0.9083, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0070
[2025-04-01 02:44:54,041]: Epoch: 112, Loss:0.3633 Train: 0.9000, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:54,048]: Epoch: 113, Loss:0.4348 Train: 0.9250, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0063
[2025-04-01 02:44:54,056]: Epoch: 114, Loss:0.4421 Train: 0.9167, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0080
[2025-04-01 02:44:54,064]: Epoch: 115, Loss:0.4393 Train: 0.8917, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:54,070]: Epoch: 116, Loss:0.4536 Train: 0.8833, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:54,078]: Epoch: 117, Loss:0.4187 Train: 0.9083, Val:0.5875, Test: 0.3725, Time(s/epoch):0.0070
[2025-04-01 02:44:54,085]: Epoch: 118, Loss:0.3821 Train: 0.8750, Val:0.6000, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:54,093]: Epoch: 119, Loss:0.4620 Train: 0.9000, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:54,099]: Epoch: 120, Loss:0.4486 Train: 0.9000, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0067
[2025-04-01 02:44:54,106]: Epoch: 121, Loss:0.4354 Train: 0.9083, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:54,114]: Epoch: 122, Loss:0.3902 Train: 0.8917, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0079
[2025-04-01 02:44:54,122]: Epoch: 123, Loss:0.4131 Train: 0.8833, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0076
[2025-04-01 02:44:54,129]: Epoch: 124, Loss:0.3974 Train: 0.8500, Val:0.6000, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:54,136]: Epoch: 125, Loss:0.4053 Train: 0.8417, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0066
[2025-04-01 02:44:54,144]: Epoch: 126, Loss:0.4737 Train: 0.9000, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0083
[2025-04-01 02:44:54,152]: Epoch: 127, Loss:0.4670 Train: 0.8833, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0073
[2025-04-01 02:44:54,160]: Epoch: 128, Loss:0.4558 Train: 0.8583, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0079
[2025-04-01 02:44:54,166]: Epoch: 129, Loss:0.4523 Train: 0.8583, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0061
[2025-04-01 02:44:54,175]: Epoch: 130, Loss:0.5212 Train: 0.8750, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:54,181]: Epoch: 131, Loss:0.4609 Train: 0.8167, Val:0.5875, Test: 0.4314, Time(s/epoch):0.0065
[2025-04-01 02:44:54,188]: Epoch: 132, Loss:0.4050 Train: 0.8417, Val:0.6125, Test: 0.4706, Time(s/epoch):0.0065
[2025-04-01 02:44:54,196]: Epoch: 133, Loss:0.5173 Train: 0.8667, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0082
[2025-04-01 02:44:54,203]: Epoch: 134, Loss:0.4647 Train: 0.8500, Val:0.5000, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:54,210]: Epoch: 135, Loss:0.4314 Train: 0.8917, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0066
[2025-04-01 02:44:54,217]: Epoch: 136, Loss:0.4417 Train: 0.8667, Val:0.5375, Test: 0.3529, Time(s/epoch):0.0073
[2025-04-01 02:44:54,224]: Epoch: 137, Loss:0.4617 Train: 0.8750, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0064
[2025-04-01 02:44:54,232]: Epoch: 138, Loss:0.4689 Train: 0.8667, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:54,240]: Epoch: 139, Loss:0.4603 Train: 0.8417, Val:0.5750, Test: 0.4706, Time(s/epoch):0.0079
[2025-04-01 02:44:54,247]: Epoch: 140, Loss:0.4478 Train: 0.8500, Val:0.5875, Test: 0.4706, Time(s/epoch):0.0073
[2025-04-01 02:44:54,255]: Epoch: 141, Loss:0.4375 Train: 0.8583, Val:0.5750, Test: 0.4510, Time(s/epoch):0.0077
[2025-04-01 02:44:54,262]: Epoch: 142, Loss:0.4277 Train: 0.8583, Val:0.5500, Test: 0.4510, Time(s/epoch):0.0071
[2025-04-01 02:44:54,270]: Epoch: 143, Loss:0.5255 Train: 0.8667, Val:0.5375, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:54,276]: Epoch: 144, Loss:0.4517 Train: 0.9000, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0058
[2025-04-01 02:44:54,283]: Epoch: 145, Loss:0.4378 Train: 0.8750, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:54,291]: Epoch: 146, Loss:0.4112 Train: 0.8667, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0078
[2025-04-01 02:44:54,299]: Epoch: 147, Loss:0.5099 Train: 0.8667, Val:0.5375, Test: 0.3529, Time(s/epoch):0.0077
[2025-04-01 02:44:54,306]: Epoch: 148, Loss:0.4346 Train: 0.9083, Val:0.5375, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:54,317]: Epoch: 149, Loss:0.3956 Train: 0.9000, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0099
[2025-04-01 02:44:54,325]: Epoch: 150, Loss:0.4404 Train: 0.9083, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0083
[2025-04-01 02:44:54,333]: Epoch: 151, Loss:0.4200 Train: 0.9250, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:54,339]: Epoch: 152, Loss:0.4375 Train: 0.9000, Val:0.5750, Test: 0.3137, Time(s/epoch):0.0060
[2025-04-01 02:44:54,348]: Epoch: 153, Loss:0.4209 Train: 0.8917, Val:0.5625, Test: 0.3529, Time(s/epoch):0.0082
[2025-04-01 02:44:54,357]: Epoch: 154, Loss:0.4366 Train: 0.9000, Val:0.5250, Test: 0.4314, Time(s/epoch):0.0086
[2025-04-01 02:44:54,366]: Epoch: 155, Loss:0.3835 Train: 0.9083, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0096
[2025-04-01 02:44:54,375]: Epoch: 156, Loss:0.4407 Train: 0.8667, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0087
[2025-04-01 02:44:54,383]: Epoch: 157, Loss:0.4102 Train: 0.8667, Val:0.5625, Test: 0.4510, Time(s/epoch):0.0075
[2025-04-01 02:44:54,391]: Epoch: 158, Loss:0.4192 Train: 0.8917, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0082
[2025-04-01 02:44:54,400]: Epoch: 159, Loss:0.3567 Train: 0.8917, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0085
[2025-04-01 02:44:54,407]: Epoch: 160, Loss:0.4473 Train: 0.9250, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0067
[2025-04-01 02:44:54,416]: Epoch: 161, Loss:0.4105 Train: 0.9167, Val:0.5625, Test: 0.3725, Time(s/epoch):0.0090
[2025-04-01 02:44:54,424]: Epoch: 162, Loss:0.4014 Train: 0.9000, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:54,431]: Epoch: 163, Loss:0.3787 Train: 0.9083, Val:0.5625, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:54,439]: Epoch: 164, Loss:0.3810 Train: 0.8667, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:54,448]: Epoch: 165, Loss:0.3781 Train: 0.8833, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0084
[2025-04-01 02:44:54,454]: Epoch: 166, Loss:0.3590 Train: 0.9000, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0065
[2025-04-01 02:44:54,462]: Epoch: 167, Loss:0.3830 Train: 0.8917, Val:0.5500, Test: 0.3529, Time(s/epoch):0.0074
[2025-04-01 02:44:54,470]: Epoch: 168, Loss:0.3937 Train: 0.8833, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0075
[2025-04-01 02:44:54,477]: Epoch: 169, Loss:0.4112 Train: 0.9167, Val:0.5625, Test: 0.3922, Time(s/epoch):0.0067
[2025-04-01 02:44:54,485]: Epoch: 170, Loss:0.4010 Train: 0.9083, Val:0.5750, Test: 0.4118, Time(s/epoch):0.0084
[2025-04-01 02:44:54,493]: Epoch: 171, Loss:0.3911 Train: 0.8917, Val:0.5875, Test: 0.4118, Time(s/epoch):0.0079
[2025-04-01 02:44:54,500]: Epoch: 172, Loss:0.3862 Train: 0.8583, Val:0.5625, Test: 0.4314, Time(s/epoch):0.0063
[2025-04-01 02:44:54,507]: Epoch: 173, Loss:0.4672 Train: 0.9083, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0069
[2025-04-01 02:44:54,515]: Epoch: 174, Loss:0.3681 Train: 0.9000, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0077
[2025-04-01 02:44:54,521]: Epoch: 175, Loss:0.3754 Train: 0.9167, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0063
[2025-04-01 02:44:54,529]: Epoch: 176, Loss:0.3403 Train: 0.9000, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:54,536]: Epoch: 177, Loss:0.4332 Train: 0.9083, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0072
[2025-04-01 02:44:54,543]: Epoch: 178, Loss:0.3919 Train: 0.8750, Val:0.5500, Test: 0.4118, Time(s/epoch):0.0065
[2025-04-01 02:44:54,552]: Epoch: 179, Loss:0.3835 Train: 0.8667, Val:0.5750, Test: 0.3922, Time(s/epoch):0.0084
[2025-04-01 02:44:54,560]: Epoch: 180, Loss:0.4357 Train: 0.8750, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0086
[2025-04-01 02:44:54,570]: Epoch: 181, Loss:0.4835 Train: 0.9167, Val:0.5125, Test: 0.4118, Time(s/epoch):0.0091
[2025-04-01 02:44:54,577]: Epoch: 182, Loss:0.4225 Train: 0.9000, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0071
[2025-04-01 02:44:54,584]: Epoch: 183, Loss:0.3615 Train: 0.8583, Val:0.4750, Test: 0.4118, Time(s/epoch):0.0068
[2025-04-01 02:44:54,590]: Epoch: 184, Loss:0.4232 Train: 0.9167, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0059
[2025-04-01 02:44:54,598]: Epoch: 185, Loss:0.4501 Train: 0.8833, Val:0.5250, Test: 0.3725, Time(s/epoch):0.0077
[2025-04-01 02:44:54,605]: Epoch: 186, Loss:0.3660 Train: 0.9083, Val:0.5375, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:54,611]: Epoch: 187, Loss:0.4089 Train: 0.9000, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0056
[2025-04-01 02:44:54,618]: Epoch: 188, Loss:0.3881 Train: 0.9000, Val:0.5500, Test: 0.4314, Time(s/epoch):0.0069
[2025-04-01 02:44:54,626]: Epoch: 189, Loss:0.4506 Train: 0.9250, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0073
[2025-04-01 02:44:54,633]: Epoch: 190, Loss:0.3514 Train: 0.8750, Val:0.5000, Test: 0.3529, Time(s/epoch):0.0073
[2025-04-01 02:44:54,640]: Epoch: 191, Loss:0.4653 Train: 0.9167, Val:0.5000, Test: 0.3529, Time(s/epoch):0.0067
[2025-04-01 02:44:54,648]: Epoch: 192, Loss:0.4240 Train: 0.9167, Val:0.5125, Test: 0.3725, Time(s/epoch):0.0080
[2025-04-01 02:44:54,656]: Epoch: 193, Loss:0.4349 Train: 0.9333, Val:0.5500, Test: 0.3922, Time(s/epoch):0.0075
[2025-04-01 02:44:54,664]: Epoch: 194, Loss:0.4202 Train: 0.9000, Val:0.5500, Test: 0.3725, Time(s/epoch):0.0082
[2025-04-01 02:44:54,672]: Epoch: 195, Loss:0.4128 Train: 0.8750, Val:0.5875, Test: 0.3529, Time(s/epoch):0.0070
[2025-04-01 02:44:54,679]: Epoch: 196, Loss:0.5018 Train: 0.8917, Val:0.5875, Test: 0.3529, Time(s/epoch):0.0075
[2025-04-01 02:44:54,687]: Epoch: 197, Loss:0.4148 Train: 0.8583, Val:0.5875, Test: 0.3922, Time(s/epoch):0.0074
[2025-04-01 02:44:54,694]: Epoch: 198, Loss:0.4835 Train: 0.8667, Val:0.5375, Test: 0.4118, Time(s/epoch):0.0067
[2025-04-01 02:44:54,701]: Epoch: 199, Loss:0.4892 Train: 0.8750, Val:0.5375, Test: 0.3725, Time(s/epoch):0.0068
[2025-04-01 02:44:54,708]: Epoch: 200, Loss:0.4924 Train: 0.9000, Val:0.5250, Test: 0.3922, Time(s/epoch):0.0069
[2025-04-01 02:44:54,708]: [Run-3 score] {'train': 0.8666666666666667, 'val': 0.625, 'test': 0.37254901960784315}
[2025-04-01 02:44:54,709]: [Average Score] {'train': np.float64(0.6400000000000001), 'val': np.float64(0.54375), 'test': np.float64(0.49673202614379086)} 
[2025-04-01 02:44:54,709]: [std Score] {'train': np.float64(0.11793752269426071), 'val': np.float64(0.04027897507798993), 'test': np.float64(0.0614518131949969)} 
[2025-04-01 02:44:54,709]: [Best Score] {'train': 0.8666666666666667, 'val': 0.625, 'test': 0.6078431372549019}
[2025-04-01 02:44:54,709]: [Best test run] 5
2.6.1
cannot load google_drive_downloader, install it if needed.
Namespace(model='GCN', num_layer=2, repeat=3, num_epoch=200, dataset='Wisconsin', data_dir='../data', gpu='0', running_id='0', log_dir=None, hidden=64, moment=1, seed=0, gnn_seed=None, auto_fixed_seed=False, use_center_moment=True, lr=0.01, wd=0.005, graph_learn=False, rewiring_type='original', lr_gl=0.001, wd_gl=0.005, thres_min_deg=3, thres_min_deg_ratio=1.0, save_dir_gl='../ckpt/', window=[10000, 10000], shuffle=[False, False], epoch_train_gl=200, epoch_finetune_gl=30, seed_gl=0, k=8, cat_self=True, drop_last=[False, False], prunning=False, epsilon=None, thres_prunning=0.0, use_cpu_cache=False, drop_edge=False, prob_drop_edge=0.0)
cpu
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4526 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4711 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5487 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5855 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5777 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.6339 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.7087 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5901 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5648 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5117 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5319 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5454 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4868 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5310 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5270 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5451 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4851 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5511 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5140 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4972 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4981 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5308 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5100 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5282 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5311 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5199 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5185 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5079 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.4994 s
Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251], val_mask=[251], test_mask=[251])
GCNNet(
  (convs): ModuleList(
    (0): GCNConv(1703, 64)
    (1): GCNConv(64, 5)
  )
)
Epoch running time:1.5107 s
Namespace(model='GCN', num_layer=2, repeat=3, num_epoch=200, dataset='Wisconsin', data_dir='../data', gpu='0', running_id='0', log_dir=None, hidden=64, moment=1, seed=0, gnn_seed=None, auto_fixed_seed=False, use_center_moment=True, lr=0.01, wd=0.005, graph_learn=False, rewiring_type='original', lr_gl=0.001, wd_gl=0.005, thres_min_deg=3, thres_min_deg_ratio=1.0, save_dir_gl='../ckpt/', window=[10000, 10000], shuffle=[False, False], epoch_train_gl=200, epoch_finetune_gl=30, seed_gl=0, k=8, cat_self=True, drop_last=[False, False], prunning=False, epsilon=None, thres_prunning=0.0, use_cpu_cache=False, drop_edge=False, prob_drop_edge=0.0)
